{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semi-supervised learning uses a combination of supervised learning and unsupervised learning techniques. It is used when we have a combination of labeled and unlabeled data. It is generally used when we have a large unlabeled dataset and we do not want to do manual labelling for any practical reason (time,cost,...). It is an approach combining small amount of labeled data with large amounts of unlabeled data.\n",
    "The steps are as follow in the paper:\n",
    "\n",
    "1- we train our model on the labeled data (classic supervised learning) and we disregard for the moment the unlabeled large portion of the data (teacher model)\n",
    "\n",
    "2- we use our trained model to predict on the remaining unlabeled portion of the data and use predictions of most relevant images for constructing new training data labeled\n",
    "\n",
    "3- train new model on this new constructed training data (student model)\n",
    "\n",
    "4- finetune student model on initial training labeled data \n",
    "\n",
    "The benefits are that now, we are able to train our model on a much larger dataset without any manual labor .\n",
    "\n",
    "\n",
    "In weakly supervised learning , we have data partially labeled (lower quality, cheaper and from non-experts) i.e data with noisy labels.\n",
    "We usually consider three types of weakly supervised learning:\n",
    "\n",
    "-incomplete supervision, where only a subset of training data is given with labels (very similar to semi-supervised)\n",
    "\n",
    "-inexact supervision, training data have poor quality labels\n",
    "\n",
    "-inaccurate supervision, labels do not represent ground truth\n",
    "\n",
    "Therefore, the  difference between weakly supervised learning and semi-supervised learning is that weakly supervised learning is supervision with noisy labels while semi supervised learning is when only a subset of data has labels.\n",
    "\n",
    "In both papers, they can use the same dataset because in the semi-supervised training , they learn the pseudo labels using the teacher model and in the weakly supervised pretraining, they make use of the hashtags from those images even if they are noisy labels , innacurate sometimes but they demonstrate that those noisy labels (hashtags here) won't affect performance later on as long as they have very large amount of data (billions of images here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels derived from hashtags can be very noisy because users creating those hashtags might be using irrelevant hashtags but also might omit relevant hashtag for specific images. However, the trained models are very robust against noise in the labels as shown in Figure 3 of the paper where they artificially replaced p% for different values of p of the hashtags by hashtags obtained by sampling from\n",
    "the marginal distribution over hashtags making  sure  they  do not use the base hastag  . They showed that a noise level of p=10% induce only a drop in accuracy of 1% and a label noise level of p=25% induce only a 2% drop in accuracy compared with no label noise. This shows that when we have large amount of data (billions of images), noisy labels are not a major issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In figure 4, we can see that resampling from the hashtag distribution is important during pretraining for transfer learning because it allows good transfer to the classification tasks. Indeed, we see an accuracy improvement of around 6% for both uniform and square-root resampling compared to normal regardless of number of  classes during the transfer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two models because we need a teacher model to train on the labeled data and use this trained model to predict on the unlabeled portion and use predictions on top K images to construct a new training data.\n",
    "From this new training date, we can now train a model (student model) and finally finetune this student model on the initial training labeled data (which is the original task).\n",
    "\n",
    "This approach allowed them to leverage billions of unlabeled data thanks to the teacher model and use this to train the student model and finetune for the relatively small specific task.\n",
    "\n",
    "Distillation technique has been proposed to achieve faster convergence and more importantly to handle lack of labeled data transferring knowledge learn from one model to another. The student teacher technique, by leveraging large amount of unlabeled data and transferring knowledge from the teacher to the student using the new constructed training data and then performing finetuning on this student data is a type of distillation technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K is the number of top examples selected from the unlabeled data for each target label (top-K images). So if they are 10 classes, they will select 10*K images with K images per target label predicted. \n",
    "\n",
    "P is the number of class retained per image ( P highest score per image ) . The output for each image is a softmax meaning that each class will receive a probability and we retain the labels associated to the top P probabilities for each image.\n",
    "\n",
    "P>1 is chosen because in the same image , we can have multiple classes. I actually asked this question during the lecture and the idea is that in the same images you can have flowers, a dog and a man cooking steak in his barbecue.\n",
    "It is therefore difficult to associate a single label to a single image especially for labels that have not been seen very often. By allowing a more flexible approach with P>1, we will also make sure that we have enough examples for each labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new dataset is created by predicting the labels for the unlabeled images  with the teacher model. We basically train a model on the labeled data and use this trained model to predict labels for unlabeled images and we select top-K images per  target label to create the  new labeled  dataset after selecting for images the highest scores P labels using the softmax probabilities per image.\n",
    "\n",
    "Since we allow each image to be associated with more than one label, we are very likely to have the same image to belong to more than one class. Actually, the higher P is , the more likely we will get the same image belonging to multiple class ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase K, the number of labeled image per class , we see that the performances of the model improves. This makes sense because increasing K means more image per class and therefore a better representation of each class through the images thanks to  a  diversification  of  images  representing the same class.\n",
    "\n",
    "However, we can see that after  a certain point, around 32k here, the performances start to drop again. This also makes sense as we are introducing noise labeling when K is too large. Increasing K also increases the probability of selecting false positives.\n",
    "\n",
    "Therefore it seems that chosing a range between 4-32k works well for this dataset as it accounts for enough diversification while limitating the noise labeling and the introduction of false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
