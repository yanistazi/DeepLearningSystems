{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0ZXdioQPy6w"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "plt.ion()\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTbP3l2zQnzi"
   },
   "source": [
    "# Q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O307qPIuQ8YD"
   },
   "source": [
    "# a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swKyuCYSP2MD",
    "outputId": "be34cc46-c202-41d6-a9b0-306ad92af0df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQBQjS93Py61"
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'drive/My Drive/vgg-flowers'\n",
    "torch.manual_seed(0)\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64,  ###change batch size to 64\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "id": "e2wKrk_cPy64",
    "outputId": "0d6aa7b1-cf38-4636-faca-d58aafa24cd9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "torch.manual_seed(0)\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Get image inputs from two classes\n",
    "indexes_2_classes = [i for i, x in enumerate(classes.tolist()) if x == 59] + \\\n",
    "                    [i for i, x in enumerate(classes.tolist()) if x == 97]\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs[indexes_2_classes])\n",
    "\n",
    "imshow(out, title=[59,59,97,97])  ## from 2 classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhFRNDB7MRhj"
   },
   "source": [
    "This dataset contains 1020 training images, 1020 validation images and 6149 test images. There are 102 classes .\n",
    "There are 10 images per class in the training and validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8paOdvLAPy67"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGIeqQOhPy6-"
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQmcaZGjN0Bb"
   },
   "outputs": [],
   "source": [
    "# Finetune the Resnet 50 and reset the final fully connected layer output to the number of classes in the target dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tD8hmyxsP9dM"
   },
   "outputs": [],
   "source": [
    "model_ft = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "# Reset final FC to the number of classes in the target dataset :\n",
    "model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1UWKC_xvQgkc"
   },
   "source": [
    "# b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "QELkveVdQkc8",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5dc53c1e-84d5-4662-8861-32db969b1726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 4.5955 Acc: 0.0206\n",
      "val Loss: 4.5352 Acc: 0.0490\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 4.4993 Acc: 0.0441\n",
      "val Loss: 4.4271 Acc: 0.1049\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 4.3910 Acc: 0.1108\n",
      "val Loss: 4.2873 Acc: 0.1598\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 4.2522 Acc: 0.1863\n",
      "val Loss: 4.1190 Acc: 0.2402\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 4.1153 Acc: 0.2412\n",
      "val Loss: 3.9007 Acc: 0.3059\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 3.9369 Acc: 0.2775\n",
      "val Loss: 3.6911 Acc: 0.3451\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 3.7387 Acc: 0.3539\n",
      "val Loss: 3.4807 Acc: 0.3873\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 3.5541 Acc: 0.3951\n",
      "val Loss: 3.2622 Acc: 0.4059\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 3.3833 Acc: 0.4176\n",
      "val Loss: 3.0699 Acc: 0.4225\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 3.2071 Acc: 0.4569\n",
      "val Loss: 2.9121 Acc: 0.4520\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 3.0368 Acc: 0.4775\n",
      "val Loss: 2.7494 Acc: 0.4873\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 2.8721 Acc: 0.5186\n",
      "val Loss: 2.5750 Acc: 0.4912\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 2.7186 Acc: 0.5363\n",
      "val Loss: 2.4543 Acc: 0.5176\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 2.6037 Acc: 0.5324\n",
      "val Loss: 2.3322 Acc: 0.5500\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 2.4289 Acc: 0.5902\n",
      "val Loss: 2.2274 Acc: 0.5676\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 2.2983 Acc: 0.5980\n",
      "val Loss: 2.1392 Acc: 0.5804\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 2.2011 Acc: 0.6098\n",
      "val Loss: 2.0445 Acc: 0.5882\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 2.0592 Acc: 0.6441\n",
      "val Loss: 1.9656 Acc: 0.6118\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 1.9970 Acc: 0.6294\n",
      "val Loss: 1.8605 Acc: 0.6324\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 1.9082 Acc: 0.6706\n",
      "val Loss: 1.8121 Acc: 0.6343\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 1.8161 Acc: 0.6755\n",
      "val Loss: 1.7411 Acc: 0.6422\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 1.7435 Acc: 0.6951\n",
      "val Loss: 1.6716 Acc: 0.6461\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 1.6327 Acc: 0.7118\n",
      "val Loss: 1.6157 Acc: 0.6647\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 1.5764 Acc: 0.7275\n",
      "val Loss: 1.5693 Acc: 0.6735\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 1.5368 Acc: 0.7304\n",
      "val Loss: 1.5219 Acc: 0.6853\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 1.4347 Acc: 0.7422\n",
      "val Loss: 1.4750 Acc: 0.6931\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 1.3605 Acc: 0.7706\n",
      "val Loss: 1.4369 Acc: 0.6971\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 1.3147 Acc: 0.7696\n",
      "val Loss: 1.4015 Acc: 0.7010\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 1.3129 Acc: 0.7559\n",
      "val Loss: 1.3565 Acc: 0.7108\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 1.2281 Acc: 0.7794\n",
      "val Loss: 1.3374 Acc: 0.7049\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 1.1674 Acc: 0.7922\n",
      "val Loss: 1.2887 Acc: 0.7206\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 1.0929 Acc: 0.8098\n",
      "val Loss: 1.2718 Acc: 0.7304\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 1.0851 Acc: 0.8127\n",
      "val Loss: 1.2383 Acc: 0.7284\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 1.0239 Acc: 0.8225\n",
      "val Loss: 1.2091 Acc: 0.7363\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 1.0061 Acc: 0.8304\n",
      "val Loss: 1.1804 Acc: 0.7441\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.9620 Acc: 0.8265\n",
      "val Loss: 1.1435 Acc: 0.7520\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.9143 Acc: 0.8471\n",
      "val Loss: 1.1338 Acc: 0.7559\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.8561 Acc: 0.8657\n",
      "val Loss: 1.0994 Acc: 0.7510\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.8532 Acc: 0.8598\n",
      "val Loss: 1.0815 Acc: 0.7637\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.8022 Acc: 0.8696\n",
      "val Loss: 1.0811 Acc: 0.7549\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.8146 Acc: 0.8686\n",
      "val Loss: 1.0440 Acc: 0.7637\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.7710 Acc: 0.8686\n",
      "val Loss: 1.0221 Acc: 0.7647\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.7412 Acc: 0.8735\n",
      "val Loss: 1.0225 Acc: 0.7647\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.7205 Acc: 0.8833\n",
      "val Loss: 1.0043 Acc: 0.7637\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.6857 Acc: 0.9020\n",
      "val Loss: 0.9900 Acc: 0.7745\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.6879 Acc: 0.8804\n",
      "val Loss: 0.9873 Acc: 0.7745\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.6588 Acc: 0.8980\n",
      "val Loss: 0.9933 Acc: 0.7598\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.6296 Acc: 0.8951\n",
      "val Loss: 0.9752 Acc: 0.7716\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.6151 Acc: 0.8971\n",
      "val Loss: 0.9666 Acc: 0.7706\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.5848 Acc: 0.9127\n",
      "val Loss: 0.9481 Acc: 0.7725\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.5243 Acc: 0.9275\n",
      "val Loss: 0.9392 Acc: 0.7775\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.5747 Acc: 0.8980\n",
      "val Loss: 0.9377 Acc: 0.7755\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.5529 Acc: 0.9176\n",
      "val Loss: 0.9382 Acc: 0.7833\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.5905 Acc: 0.9108\n",
      "val Loss: 0.9398 Acc: 0.7804\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.5377 Acc: 0.9265\n",
      "val Loss: 0.9353 Acc: 0.7775\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.5139 Acc: 0.9235\n",
      "val Loss: 0.9347 Acc: 0.7794\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.5420 Acc: 0.9118\n",
      "val Loss: 0.9315 Acc: 0.7824\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.5740 Acc: 0.9078\n",
      "val Loss: 0.9293 Acc: 0.7775\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.5813 Acc: 0.9098\n",
      "val Loss: 0.9272 Acc: 0.7804\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.5501 Acc: 0.9186\n",
      "val Loss: 0.9312 Acc: 0.7755\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.5463 Acc: 0.9108\n",
      "val Loss: 0.9304 Acc: 0.7775\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.5460 Acc: 0.9127\n",
      "val Loss: 0.9259 Acc: 0.7775\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.5348 Acc: 0.9206\n",
      "val Loss: 0.9219 Acc: 0.7765\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.5446 Acc: 0.9118\n",
      "val Loss: 0.9265 Acc: 0.7804\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.5331 Acc: 0.9167\n",
      "val Loss: 0.9252 Acc: 0.7833\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.5419 Acc: 0.9029\n",
      "val Loss: 0.9234 Acc: 0.7794\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.4953 Acc: 0.9245\n",
      "val Loss: 0.9267 Acc: 0.7784\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.5396 Acc: 0.9157\n",
      "val Loss: 0.9245 Acc: 0.7784\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.4938 Acc: 0.9304\n",
      "val Loss: 0.9251 Acc: 0.7765\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.5644 Acc: 0.9098\n",
      "val Loss: 0.9224 Acc: 0.7814\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.5170 Acc: 0.9186\n",
      "val Loss: 0.9208 Acc: 0.7833\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.5363 Acc: 0.9118\n",
      "val Loss: 0.9194 Acc: 0.7814\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.5151 Acc: 0.9176\n",
      "val Loss: 0.9169 Acc: 0.7833\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.5015 Acc: 0.9255\n",
      "val Loss: 0.9185 Acc: 0.7804\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.5144 Acc: 0.9127\n",
      "val Loss: 0.9146 Acc: 0.7843\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.4930 Acc: 0.9255\n",
      "val Loss: 0.9126 Acc: 0.7824\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.5179 Acc: 0.9245\n",
      "val Loss: 0.9142 Acc: 0.7824\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.4983 Acc: 0.9265\n",
      "val Loss: 0.9146 Acc: 0.7804\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.4986 Acc: 0.9216\n",
      "val Loss: 0.9165 Acc: 0.7804\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.5485 Acc: 0.9069\n",
      "val Loss: 0.9159 Acc: 0.7755\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.4829 Acc: 0.9255\n",
      "val Loss: 0.9125 Acc: 0.7833\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.5356 Acc: 0.9098\n",
      "val Loss: 0.9165 Acc: 0.7794\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.5041 Acc: 0.9225\n",
      "val Loss: 0.9109 Acc: 0.7863\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.4706 Acc: 0.9314\n",
      "val Loss: 0.9152 Acc: 0.7784\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.4965 Acc: 0.9373\n",
      "val Loss: 0.9103 Acc: 0.7804\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.4880 Acc: 0.9353\n",
      "val Loss: 0.9111 Acc: 0.7775\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.4724 Acc: 0.9353\n",
      "val Loss: 0.9104 Acc: 0.7794\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.4769 Acc: 0.9304\n",
      "val Loss: 0.9087 Acc: 0.7814\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.4865 Acc: 0.9333\n",
      "val Loss: 0.9029 Acc: 0.7853\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.4814 Acc: 0.9314\n",
      "val Loss: 0.9054 Acc: 0.7863\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.5184 Acc: 0.9176\n",
      "val Loss: 0.9117 Acc: 0.7833\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.5265 Acc: 0.9118\n",
      "val Loss: 0.8986 Acc: 0.7892\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.4619 Acc: 0.9431\n",
      "val Loss: 0.8987 Acc: 0.7833\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.4782 Acc: 0.9314\n",
      "val Loss: 0.9065 Acc: 0.7824\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.4846 Acc: 0.9284\n",
      "val Loss: 0.9073 Acc: 0.7775\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.4869 Acc: 0.9314\n",
      "val Loss: 0.9040 Acc: 0.7873\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.5242 Acc: 0.9157\n",
      "val Loss: 0.9018 Acc: 0.7853\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.4416 Acc: 0.9304\n",
      "val Loss: 0.8997 Acc: 0.7824\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.4643 Acc: 0.9255\n",
      "val Loss: 0.8992 Acc: 0.7843\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.4469 Acc: 0.9412\n",
      "val Loss: 0.9026 Acc: 0.7843\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.4641 Acc: 0.9363\n",
      "val Loss: 0.9087 Acc: 0.7794\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.4509 Acc: 0.9392\n",
      "val Loss: 0.9066 Acc: 0.7853\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.5062 Acc: 0.9235\n",
      "val Loss: 0.9066 Acc: 0.7843\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.4820 Acc: 0.9284\n",
      "val Loss: 0.8996 Acc: 0.7853\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.4755 Acc: 0.9235\n",
      "val Loss: 0.9004 Acc: 0.7843\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.4739 Acc: 0.9284\n",
      "val Loss: 0.9028 Acc: 0.7804\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.4619 Acc: 0.9265\n",
      "val Loss: 0.9062 Acc: 0.7824\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.4421 Acc: 0.9480\n",
      "val Loss: 0.8992 Acc: 0.7863\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.5017 Acc: 0.9216\n",
      "val Loss: 0.8977 Acc: 0.7892\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.4329 Acc: 0.9382\n",
      "val Loss: 0.9007 Acc: 0.7863\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.4637 Acc: 0.9255\n",
      "val Loss: 0.8997 Acc: 0.7784\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.5163 Acc: 0.9078\n",
      "val Loss: 0.8980 Acc: 0.7804\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.5137 Acc: 0.9196\n",
      "val Loss: 0.8965 Acc: 0.7882\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.4362 Acc: 0.9382\n",
      "val Loss: 0.9019 Acc: 0.7843\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.4854 Acc: 0.9206\n",
      "val Loss: 0.9003 Acc: 0.7873\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.4786 Acc: 0.9294\n",
      "val Loss: 0.9019 Acc: 0.7843\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.4415 Acc: 0.9382\n",
      "val Loss: 0.8954 Acc: 0.7902\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.4798 Acc: 0.9235\n",
      "val Loss: 0.8968 Acc: 0.7863\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.5008 Acc: 0.9147\n",
      "val Loss: 0.8959 Acc: 0.7833\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.4823 Acc: 0.9176\n",
      "val Loss: 0.8977 Acc: 0.7814\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.4709 Acc: 0.9304\n",
      "val Loss: 0.8942 Acc: 0.7853\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.4493 Acc: 0.9363\n",
      "val Loss: 0.9000 Acc: 0.7833\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.4752 Acc: 0.9294\n",
      "val Loss: 0.8979 Acc: 0.7843\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.4716 Acc: 0.9284\n",
      "val Loss: 0.8974 Acc: 0.7843\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.4832 Acc: 0.9275\n",
      "val Loss: 0.9008 Acc: 0.7863\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.4507 Acc: 0.9275\n",
      "val Loss: 0.8978 Acc: 0.7824\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.4764 Acc: 0.9265\n",
      "val Loss: 0.8961 Acc: 0.7873\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.4363 Acc: 0.9324\n",
      "val Loss: 0.8933 Acc: 0.7853\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.4743 Acc: 0.9196\n",
      "val Loss: 0.9002 Acc: 0.7853\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.4888 Acc: 0.9284\n",
      "val Loss: 0.9007 Acc: 0.7843\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.4878 Acc: 0.9176\n",
      "val Loss: 0.8969 Acc: 0.7824\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.4325 Acc: 0.9392\n",
      "val Loss: 0.8958 Acc: 0.7824\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.4994 Acc: 0.9216\n",
      "val Loss: 0.8949 Acc: 0.7843\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.4706 Acc: 0.9245\n",
      "val Loss: 0.8983 Acc: 0.7833\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.4392 Acc: 0.9402\n",
      "val Loss: 0.8953 Acc: 0.7863\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.4803 Acc: 0.9225\n",
      "val Loss: 0.8964 Acc: 0.7892\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.5041 Acc: 0.9216\n",
      "val Loss: 0.9005 Acc: 0.7833\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.4160 Acc: 0.9422\n",
      "val Loss: 0.8989 Acc: 0.7824\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.4852 Acc: 0.9255\n",
      "val Loss: 0.9025 Acc: 0.7784\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.4837 Acc: 0.9304\n",
      "val Loss: 0.9004 Acc: 0.7853\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.4842 Acc: 0.9265\n",
      "val Loss: 0.8987 Acc: 0.7833\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.4200 Acc: 0.9353\n",
      "val Loss: 0.8966 Acc: 0.7833\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.4697 Acc: 0.9343\n",
      "val Loss: 0.9012 Acc: 0.7814\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.4094 Acc: 0.9471\n",
      "val Loss: 0.8979 Acc: 0.7843\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.4736 Acc: 0.9265\n",
      "val Loss: 0.8968 Acc: 0.7824\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.4840 Acc: 0.9127\n",
      "val Loss: 0.8962 Acc: 0.7824\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.4947 Acc: 0.9118\n",
      "val Loss: 0.8965 Acc: 0.7814\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.4893 Acc: 0.9245\n",
      "val Loss: 0.8954 Acc: 0.7873\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.4581 Acc: 0.9314\n",
      "val Loss: 0.8921 Acc: 0.7863\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.4471 Acc: 0.9461\n",
      "val Loss: 0.8961 Acc: 0.7833\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.4474 Acc: 0.9324\n",
      "val Loss: 0.8938 Acc: 0.7873\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.4914 Acc: 0.9265\n",
      "val Loss: 0.8949 Acc: 0.7804\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.4487 Acc: 0.9314\n",
      "val Loss: 0.8958 Acc: 0.7873\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.4663 Acc: 0.9245\n",
      "val Loss: 0.8982 Acc: 0.7863\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.4257 Acc: 0.9471\n",
      "val Loss: 0.8980 Acc: 0.7843\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.4502 Acc: 0.9392\n",
      "val Loss: 0.8963 Acc: 0.7853\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.4397 Acc: 0.9392\n",
      "val Loss: 0.8959 Acc: 0.7843\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.4411 Acc: 0.9333\n",
      "val Loss: 0.8950 Acc: 0.7863\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.4571 Acc: 0.9324\n",
      "val Loss: 0.8985 Acc: 0.7843\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.4762 Acc: 0.9235\n",
      "val Loss: 0.9009 Acc: 0.7814\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.4950 Acc: 0.9206\n",
      "val Loss: 0.9023 Acc: 0.7833\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.4759 Acc: 0.9176\n",
      "val Loss: 0.8984 Acc: 0.7853\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.4876 Acc: 0.9186\n",
      "val Loss: 0.8960 Acc: 0.7843\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.4685 Acc: 0.9304\n",
      "val Loss: 0.8949 Acc: 0.7824\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.4653 Acc: 0.9314\n",
      "val Loss: 0.8978 Acc: 0.7824\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.4332 Acc: 0.9373\n",
      "val Loss: 0.8973 Acc: 0.7804\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.4675 Acc: 0.9294\n",
      "val Loss: 0.8980 Acc: 0.7873\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.4815 Acc: 0.9265\n",
      "val Loss: 0.8952 Acc: 0.7824\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.4400 Acc: 0.9304\n",
      "val Loss: 0.8970 Acc: 0.7784\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.4737 Acc: 0.9284\n",
      "val Loss: 0.8982 Acc: 0.7814\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.4254 Acc: 0.9402\n",
      "val Loss: 0.9003 Acc: 0.7814\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.4788 Acc: 0.9275\n",
      "val Loss: 0.8994 Acc: 0.7824\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.4411 Acc: 0.9402\n",
      "val Loss: 0.8938 Acc: 0.7843\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.4609 Acc: 0.9294\n",
      "val Loss: 0.8958 Acc: 0.7784\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.4596 Acc: 0.9343\n",
      "val Loss: 0.8930 Acc: 0.7784\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.4226 Acc: 0.9402\n",
      "val Loss: 0.8953 Acc: 0.7843\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.4344 Acc: 0.9402\n",
      "val Loss: 0.8953 Acc: 0.7853\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.4846 Acc: 0.9265\n",
      "val Loss: 0.8922 Acc: 0.7843\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.4577 Acc: 0.9343\n",
      "val Loss: 0.8946 Acc: 0.7853\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.4690 Acc: 0.9324\n",
      "val Loss: 0.8948 Acc: 0.7873\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.4657 Acc: 0.9275\n",
      "val Loss: 0.8904 Acc: 0.7853\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.4708 Acc: 0.9324\n",
      "val Loss: 0.8985 Acc: 0.7843\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.4822 Acc: 0.9255\n",
      "val Loss: 0.8931 Acc: 0.7824\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.4567 Acc: 0.9294\n",
      "val Loss: 0.8910 Acc: 0.7853\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.5027 Acc: 0.9216\n",
      "val Loss: 0.8953 Acc: 0.7804\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.4778 Acc: 0.9235\n",
      "val Loss: 0.8951 Acc: 0.7843\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.4678 Acc: 0.9235\n",
      "val Loss: 0.8978 Acc: 0.7804\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.4353 Acc: 0.9382\n",
      "val Loss: 0.9005 Acc: 0.7873\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.4624 Acc: 0.9373\n",
      "val Loss: 0.8950 Acc: 0.7853\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.5059 Acc: 0.9245\n",
      "val Loss: 0.8988 Acc: 0.7784\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.4521 Acc: 0.9324\n",
      "val Loss: 0.8965 Acc: 0.7843\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.4574 Acc: 0.9275\n",
      "val Loss: 0.8934 Acc: 0.7892\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.4483 Acc: 0.9275\n",
      "val Loss: 0.8959 Acc: 0.7902\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.4670 Acc: 0.9324\n",
      "val Loss: 0.8963 Acc: 0.7853\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.4638 Acc: 0.9314\n",
      "val Loss: 0.8945 Acc: 0.7814\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.4829 Acc: 0.9245\n",
      "val Loss: 0.8967 Acc: 0.7833\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.4482 Acc: 0.9314\n",
      "val Loss: 0.8974 Acc: 0.7814\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.4182 Acc: 0.9412\n",
      "val Loss: 0.8934 Acc: 0.7833\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.5033 Acc: 0.9225\n",
      "val Loss: 0.8965 Acc: 0.7853\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.4835 Acc: 0.9314\n",
      "val Loss: 0.8936 Acc: 0.7882\n",
      "\n",
      "Training complete in 54m 1s\n",
      "Best val Acc: 0.790196\n"
     ]
    }
   ],
   "source": [
    "model_ft = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "# Reset final FC to the number of classes in the target dataset :\n",
    "model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "#Finetune by seeting the same value of hyperparameters for all the layers.\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 200\n",
    "# Decay LR every total_num_epochs/4 so that we have 3 drops\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=num_epochs/4, gamma=0.1)  ## step size 100000 so that we do not use the decay\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft,exp_lr_scheduler, \n",
    "                       num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train Loss: 0.4835 Acc: 0.9314\n",
    "\n",
    "val Loss: 0.8936 Acc: 0.7882\n",
    "\n",
    "Training complete in 54m 1s\n",
    "\n",
    "Best val Acc: 0.790196"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0ybOhDXY1kT"
   },
   "source": [
    "# c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTBZrnz8Y4A9"
   },
   "source": [
    "# lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "p5qQ-oQ7Yp7S",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4be1ee7c-320c-4612-b314-c7f6602c14ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 4.6278 Acc: 0.0206\n",
      "val Loss: 4.1709 Acc: 0.1255\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 3.7576 Acc: 0.2029\n",
      "val Loss: 2.8125 Acc: 0.3843\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 2.6949 Acc: 0.4029\n",
      "val Loss: 1.9472 Acc: 0.5069\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 1.9877 Acc: 0.5363\n",
      "val Loss: 1.7059 Acc: 0.5745\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 1.4526 Acc: 0.6637\n",
      "val Loss: 1.3538 Acc: 0.6471\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 1.1572 Acc: 0.7206\n",
      "val Loss: 1.2084 Acc: 0.6843\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.8761 Acc: 0.7922\n",
      "val Loss: 1.0267 Acc: 0.7363\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.7118 Acc: 0.8275\n",
      "val Loss: 1.0466 Acc: 0.7137\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.6163 Acc: 0.8657\n",
      "val Loss: 1.0021 Acc: 0.7422\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.5093 Acc: 0.8794\n",
      "val Loss: 0.9810 Acc: 0.7363\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.5455 Acc: 0.8667\n",
      "val Loss: 0.9689 Acc: 0.7549\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.4290 Acc: 0.9049\n",
      "val Loss: 0.9863 Acc: 0.7422\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.4115 Acc: 0.9078\n",
      "val Loss: 0.9622 Acc: 0.7490\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.3505 Acc: 0.9098\n",
      "val Loss: 0.9750 Acc: 0.7451\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.3532 Acc: 0.9196\n",
      "val Loss: 0.8828 Acc: 0.7637\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.3869 Acc: 0.9098\n",
      "val Loss: 0.9297 Acc: 0.7578\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.3542 Acc: 0.9157\n",
      "val Loss: 0.9079 Acc: 0.7559\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.3440 Acc: 0.9216\n",
      "val Loss: 0.8872 Acc: 0.7765\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.3237 Acc: 0.9176\n",
      "val Loss: 0.8801 Acc: 0.7794\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.3392 Acc: 0.9196\n",
      "val Loss: 0.9346 Acc: 0.7637\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.3004 Acc: 0.9235\n",
      "val Loss: 0.9433 Acc: 0.7608\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.2745 Acc: 0.9333\n",
      "val Loss: 0.8350 Acc: 0.7863\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.2371 Acc: 0.9402\n",
      "val Loss: 0.8323 Acc: 0.7745\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.2865 Acc: 0.9235\n",
      "val Loss: 0.9042 Acc: 0.7618\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.2668 Acc: 0.9284\n",
      "val Loss: 0.9133 Acc: 0.7716\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.2717 Acc: 0.9333\n",
      "val Loss: 0.9216 Acc: 0.7794\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.3145 Acc: 0.9167\n",
      "val Loss: 0.9111 Acc: 0.7716\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.1854 Acc: 0.9569\n",
      "val Loss: 0.9822 Acc: 0.7588\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.2176 Acc: 0.9461\n",
      "val Loss: 0.8587 Acc: 0.7892\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.2268 Acc: 0.9392\n",
      "val Loss: 0.8513 Acc: 0.7873\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.2479 Acc: 0.9382\n",
      "val Loss: 0.9262 Acc: 0.7696\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.2291 Acc: 0.9441\n",
      "val Loss: 1.0067 Acc: 0.7627\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.1924 Acc: 0.9461\n",
      "val Loss: 0.9501 Acc: 0.7755\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.2149 Acc: 0.9480\n",
      "val Loss: 0.8970 Acc: 0.7735\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.1496 Acc: 0.9647\n",
      "val Loss: 0.8727 Acc: 0.7990\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.1886 Acc: 0.9549\n",
      "val Loss: 0.9287 Acc: 0.7755\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.1857 Acc: 0.9569\n",
      "val Loss: 0.8987 Acc: 0.7931\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.1856 Acc: 0.9549\n",
      "val Loss: 0.8751 Acc: 0.8059\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.2053 Acc: 0.9539\n",
      "val Loss: 0.9362 Acc: 0.7696\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.2256 Acc: 0.9412\n",
      "val Loss: 1.0115 Acc: 0.7588\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.1790 Acc: 0.9529\n",
      "val Loss: 0.9388 Acc: 0.7578\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.2067 Acc: 0.9529\n",
      "val Loss: 0.9507 Acc: 0.7696\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.2590 Acc: 0.9373\n",
      "val Loss: 0.8950 Acc: 0.7804\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.2233 Acc: 0.9441\n",
      "val Loss: 0.9987 Acc: 0.7520\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.2429 Acc: 0.9431\n",
      "val Loss: 0.8817 Acc: 0.7804\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.1701 Acc: 0.9578\n",
      "val Loss: 0.8245 Acc: 0.7980\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.1866 Acc: 0.9500\n",
      "val Loss: 0.8869 Acc: 0.7863\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.1729 Acc: 0.9539\n",
      "val Loss: 0.8827 Acc: 0.7863\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.2039 Acc: 0.9480\n",
      "val Loss: 0.9402 Acc: 0.7853\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.1645 Acc: 0.9569\n",
      "val Loss: 0.9433 Acc: 0.7824\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.1619 Acc: 0.9588\n",
      "val Loss: 0.8519 Acc: 0.7971\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.1311 Acc: 0.9667\n",
      "val Loss: 0.8173 Acc: 0.8020\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.1413 Acc: 0.9608\n",
      "val Loss: 0.7971 Acc: 0.8049\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.1068 Acc: 0.9725\n",
      "val Loss: 0.7894 Acc: 0.8059\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.1162 Acc: 0.9696\n",
      "val Loss: 0.7852 Acc: 0.8059\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.1126 Acc: 0.9716\n",
      "val Loss: 0.7790 Acc: 0.8088\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.1126 Acc: 0.9735\n",
      "val Loss: 0.7706 Acc: 0.8098\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.0961 Acc: 0.9794\n",
      "val Loss: 0.7620 Acc: 0.8127\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.0967 Acc: 0.9725\n",
      "val Loss: 0.7672 Acc: 0.8108\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.1380 Acc: 0.9647\n",
      "val Loss: 0.7561 Acc: 0.8147\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.0885 Acc: 0.9784\n",
      "val Loss: 0.7508 Acc: 0.8118\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.0842 Acc: 0.9804\n",
      "val Loss: 0.7514 Acc: 0.8127\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.0813 Acc: 0.9784\n",
      "val Loss: 0.7522 Acc: 0.8088\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.0983 Acc: 0.9716\n",
      "val Loss: 0.7547 Acc: 0.8137\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.0678 Acc: 0.9882\n",
      "val Loss: 0.7474 Acc: 0.8167\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.0800 Acc: 0.9794\n",
      "val Loss: 0.7481 Acc: 0.8196\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.0971 Acc: 0.9765\n",
      "val Loss: 0.7390 Acc: 0.8196\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.0865 Acc: 0.9784\n",
      "val Loss: 0.7380 Acc: 0.8137\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.0879 Acc: 0.9794\n",
      "val Loss: 0.7326 Acc: 0.8147\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.0925 Acc: 0.9794\n",
      "val Loss: 0.7310 Acc: 0.8196\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.0896 Acc: 0.9765\n",
      "val Loss: 0.7378 Acc: 0.8186\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.0815 Acc: 0.9814\n",
      "val Loss: 0.7332 Acc: 0.8147\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.0871 Acc: 0.9804\n",
      "val Loss: 0.7350 Acc: 0.8196\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.0704 Acc: 0.9824\n",
      "val Loss: 0.7318 Acc: 0.8186\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.0842 Acc: 0.9765\n",
      "val Loss: 0.7293 Acc: 0.8206\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.0892 Acc: 0.9833\n",
      "val Loss: 0.7268 Acc: 0.8216\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.0963 Acc: 0.9716\n",
      "val Loss: 0.7300 Acc: 0.8206\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.0677 Acc: 0.9863\n",
      "val Loss: 0.7259 Acc: 0.8245\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.0645 Acc: 0.9843\n",
      "val Loss: 0.7297 Acc: 0.8216\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.0796 Acc: 0.9794\n",
      "val Loss: 0.7264 Acc: 0.8196\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.0851 Acc: 0.9814\n",
      "val Loss: 0.7295 Acc: 0.8176\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.0725 Acc: 0.9814\n",
      "val Loss: 0.7320 Acc: 0.8216\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.0599 Acc: 0.9843\n",
      "val Loss: 0.7264 Acc: 0.8225\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.0929 Acc: 0.9775\n",
      "val Loss: 0.7067 Acc: 0.8216\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.0942 Acc: 0.9794\n",
      "val Loss: 0.7089 Acc: 0.8206\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.0679 Acc: 0.9833\n",
      "val Loss: 0.7199 Acc: 0.8225\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.0748 Acc: 0.9824\n",
      "val Loss: 0.7240 Acc: 0.8275\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.0824 Acc: 0.9824\n",
      "val Loss: 0.7166 Acc: 0.8245\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.0787 Acc: 0.9824\n",
      "val Loss: 0.7125 Acc: 0.8225\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.0633 Acc: 0.9833\n",
      "val Loss: 0.7152 Acc: 0.8235\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.0769 Acc: 0.9765\n",
      "val Loss: 0.7172 Acc: 0.8265\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.0870 Acc: 0.9794\n",
      "val Loss: 0.7155 Acc: 0.8255\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.0711 Acc: 0.9794\n",
      "val Loss: 0.7144 Acc: 0.8206\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.0967 Acc: 0.9676\n",
      "val Loss: 0.7238 Acc: 0.8176\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.0624 Acc: 0.9794\n",
      "val Loss: 0.7232 Acc: 0.8127\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.0864 Acc: 0.9814\n",
      "val Loss: 0.7194 Acc: 0.8176\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.0884 Acc: 0.9833\n",
      "val Loss: 0.7171 Acc: 0.8186\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.0719 Acc: 0.9794\n",
      "val Loss: 0.7241 Acc: 0.8245\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.0713 Acc: 0.9833\n",
      "val Loss: 0.7178 Acc: 0.8216\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.0695 Acc: 0.9804\n",
      "val Loss: 0.7202 Acc: 0.8206\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.0650 Acc: 0.9784\n",
      "val Loss: 0.7178 Acc: 0.8206\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.0838 Acc: 0.9784\n",
      "val Loss: 0.7154 Acc: 0.8196\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.0992 Acc: 0.9716\n",
      "val Loss: 0.7220 Acc: 0.8225\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.0753 Acc: 0.9833\n",
      "val Loss: 0.7187 Acc: 0.8216\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.0865 Acc: 0.9775\n",
      "val Loss: 0.7180 Acc: 0.8216\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.0644 Acc: 0.9843\n",
      "val Loss: 0.7183 Acc: 0.8186\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.0739 Acc: 0.9804\n",
      "val Loss: 0.7155 Acc: 0.8245\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.0941 Acc: 0.9755\n",
      "val Loss: 0.7136 Acc: 0.8235\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.0826 Acc: 0.9814\n",
      "val Loss: 0.7128 Acc: 0.8216\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.0769 Acc: 0.9804\n",
      "val Loss: 0.7168 Acc: 0.8216\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.0615 Acc: 0.9882\n",
      "val Loss: 0.7133 Acc: 0.8216\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.0589 Acc: 0.9873\n",
      "val Loss: 0.7143 Acc: 0.8196\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.0889 Acc: 0.9784\n",
      "val Loss: 0.7154 Acc: 0.8225\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.0486 Acc: 0.9892\n",
      "val Loss: 0.7179 Acc: 0.8206\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.0874 Acc: 0.9784\n",
      "val Loss: 0.7167 Acc: 0.8245\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.0562 Acc: 0.9902\n",
      "val Loss: 0.7171 Acc: 0.8206\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.0644 Acc: 0.9824\n",
      "val Loss: 0.7198 Acc: 0.8216\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.0729 Acc: 0.9833\n",
      "val Loss: 0.7088 Acc: 0.8206\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.0614 Acc: 0.9863\n",
      "val Loss: 0.7198 Acc: 0.8225\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.0482 Acc: 0.9853\n",
      "val Loss: 0.7116 Acc: 0.8225\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.0775 Acc: 0.9784\n",
      "val Loss: 0.7107 Acc: 0.8157\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.0761 Acc: 0.9804\n",
      "val Loss: 0.7160 Acc: 0.8225\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.0742 Acc: 0.9824\n",
      "val Loss: 0.7184 Acc: 0.8206\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.0603 Acc: 0.9863\n",
      "val Loss: 0.7129 Acc: 0.8206\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.0687 Acc: 0.9794\n",
      "val Loss: 0.7135 Acc: 0.8196\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.0719 Acc: 0.9804\n",
      "val Loss: 0.7174 Acc: 0.8206\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.0854 Acc: 0.9784\n",
      "val Loss: 0.7162 Acc: 0.8186\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.0682 Acc: 0.9814\n",
      "val Loss: 0.7130 Acc: 0.8225\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.0797 Acc: 0.9794\n",
      "val Loss: 0.7140 Acc: 0.8216\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.0529 Acc: 0.9882\n",
      "val Loss: 0.7187 Acc: 0.8225\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.1001 Acc: 0.9755\n",
      "val Loss: 0.7118 Acc: 0.8186\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.0526 Acc: 0.9873\n",
      "val Loss: 0.7101 Acc: 0.8167\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.0810 Acc: 0.9824\n",
      "val Loss: 0.7152 Acc: 0.8186\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.0958 Acc: 0.9804\n",
      "val Loss: 0.7148 Acc: 0.8186\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.0687 Acc: 0.9833\n",
      "val Loss: 0.7115 Acc: 0.8176\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.0765 Acc: 0.9794\n",
      "val Loss: 0.7125 Acc: 0.8225\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.0745 Acc: 0.9824\n",
      "val Loss: 0.7090 Acc: 0.8167\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.0701 Acc: 0.9784\n",
      "val Loss: 0.7112 Acc: 0.8235\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.0873 Acc: 0.9775\n",
      "val Loss: 0.7147 Acc: 0.8235\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.0774 Acc: 0.9775\n",
      "val Loss: 0.7101 Acc: 0.8206\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.0776 Acc: 0.9824\n",
      "val Loss: 0.7118 Acc: 0.8225\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.0719 Acc: 0.9735\n",
      "val Loss: 0.7118 Acc: 0.8206\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.0643 Acc: 0.9853\n",
      "val Loss: 0.7124 Acc: 0.8206\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.0679 Acc: 0.9794\n",
      "val Loss: 0.7123 Acc: 0.8157\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.0747 Acc: 0.9784\n",
      "val Loss: 0.7157 Acc: 0.8196\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.0699 Acc: 0.9833\n",
      "val Loss: 0.7168 Acc: 0.8216\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.0893 Acc: 0.9735\n",
      "val Loss: 0.7092 Acc: 0.8196\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.0518 Acc: 0.9882\n",
      "val Loss: 0.7149 Acc: 0.8206\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.0736 Acc: 0.9794\n",
      "val Loss: 0.7144 Acc: 0.8186\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.0657 Acc: 0.9863\n",
      "val Loss: 0.7143 Acc: 0.8186\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.0663 Acc: 0.9833\n",
      "val Loss: 0.7162 Acc: 0.8216\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.0498 Acc: 0.9863\n",
      "val Loss: 0.7105 Acc: 0.8235\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.0584 Acc: 0.9853\n",
      "val Loss: 0.7150 Acc: 0.8235\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.0681 Acc: 0.9824\n",
      "val Loss: 0.7133 Acc: 0.8186\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.0556 Acc: 0.9863\n",
      "val Loss: 0.7158 Acc: 0.8186\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.0726 Acc: 0.9824\n",
      "val Loss: 0.7110 Acc: 0.8216\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.0755 Acc: 0.9814\n",
      "val Loss: 0.7092 Acc: 0.8167\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.0841 Acc: 0.9775\n",
      "val Loss: 0.7114 Acc: 0.8157\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.0628 Acc: 0.9873\n",
      "val Loss: 0.7165 Acc: 0.8216\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.0702 Acc: 0.9824\n",
      "val Loss: 0.7130 Acc: 0.8186\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.0579 Acc: 0.9882\n",
      "val Loss: 0.7175 Acc: 0.8176\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.0704 Acc: 0.9824\n",
      "val Loss: 0.7177 Acc: 0.8196\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.0628 Acc: 0.9843\n",
      "val Loss: 0.7145 Acc: 0.8206\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.0689 Acc: 0.9814\n",
      "val Loss: 0.7212 Acc: 0.8176\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.0958 Acc: 0.9716\n",
      "val Loss: 0.7142 Acc: 0.8216\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.0791 Acc: 0.9804\n",
      "val Loss: 0.7133 Acc: 0.8147\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.0692 Acc: 0.9824\n",
      "val Loss: 0.7134 Acc: 0.8216\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.0748 Acc: 0.9824\n",
      "val Loss: 0.7150 Acc: 0.8186\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.0851 Acc: 0.9804\n",
      "val Loss: 0.7163 Acc: 0.8186\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.0888 Acc: 0.9804\n",
      "val Loss: 0.7143 Acc: 0.8186\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.0644 Acc: 0.9814\n",
      "val Loss: 0.7116 Acc: 0.8196\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.0626 Acc: 0.9863\n",
      "val Loss: 0.7176 Acc: 0.8225\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.0484 Acc: 0.9892\n",
      "val Loss: 0.7084 Acc: 0.8196\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.0671 Acc: 0.9824\n",
      "val Loss: 0.7118 Acc: 0.8255\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.0906 Acc: 0.9814\n",
      "val Loss: 0.7154 Acc: 0.8216\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.0648 Acc: 0.9853\n",
      "val Loss: 0.7061 Acc: 0.8225\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.0784 Acc: 0.9794\n",
      "val Loss: 0.7146 Acc: 0.8216\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.0751 Acc: 0.9843\n",
      "val Loss: 0.7134 Acc: 0.8196\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.0708 Acc: 0.9833\n",
      "val Loss: 0.7104 Acc: 0.8196\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.0719 Acc: 0.9833\n",
      "val Loss: 0.7116 Acc: 0.8225\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.0556 Acc: 0.9902\n",
      "val Loss: 0.7140 Acc: 0.8216\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.0673 Acc: 0.9833\n",
      "val Loss: 0.7150 Acc: 0.8206\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.0683 Acc: 0.9833\n",
      "val Loss: 0.7106 Acc: 0.8225\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.0729 Acc: 0.9814\n",
      "val Loss: 0.7085 Acc: 0.8196\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.0856 Acc: 0.9824\n",
      "val Loss: 0.7108 Acc: 0.8196\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.0699 Acc: 0.9814\n",
      "val Loss: 0.7112 Acc: 0.8196\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.0531 Acc: 0.9882\n",
      "val Loss: 0.7110 Acc: 0.8206\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.0612 Acc: 0.9853\n",
      "val Loss: 0.7130 Acc: 0.8206\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.0697 Acc: 0.9873\n",
      "val Loss: 0.7141 Acc: 0.8225\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.0641 Acc: 0.9824\n",
      "val Loss: 0.7156 Acc: 0.8196\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.0733 Acc: 0.9863\n",
      "val Loss: 0.7128 Acc: 0.8186\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.0433 Acc: 0.9882\n",
      "val Loss: 0.7147 Acc: 0.8206\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.0915 Acc: 0.9755\n",
      "val Loss: 0.7102 Acc: 0.8196\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.0471 Acc: 0.9931\n",
      "val Loss: 0.7131 Acc: 0.8235\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.0734 Acc: 0.9833\n",
      "val Loss: 0.7126 Acc: 0.8206\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.0638 Acc: 0.9843\n",
      "val Loss: 0.7154 Acc: 0.8196\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.0606 Acc: 0.9873\n",
      "val Loss: 0.7119 Acc: 0.8225\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.0660 Acc: 0.9873\n",
      "val Loss: 0.7125 Acc: 0.8176\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.0915 Acc: 0.9745\n",
      "val Loss: 0.7126 Acc: 0.8196\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.0797 Acc: 0.9784\n",
      "val Loss: 0.7133 Acc: 0.8206\n",
      "\n",
      "Training complete in 52m 56s\n",
      "Best val Acc: 0.827451\n"
     ]
    }
   ],
   "source": [
    "# c) lr = 0.01\n",
    "model_ft = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "# Reset final FC to the number of classes in the target dataset :\n",
    "model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "num_epochs = 200\n",
    "# Decay LR every total_num_epochs/4 so that we have 3 drops\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=num_epochs/4, gamma=0.1)  ## step size 100000 so that we do not use the decay\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft,exp_lr_scheduler, \n",
    "                       num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train Loss: 0.0797 Acc: 0.9784\n",
    "\n",
    "val Loss: 0.7133 Acc: 0.8206\n",
    "\n",
    "Training complete in 52m 56s\n",
    "\n",
    "Best val Acc: 0.827451"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqELJnOSQm0-"
   },
   "source": [
    "# lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "qH9XLc_QZDwI",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "3495e86d-9e8b-4259-b4ff-f87f93025b14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 5.0843 Acc: 0.0245\n",
      "val Loss: 35783.4348 Acc: 0.0098\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 4.7837 Acc: 0.0186\n",
      "val Loss: 85.7471 Acc: 0.0127\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 4.3425 Acc: 0.0314\n",
      "val Loss: 5.1301 Acc: 0.0118\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 4.1210 Acc: 0.0333\n",
      "val Loss: 4.4133 Acc: 0.0422\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 3.9462 Acc: 0.0588\n",
      "val Loss: 3.9545 Acc: 0.0814\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 3.7626 Acc: 0.0735\n",
      "val Loss: 3.8325 Acc: 0.0951\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 3.6302 Acc: 0.0912\n",
      "val Loss: 3.7555 Acc: 0.1049\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 3.5863 Acc: 0.1118\n",
      "val Loss: 3.5184 Acc: 0.1461\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 3.4172 Acc: 0.1284\n",
      "val Loss: 3.5115 Acc: 0.1598\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 3.3322 Acc: 0.1471\n",
      "val Loss: 3.4257 Acc: 0.1912\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 3.2757 Acc: 0.1608\n",
      "val Loss: 5.2219 Acc: 0.1333\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 3.2371 Acc: 0.1941\n",
      "val Loss: 3.4358 Acc: 0.1725\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 3.1545 Acc: 0.1824\n",
      "val Loss: 3.6245 Acc: 0.1578\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 3.0734 Acc: 0.1980\n",
      "val Loss: 3.3255 Acc: 0.2196\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 2.9886 Acc: 0.2206\n",
      "val Loss: 3.3242 Acc: 0.2127\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 2.9133 Acc: 0.2324\n",
      "val Loss: 3.1917 Acc: 0.2412\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 2.7676 Acc: 0.2618\n",
      "val Loss: 3.2230 Acc: 0.2539\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 2.9250 Acc: 0.2422\n",
      "val Loss: 3.6421 Acc: 0.2078\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 2.7598 Acc: 0.2647\n",
      "val Loss: 3.2831 Acc: 0.2127\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 2.7479 Acc: 0.2588\n",
      "val Loss: 3.2040 Acc: 0.2627\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 2.6056 Acc: 0.3059\n",
      "val Loss: 3.5225 Acc: 0.2098\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 2.4378 Acc: 0.3225\n",
      "val Loss: 3.1798 Acc: 0.2824\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 2.6129 Acc: 0.3000\n",
      "val Loss: 3.3054 Acc: 0.2324\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 2.4569 Acc: 0.3431\n",
      "val Loss: 3.3361 Acc: 0.2402\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 2.3472 Acc: 0.3667\n",
      "val Loss: 3.4092 Acc: 0.2647\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 2.3067 Acc: 0.3725\n",
      "val Loss: 3.5008 Acc: 0.2275\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 2.2979 Acc: 0.3520\n",
      "val Loss: 3.1458 Acc: 0.2912\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 2.1998 Acc: 0.4029\n",
      "val Loss: 9.1557 Acc: 0.1333\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 2.2172 Acc: 0.3882\n",
      "val Loss: 3.8191 Acc: 0.2490\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 2.1630 Acc: 0.3951\n",
      "val Loss: 2.9719 Acc: 0.3265\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 2.0910 Acc: 0.4186\n",
      "val Loss: 3.5569 Acc: 0.2520\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 1.9752 Acc: 0.4412\n",
      "val Loss: 3.4693 Acc: 0.2912\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 1.8730 Acc: 0.4755\n",
      "val Loss: 3.1503 Acc: 0.3284\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 1.9123 Acc: 0.4569\n",
      "val Loss: 3.0295 Acc: 0.3196\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 1.8100 Acc: 0.4971\n",
      "val Loss: 2.8951 Acc: 0.3529\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 1.6959 Acc: 0.5020\n",
      "val Loss: 3.0996 Acc: 0.3402\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 1.7516 Acc: 0.4882\n",
      "val Loss: 3.2283 Acc: 0.3471\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 1.6987 Acc: 0.5363\n",
      "val Loss: 3.1281 Acc: 0.3314\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 1.6486 Acc: 0.5225\n",
      "val Loss: 3.1514 Acc: 0.3471\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 1.5876 Acc: 0.5343\n",
      "val Loss: 3.0451 Acc: 0.3686\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 1.6071 Acc: 0.5108\n",
      "val Loss: 3.5604 Acc: 0.3098\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 1.4857 Acc: 0.5490\n",
      "val Loss: 3.1762 Acc: 0.3402\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 1.5967 Acc: 0.5539\n",
      "val Loss: 3.3484 Acc: 0.3314\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 1.3497 Acc: 0.5971\n",
      "val Loss: 2.9993 Acc: 0.3922\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 1.3296 Acc: 0.6176\n",
      "val Loss: 3.2074 Acc: 0.3618\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 1.3459 Acc: 0.6118\n",
      "val Loss: 3.2888 Acc: 0.3667\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 1.3809 Acc: 0.5951\n",
      "val Loss: 3.9954 Acc: 0.2824\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 1.2861 Acc: 0.6098\n",
      "val Loss: 3.1827 Acc: 0.3686\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 1.2948 Acc: 0.6235\n",
      "val Loss: 3.1628 Acc: 0.3814\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 1.2335 Acc: 0.6373\n",
      "val Loss: 3.0762 Acc: 0.3833\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.9225 Acc: 0.7265\n",
      "val Loss: 2.6900 Acc: 0.4392\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.8485 Acc: 0.7667\n",
      "val Loss: 2.5879 Acc: 0.4696\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.7207 Acc: 0.8020\n",
      "val Loss: 2.6157 Acc: 0.4745\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.6799 Acc: 0.8167\n",
      "val Loss: 2.6582 Acc: 0.4765\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.6723 Acc: 0.8196\n",
      "val Loss: 2.6815 Acc: 0.4794\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.7049 Acc: 0.8147\n",
      "val Loss: 2.7350 Acc: 0.4824\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.6255 Acc: 0.8245\n",
      "val Loss: 2.7699 Acc: 0.4873\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.6264 Acc: 0.8304\n",
      "val Loss: 2.7023 Acc: 0.4824\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.5750 Acc: 0.8402\n",
      "val Loss: 2.7059 Acc: 0.4863\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.6474 Acc: 0.8167\n",
      "val Loss: 2.7378 Acc: 0.4676\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.5923 Acc: 0.8324\n",
      "val Loss: 2.7705 Acc: 0.4775\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.5488 Acc: 0.8431\n",
      "val Loss: 2.8501 Acc: 0.4755\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.5778 Acc: 0.8402\n",
      "val Loss: 2.8135 Acc: 0.4814\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.5717 Acc: 0.8461\n",
      "val Loss: 2.9397 Acc: 0.4775\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.5499 Acc: 0.8500\n",
      "val Loss: 2.8898 Acc: 0.4755\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.5696 Acc: 0.8373\n",
      "val Loss: 2.8970 Acc: 0.4833\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.5001 Acc: 0.8667\n",
      "val Loss: 2.8698 Acc: 0.4833\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.4693 Acc: 0.8843\n",
      "val Loss: 2.8852 Acc: 0.4833\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.4899 Acc: 0.8588\n",
      "val Loss: 2.8725 Acc: 0.4882\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.4942 Acc: 0.8667\n",
      "val Loss: 2.9467 Acc: 0.4882\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.5359 Acc: 0.8637\n",
      "val Loss: 2.9604 Acc: 0.4853\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.5323 Acc: 0.8500\n",
      "val Loss: 3.0223 Acc: 0.4814\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.5362 Acc: 0.8441\n",
      "val Loss: 3.0421 Acc: 0.4755\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.4822 Acc: 0.8549\n",
      "val Loss: 3.0226 Acc: 0.4873\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.4944 Acc: 0.8627\n",
      "val Loss: 3.1386 Acc: 0.4833\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.5026 Acc: 0.8461\n",
      "val Loss: 3.0499 Acc: 0.4765\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.5082 Acc: 0.8618\n",
      "val Loss: 3.0340 Acc: 0.4804\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.4059 Acc: 0.8843\n",
      "val Loss: 3.0060 Acc: 0.4902\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.3819 Acc: 0.8882\n",
      "val Loss: 3.1028 Acc: 0.4873\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.4762 Acc: 0.8667\n",
      "val Loss: 3.1622 Acc: 0.4804\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.3983 Acc: 0.8902\n",
      "val Loss: 3.1835 Acc: 0.4824\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.4130 Acc: 0.8882\n",
      "val Loss: 3.1280 Acc: 0.4853\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.3750 Acc: 0.8951\n",
      "val Loss: 3.0917 Acc: 0.4931\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.4015 Acc: 0.8804\n",
      "val Loss: 3.2057 Acc: 0.4873\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.4229 Acc: 0.8882\n",
      "val Loss: 3.1623 Acc: 0.4794\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.3940 Acc: 0.8941\n",
      "val Loss: 3.2126 Acc: 0.4765\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.3733 Acc: 0.8980\n",
      "val Loss: 3.1685 Acc: 0.4892\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.4178 Acc: 0.8765\n",
      "val Loss: 3.1865 Acc: 0.4775\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.3751 Acc: 0.8873\n",
      "val Loss: 3.1779 Acc: 0.4814\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.4154 Acc: 0.8912\n",
      "val Loss: 3.3036 Acc: 0.4716\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.4036 Acc: 0.8784\n",
      "val Loss: 3.2392 Acc: 0.4843\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.4296 Acc: 0.8794\n",
      "val Loss: 3.1125 Acc: 0.4902\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.3655 Acc: 0.8961\n",
      "val Loss: 3.1935 Acc: 0.4833\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.4043 Acc: 0.8833\n",
      "val Loss: 3.1584 Acc: 0.4882\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.3342 Acc: 0.9049\n",
      "val Loss: 3.2528 Acc: 0.4824\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.3953 Acc: 0.8784\n",
      "val Loss: 3.2434 Acc: 0.4951\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.4527 Acc: 0.8765\n",
      "val Loss: 3.2152 Acc: 0.4775\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.3932 Acc: 0.8912\n",
      "val Loss: 3.2137 Acc: 0.4882\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.3867 Acc: 0.8873\n",
      "val Loss: 3.2040 Acc: 0.4951\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.3096 Acc: 0.9127\n",
      "val Loss: 3.2131 Acc: 0.4922\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.3644 Acc: 0.9000\n",
      "val Loss: 3.1760 Acc: 0.4931\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.3004 Acc: 0.9069\n",
      "val Loss: 3.1535 Acc: 0.4931\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.3334 Acc: 0.9118\n",
      "val Loss: 3.1803 Acc: 0.4912\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.3195 Acc: 0.9186\n",
      "val Loss: 3.2387 Acc: 0.4922\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.3306 Acc: 0.9059\n",
      "val Loss: 3.2036 Acc: 0.4912\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.2950 Acc: 0.9167\n",
      "val Loss: 3.2566 Acc: 0.4902\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.2533 Acc: 0.9284\n",
      "val Loss: 3.2405 Acc: 0.4971\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.2604 Acc: 0.9324\n",
      "val Loss: 3.2978 Acc: 0.4902\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.3473 Acc: 0.8951\n",
      "val Loss: 3.3025 Acc: 0.4931\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.2980 Acc: 0.9147\n",
      "val Loss: 3.3317 Acc: 0.4833\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.3442 Acc: 0.9127\n",
      "val Loss: 3.3237 Acc: 0.4873\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.3271 Acc: 0.9088\n",
      "val Loss: 3.2797 Acc: 0.4824\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.3358 Acc: 0.9137\n",
      "val Loss: 3.3019 Acc: 0.4882\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.3108 Acc: 0.9225\n",
      "val Loss: 3.2628 Acc: 0.4892\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.3122 Acc: 0.9098\n",
      "val Loss: 3.2464 Acc: 0.4951\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.3301 Acc: 0.9127\n",
      "val Loss: 3.2841 Acc: 0.4931\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.2983 Acc: 0.9206\n",
      "val Loss: 3.2576 Acc: 0.4961\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.3063 Acc: 0.9235\n",
      "val Loss: 3.2627 Acc: 0.4971\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.2835 Acc: 0.9186\n",
      "val Loss: 3.2430 Acc: 0.4922\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.2422 Acc: 0.9402\n",
      "val Loss: 3.2446 Acc: 0.4941\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.2592 Acc: 0.9294\n",
      "val Loss: 3.2989 Acc: 0.4941\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.3043 Acc: 0.9127\n",
      "val Loss: 3.2504 Acc: 0.4931\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.2968 Acc: 0.9176\n",
      "val Loss: 3.3131 Acc: 0.4912\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.2609 Acc: 0.9265\n",
      "val Loss: 3.2785 Acc: 0.4922\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.2771 Acc: 0.9186\n",
      "val Loss: 3.3514 Acc: 0.4922\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.2993 Acc: 0.9127\n",
      "val Loss: 3.3098 Acc: 0.4922\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.2613 Acc: 0.9265\n",
      "val Loss: 3.3196 Acc: 0.4971\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.2935 Acc: 0.9294\n",
      "val Loss: 3.3253 Acc: 0.4902\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.3188 Acc: 0.9108\n",
      "val Loss: 3.3526 Acc: 0.4980\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.2731 Acc: 0.9314\n",
      "val Loss: 3.3250 Acc: 0.4922\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.2920 Acc: 0.9225\n",
      "val Loss: 3.3348 Acc: 0.4961\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.2608 Acc: 0.9245\n",
      "val Loss: 3.3076 Acc: 0.4912\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.2823 Acc: 0.9265\n",
      "val Loss: 3.3242 Acc: 0.4931\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.2563 Acc: 0.9333\n",
      "val Loss: 3.3326 Acc: 0.4980\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.2615 Acc: 0.9245\n",
      "val Loss: 3.3227 Acc: 0.4941\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.2928 Acc: 0.9196\n",
      "val Loss: 3.3650 Acc: 0.4951\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.2954 Acc: 0.9343\n",
      "val Loss: 3.3799 Acc: 0.4980\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.2495 Acc: 0.9304\n",
      "val Loss: 3.3654 Acc: 0.4951\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.2770 Acc: 0.9225\n",
      "val Loss: 3.3772 Acc: 0.4912\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.2901 Acc: 0.9225\n",
      "val Loss: 3.3637 Acc: 0.4941\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.3183 Acc: 0.9059\n",
      "val Loss: 3.3980 Acc: 0.4961\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.2892 Acc: 0.9284\n",
      "val Loss: 3.3518 Acc: 0.4922\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.2566 Acc: 0.9304\n",
      "val Loss: 3.3542 Acc: 0.4912\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.3100 Acc: 0.9118\n",
      "val Loss: 3.3870 Acc: 0.4922\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.3098 Acc: 0.9147\n",
      "val Loss: 3.3864 Acc: 0.4941\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.3334 Acc: 0.9049\n",
      "val Loss: 3.3990 Acc: 0.4873\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.2832 Acc: 0.9245\n",
      "val Loss: 3.4015 Acc: 0.4912\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.2615 Acc: 0.9324\n",
      "val Loss: 3.3779 Acc: 0.4882\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.2895 Acc: 0.9225\n",
      "val Loss: 3.3533 Acc: 0.4931\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.2286 Acc: 0.9363\n",
      "val Loss: 3.3777 Acc: 0.4980\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.3151 Acc: 0.9196\n",
      "val Loss: 3.3869 Acc: 0.4892\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.2439 Acc: 0.9343\n",
      "val Loss: 3.3611 Acc: 0.4922\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.2579 Acc: 0.9304\n",
      "val Loss: 3.3475 Acc: 0.4951\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.2614 Acc: 0.9255\n",
      "val Loss: 3.3695 Acc: 0.4961\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.2465 Acc: 0.9333\n",
      "val Loss: 3.3501 Acc: 0.4961\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.3097 Acc: 0.9039\n",
      "val Loss: 3.3956 Acc: 0.4941\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.2463 Acc: 0.9284\n",
      "val Loss: 3.3557 Acc: 0.4931\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.2785 Acc: 0.9216\n",
      "val Loss: 3.4004 Acc: 0.4892\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.2712 Acc: 0.9245\n",
      "val Loss: 3.3919 Acc: 0.4912\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.2922 Acc: 0.9167\n",
      "val Loss: 3.3728 Acc: 0.4961\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.2901 Acc: 0.9235\n",
      "val Loss: 3.3424 Acc: 0.4922\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.2643 Acc: 0.9324\n",
      "val Loss: 3.3987 Acc: 0.4873\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.2705 Acc: 0.9353\n",
      "val Loss: 3.3479 Acc: 0.4902\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.2275 Acc: 0.9333\n",
      "val Loss: 3.3443 Acc: 0.4951\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.2580 Acc: 0.9324\n",
      "val Loss: 3.3542 Acc: 0.4902\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.2452 Acc: 0.9314\n",
      "val Loss: 3.3742 Acc: 0.4912\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.2693 Acc: 0.9216\n",
      "val Loss: 3.3761 Acc: 0.4873\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.2352 Acc: 0.9284\n",
      "val Loss: 3.3521 Acc: 0.4902\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.2519 Acc: 0.9314\n",
      "val Loss: 3.3997 Acc: 0.4941\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.2611 Acc: 0.9235\n",
      "val Loss: 3.3450 Acc: 0.4961\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.2840 Acc: 0.9157\n",
      "val Loss: 3.3948 Acc: 0.4951\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.2739 Acc: 0.9225\n",
      "val Loss: 3.3475 Acc: 0.4990\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.2839 Acc: 0.9225\n",
      "val Loss: 3.3379 Acc: 0.4882\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.2463 Acc: 0.9304\n",
      "val Loss: 3.3517 Acc: 0.4922\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.2376 Acc: 0.9255\n",
      "val Loss: 3.3483 Acc: 0.4882\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.2270 Acc: 0.9353\n",
      "val Loss: 3.3480 Acc: 0.4941\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.2935 Acc: 0.9275\n",
      "val Loss: 3.3666 Acc: 0.5000\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.2365 Acc: 0.9402\n",
      "val Loss: 3.3372 Acc: 0.4941\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.2189 Acc: 0.9402\n",
      "val Loss: 3.3776 Acc: 0.4941\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.2665 Acc: 0.9255\n",
      "val Loss: 3.4108 Acc: 0.4892\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.2490 Acc: 0.9343\n",
      "val Loss: 3.3428 Acc: 0.4941\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.2760 Acc: 0.9245\n",
      "val Loss: 3.3529 Acc: 0.4863\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.3031 Acc: 0.9255\n",
      "val Loss: 3.3515 Acc: 0.4941\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.2657 Acc: 0.9206\n",
      "val Loss: 3.3557 Acc: 0.4882\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.2911 Acc: 0.9196\n",
      "val Loss: 3.3471 Acc: 0.4931\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.2508 Acc: 0.9333\n",
      "val Loss: 3.3875 Acc: 0.4843\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.2560 Acc: 0.9255\n",
      "val Loss: 3.3484 Acc: 0.4922\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.2925 Acc: 0.9216\n",
      "val Loss: 3.3339 Acc: 0.4951\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.2985 Acc: 0.9225\n",
      "val Loss: 3.3794 Acc: 0.4922\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.3185 Acc: 0.9088\n",
      "val Loss: 3.3797 Acc: 0.4882\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.2248 Acc: 0.9382\n",
      "val Loss: 3.3895 Acc: 0.4912\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.2945 Acc: 0.9127\n",
      "val Loss: 3.3668 Acc: 0.4931\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.2754 Acc: 0.9206\n",
      "val Loss: 3.3627 Acc: 0.4902\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.2079 Acc: 0.9471\n",
      "val Loss: 3.3313 Acc: 0.4892\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.2164 Acc: 0.9373\n",
      "val Loss: 3.3477 Acc: 0.4922\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.2285 Acc: 0.9353\n",
      "val Loss: 3.3411 Acc: 0.4931\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.2433 Acc: 0.9324\n",
      "val Loss: 3.3740 Acc: 0.4912\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.2758 Acc: 0.9216\n",
      "val Loss: 3.3719 Acc: 0.4922\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.2446 Acc: 0.9304\n",
      "val Loss: 3.3762 Acc: 0.4931\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.2352 Acc: 0.9402\n",
      "val Loss: 3.3718 Acc: 0.4922\n",
      "\n",
      "Training complete in 52m 9s\n",
      "Best val Acc: 0.500000\n"
     ]
    }
   ],
   "source": [
    "model_ft = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "# Reset final FC to the number of classes in the target dataset :\n",
    "model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "num_epochs = 200\n",
    "# Decay LR every total_num_epochs/4 so that we have 3 drops\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=num_epochs/4, gamma=0.1)  ## step size 100000 so that we do not use the decay\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft,exp_lr_scheduler, \n",
    "                       num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train Loss: 0.2352 Acc: 0.9402\n",
    "\n",
    "val Loss: 3.3718 Acc: 0.4922\n",
    "\n",
    "Training complete in 52m 9s\n",
    "\n",
    "Best val Acc: 0.500000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# With  a learning rate of 0.001, the training accuracy after 200 epochs is  0.9314 and the  validation accuracy is 0.7882.\n",
    "\n",
    "# With  a learning rate of 0.01, the training accuracy after 200 epochs is  0.9784 and the  validation accuracy is 0.8206\n",
    "\n",
    "# With  a learning rate of 0.1, the training accuracy after 200 epochs is  0.9402 and the  validation accuracy is 0.4922.\n",
    "\n",
    "# The best accuracy on both the validation and training set is the model with a learning rate of 0.01 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKm4NeEffZlp"
   },
   "source": [
    "# Q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-n1yU70fepb"
   },
   "source": [
    "# a) Now we use the petrained model as a feature extractor meaning that we freeze the weights of all except last layer using requires_grad=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbHbLoBpfdOl"
   },
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEiZfnd3f-vj"
   },
   "source": [
    "# lr = 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "3-1h_NhdgBMy",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "419d5597-e80c-4d92-bdb7-9f13cd3a74d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 110.6262 Acc: 0.0363\n",
      "val Loss: 293.0315 Acc: 0.1471\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 399.7550 Acc: 0.1461\n",
      "val Loss: 411.8927 Acc: 0.1598\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 389.2325 Acc: 0.1657\n",
      "val Loss: 327.6356 Acc: 0.2020\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 252.4545 Acc: 0.2196\n",
      "val Loss: 179.2175 Acc: 0.2382\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 135.8676 Acc: 0.3088\n",
      "val Loss: 96.3963 Acc: 0.2814\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 75.8504 Acc: 0.3627\n",
      "val Loss: 47.2605 Acc: 0.4206\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 40.1100 Acc: 0.4314\n",
      "val Loss: 29.5877 Acc: 0.4667\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 31.9336 Acc: 0.4706\n",
      "val Loss: 34.3359 Acc: 0.4471\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 34.2326 Acc: 0.4735\n",
      "val Loss: 38.1779 Acc: 0.4657\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 29.2034 Acc: 0.5314\n",
      "val Loss: 30.4959 Acc: 0.4735\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 26.1908 Acc: 0.5235\n",
      "val Loss: 32.6482 Acc: 0.4706\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 26.9027 Acc: 0.5098\n",
      "val Loss: 33.1939 Acc: 0.4882\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 25.9907 Acc: 0.5382\n",
      "val Loss: 38.2995 Acc: 0.4765\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 31.2520 Acc: 0.5314\n",
      "val Loss: 31.4886 Acc: 0.5069\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 24.2622 Acc: 0.5873\n",
      "val Loss: 27.2084 Acc: 0.5245\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 24.3597 Acc: 0.5529\n",
      "val Loss: 30.3431 Acc: 0.5078\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 24.1660 Acc: 0.5931\n",
      "val Loss: 26.2951 Acc: 0.5794\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 20.9798 Acc: 0.5980\n",
      "val Loss: 27.4721 Acc: 0.5451\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 19.9659 Acc: 0.6059\n",
      "val Loss: 27.3367 Acc: 0.5304\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 22.0747 Acc: 0.6118\n",
      "val Loss: 28.5484 Acc: 0.5275\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 20.8948 Acc: 0.6147\n",
      "val Loss: 26.3035 Acc: 0.5863\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 23.3093 Acc: 0.5863\n",
      "val Loss: 29.8859 Acc: 0.5353\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 21.9418 Acc: 0.6186\n",
      "val Loss: 27.2542 Acc: 0.5588\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 18.4580 Acc: 0.6353\n",
      "val Loss: 25.9856 Acc: 0.5676\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 20.7648 Acc: 0.6078\n",
      "val Loss: 29.3258 Acc: 0.5363\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 21.3908 Acc: 0.6265\n",
      "val Loss: 27.3108 Acc: 0.5529\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 23.7679 Acc: 0.6088\n",
      "val Loss: 30.6835 Acc: 0.5529\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 24.4549 Acc: 0.6353\n",
      "val Loss: 33.3544 Acc: 0.5216\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 22.3674 Acc: 0.6059\n",
      "val Loss: 27.1439 Acc: 0.5647\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 19.3926 Acc: 0.6520\n",
      "val Loss: 30.9461 Acc: 0.5490\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 20.6237 Acc: 0.6196\n",
      "val Loss: 24.5081 Acc: 0.5892\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 20.2638 Acc: 0.6422\n",
      "val Loss: 29.7634 Acc: 0.5637\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 22.3061 Acc: 0.6382\n",
      "val Loss: 28.8969 Acc: 0.5716\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 19.0315 Acc: 0.6745\n",
      "val Loss: 26.9943 Acc: 0.5902\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 19.8340 Acc: 0.6647\n",
      "val Loss: 27.5998 Acc: 0.5912\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 18.6692 Acc: 0.6814\n",
      "val Loss: 26.7797 Acc: 0.5657\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 16.9073 Acc: 0.6765\n",
      "val Loss: 28.9114 Acc: 0.5814\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 17.3598 Acc: 0.6549\n",
      "val Loss: 29.7387 Acc: 0.5735\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 19.0523 Acc: 0.6804\n",
      "val Loss: 30.5497 Acc: 0.5382\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 17.1008 Acc: 0.6794\n",
      "val Loss: 27.6999 Acc: 0.6078\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 17.7105 Acc: 0.7108\n",
      "val Loss: 29.3608 Acc: 0.5765\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 20.6235 Acc: 0.6627\n",
      "val Loss: 28.0179 Acc: 0.5961\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 16.3681 Acc: 0.6863\n",
      "val Loss: 27.5335 Acc: 0.5794\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 17.0750 Acc: 0.7059\n",
      "val Loss: 28.5732 Acc: 0.5735\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 18.2335 Acc: 0.6794\n",
      "val Loss: 28.6256 Acc: 0.6235\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 20.6979 Acc: 0.6549\n",
      "val Loss: 27.6950 Acc: 0.6020\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 16.4448 Acc: 0.7127\n",
      "val Loss: 25.5686 Acc: 0.6196\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 14.4861 Acc: 0.7206\n",
      "val Loss: 30.5717 Acc: 0.5765\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 18.2808 Acc: 0.7020\n",
      "val Loss: 27.8856 Acc: 0.5843\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 16.5389 Acc: 0.7059\n",
      "val Loss: 26.2832 Acc: 0.6059\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 12.9198 Acc: 0.7441\n",
      "val Loss: 19.6865 Acc: 0.6647\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 9.0250 Acc: 0.7843\n",
      "val Loss: 17.9603 Acc: 0.6814\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 7.1771 Acc: 0.8353\n",
      "val Loss: 17.4359 Acc: 0.6931\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 6.7616 Acc: 0.8324\n",
      "val Loss: 17.0396 Acc: 0.6941\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 7.4311 Acc: 0.8265\n",
      "val Loss: 16.7750 Acc: 0.6941\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 7.9680 Acc: 0.8196\n",
      "val Loss: 16.8827 Acc: 0.6902\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 6.6681 Acc: 0.8333\n",
      "val Loss: 17.2223 Acc: 0.6882\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 6.2507 Acc: 0.8480\n",
      "val Loss: 16.8094 Acc: 0.6882\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 7.1364 Acc: 0.8284\n",
      "val Loss: 16.7777 Acc: 0.6941\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 7.2766 Acc: 0.8343\n",
      "val Loss: 17.3886 Acc: 0.6941\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 7.4524 Acc: 0.8225\n",
      "val Loss: 16.7881 Acc: 0.6961\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 6.4100 Acc: 0.8422\n",
      "val Loss: 16.5783 Acc: 0.6980\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 6.4432 Acc: 0.8343\n",
      "val Loss: 16.5760 Acc: 0.6931\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 7.2093 Acc: 0.8402\n",
      "val Loss: 16.6237 Acc: 0.6912\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 6.6062 Acc: 0.8324\n",
      "val Loss: 16.6228 Acc: 0.6951\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 6.0224 Acc: 0.8402\n",
      "val Loss: 16.8909 Acc: 0.6922\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 6.7553 Acc: 0.8422\n",
      "val Loss: 16.8127 Acc: 0.6902\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 6.2896 Acc: 0.8392\n",
      "val Loss: 16.4887 Acc: 0.6931\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 6.1768 Acc: 0.8373\n",
      "val Loss: 16.3241 Acc: 0.7029\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 5.7196 Acc: 0.8520\n",
      "val Loss: 16.3002 Acc: 0.6902\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 5.8286 Acc: 0.8373\n",
      "val Loss: 16.4075 Acc: 0.7000\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 6.5315 Acc: 0.8373\n",
      "val Loss: 16.3913 Acc: 0.6961\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 7.0716 Acc: 0.8324\n",
      "val Loss: 16.2626 Acc: 0.6873\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 6.6091 Acc: 0.8304\n",
      "val Loss: 16.1588 Acc: 0.7078\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 6.2487 Acc: 0.8412\n",
      "val Loss: 16.2919 Acc: 0.6971\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 6.2591 Acc: 0.8422\n",
      "val Loss: 16.1570 Acc: 0.7020\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 6.4875 Acc: 0.8314\n",
      "val Loss: 16.1760 Acc: 0.7069\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 6.9748 Acc: 0.8373\n",
      "val Loss: 16.2045 Acc: 0.6980\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 6.4412 Acc: 0.8461\n",
      "val Loss: 16.0972 Acc: 0.6941\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 6.4479 Acc: 0.8382\n",
      "val Loss: 16.2008 Acc: 0.6941\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 6.4523 Acc: 0.8343\n",
      "val Loss: 16.0895 Acc: 0.6990\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 6.7651 Acc: 0.8324\n",
      "val Loss: 16.0336 Acc: 0.6853\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 6.8034 Acc: 0.8324\n",
      "val Loss: 16.1191 Acc: 0.6892\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 5.8852 Acc: 0.8402\n",
      "val Loss: 16.1952 Acc: 0.6941\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 6.3320 Acc: 0.8382\n",
      "val Loss: 16.3309 Acc: 0.6971\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 6.6682 Acc: 0.8373\n",
      "val Loss: 16.8733 Acc: 0.6941\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 5.1849 Acc: 0.8578\n",
      "val Loss: 16.3632 Acc: 0.6990\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 6.1723 Acc: 0.8431\n",
      "val Loss: 16.3467 Acc: 0.6873\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 6.6466 Acc: 0.8461\n",
      "val Loss: 16.4136 Acc: 0.6853\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 6.1625 Acc: 0.8471\n",
      "val Loss: 16.2030 Acc: 0.6902\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 6.5427 Acc: 0.8373\n",
      "val Loss: 16.0836 Acc: 0.6912\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 4.6582 Acc: 0.8549\n",
      "val Loss: 16.3386 Acc: 0.6961\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 5.6854 Acc: 0.8500\n",
      "val Loss: 16.4880 Acc: 0.6961\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 4.9419 Acc: 0.8598\n",
      "val Loss: 16.1532 Acc: 0.6951\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 6.8103 Acc: 0.8363\n",
      "val Loss: 15.9055 Acc: 0.7059\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 6.0865 Acc: 0.8471\n",
      "val Loss: 15.9572 Acc: 0.7059\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 5.5353 Acc: 0.8510\n",
      "val Loss: 15.9669 Acc: 0.7059\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 5.7888 Acc: 0.8392\n",
      "val Loss: 15.9801 Acc: 0.6990\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 5.4303 Acc: 0.8461\n",
      "val Loss: 15.8535 Acc: 0.6980\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 5.8989 Acc: 0.8431\n",
      "val Loss: 16.0451 Acc: 0.6941\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 5.2117 Acc: 0.8500\n",
      "val Loss: 15.9046 Acc: 0.6971\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 5.5301 Acc: 0.8490\n",
      "val Loss: 15.9183 Acc: 0.6990\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 4.7652 Acc: 0.8627\n",
      "val Loss: 15.6368 Acc: 0.7029\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 5.6729 Acc: 0.8382\n",
      "val Loss: 15.6012 Acc: 0.7020\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 5.0212 Acc: 0.8578\n",
      "val Loss: 15.6110 Acc: 0.7059\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 5.2689 Acc: 0.8529\n",
      "val Loss: 15.4234 Acc: 0.7029\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 4.7770 Acc: 0.8569\n",
      "val Loss: 15.5989 Acc: 0.7078\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 5.3112 Acc: 0.8490\n",
      "val Loss: 15.4950 Acc: 0.7049\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 5.6477 Acc: 0.8441\n",
      "val Loss: 15.5762 Acc: 0.7049\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 5.0496 Acc: 0.8471\n",
      "val Loss: 15.7352 Acc: 0.7078\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 4.9666 Acc: 0.8696\n",
      "val Loss: 15.6695 Acc: 0.6990\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 5.2332 Acc: 0.8647\n",
      "val Loss: 15.5439 Acc: 0.7020\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 5.0908 Acc: 0.8608\n",
      "val Loss: 15.4062 Acc: 0.7069\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 5.1863 Acc: 0.8578\n",
      "val Loss: 15.5042 Acc: 0.7020\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 4.9126 Acc: 0.8549\n",
      "val Loss: 15.3319 Acc: 0.7020\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 4.2117 Acc: 0.8490\n",
      "val Loss: 15.4862 Acc: 0.7029\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 5.8260 Acc: 0.8500\n",
      "val Loss: 15.4494 Acc: 0.7078\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 5.4554 Acc: 0.8422\n",
      "val Loss: 15.4113 Acc: 0.7069\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 4.9165 Acc: 0.8578\n",
      "val Loss: 15.4016 Acc: 0.7088\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 5.5883 Acc: 0.8598\n",
      "val Loss: 15.5670 Acc: 0.7069\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 5.3613 Acc: 0.8588\n",
      "val Loss: 15.5565 Acc: 0.7069\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 5.1439 Acc: 0.8637\n",
      "val Loss: 15.4644 Acc: 0.7049\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 5.3290 Acc: 0.8520\n",
      "val Loss: 15.6426 Acc: 0.7010\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 4.7885 Acc: 0.8627\n",
      "val Loss: 15.3612 Acc: 0.7088\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 5.4936 Acc: 0.8520\n",
      "val Loss: 15.4183 Acc: 0.7020\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 4.3686 Acc: 0.8725\n",
      "val Loss: 15.3213 Acc: 0.7049\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 5.1853 Acc: 0.8569\n",
      "val Loss: 15.4960 Acc: 0.7020\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 5.0091 Acc: 0.8686\n",
      "val Loss: 15.4249 Acc: 0.7078\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 4.9785 Acc: 0.8559\n",
      "val Loss: 15.5759 Acc: 0.7029\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 4.6786 Acc: 0.8706\n",
      "val Loss: 15.5099 Acc: 0.7108\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 5.5076 Acc: 0.8588\n",
      "val Loss: 15.6058 Acc: 0.7039\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 5.0832 Acc: 0.8431\n",
      "val Loss: 15.4741 Acc: 0.7098\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 4.8923 Acc: 0.8569\n",
      "val Loss: 15.3085 Acc: 0.7059\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 5.4335 Acc: 0.8598\n",
      "val Loss: 15.3612 Acc: 0.7078\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 5.2353 Acc: 0.8588\n",
      "val Loss: 15.4388 Acc: 0.7108\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 5.7852 Acc: 0.8520\n",
      "val Loss: 15.5222 Acc: 0.7088\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 5.7589 Acc: 0.8451\n",
      "val Loss: 15.4364 Acc: 0.7069\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 5.6952 Acc: 0.8647\n",
      "val Loss: 15.5458 Acc: 0.7039\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 4.8806 Acc: 0.8529\n",
      "val Loss: 15.5496 Acc: 0.7020\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 4.3350 Acc: 0.8706\n",
      "val Loss: 15.5447 Acc: 0.7088\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 4.9486 Acc: 0.8451\n",
      "val Loss: 15.5983 Acc: 0.7049\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 4.7524 Acc: 0.8598\n",
      "val Loss: 15.5200 Acc: 0.7029\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 5.5725 Acc: 0.8520\n",
      "val Loss: 15.4240 Acc: 0.7108\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 4.0724 Acc: 0.8676\n",
      "val Loss: 15.3787 Acc: 0.7088\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 4.2417 Acc: 0.8735\n",
      "val Loss: 15.3551 Acc: 0.7039\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 5.0540 Acc: 0.8667\n",
      "val Loss: 15.2962 Acc: 0.7029\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 5.9474 Acc: 0.8569\n",
      "val Loss: 15.4243 Acc: 0.7029\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 5.2119 Acc: 0.8500\n",
      "val Loss: 15.2630 Acc: 0.7049\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 3.8590 Acc: 0.8814\n",
      "val Loss: 15.2085 Acc: 0.7118\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 5.6165 Acc: 0.8431\n",
      "val Loss: 15.4372 Acc: 0.7088\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 4.3447 Acc: 0.8735\n",
      "val Loss: 15.3619 Acc: 0.7029\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 5.1206 Acc: 0.8363\n",
      "val Loss: 15.4005 Acc: 0.7069\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 4.4568 Acc: 0.8608\n",
      "val Loss: 15.4131 Acc: 0.7059\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 5.6650 Acc: 0.8402\n",
      "val Loss: 15.4155 Acc: 0.7049\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 6.3457 Acc: 0.8490\n",
      "val Loss: 15.3563 Acc: 0.7039\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 4.7018 Acc: 0.8706\n",
      "val Loss: 15.2726 Acc: 0.7020\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 5.4727 Acc: 0.8333\n",
      "val Loss: 15.5010 Acc: 0.7010\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 5.4185 Acc: 0.8529\n",
      "val Loss: 15.5041 Acc: 0.7069\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 4.9445 Acc: 0.8657\n",
      "val Loss: 15.3020 Acc: 0.7059\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 4.2323 Acc: 0.8627\n",
      "val Loss: 15.3385 Acc: 0.7029\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 4.5759 Acc: 0.8667\n",
      "val Loss: 15.3479 Acc: 0.7020\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 4.7529 Acc: 0.8647\n",
      "val Loss: 15.2343 Acc: 0.7000\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 5.6924 Acc: 0.8539\n",
      "val Loss: 15.4288 Acc: 0.6971\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 4.8968 Acc: 0.8392\n",
      "val Loss: 15.3580 Acc: 0.7059\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 6.2689 Acc: 0.8382\n",
      "val Loss: 15.3184 Acc: 0.7059\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 5.7502 Acc: 0.8529\n",
      "val Loss: 15.5051 Acc: 0.7039\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 4.7756 Acc: 0.8598\n",
      "val Loss: 15.2105 Acc: 0.7108\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 4.8567 Acc: 0.8627\n",
      "val Loss: 15.4093 Acc: 0.7020\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 4.3289 Acc: 0.8676\n",
      "val Loss: 15.3627 Acc: 0.6980\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 5.1706 Acc: 0.8569\n",
      "val Loss: 15.4467 Acc: 0.7049\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 5.5467 Acc: 0.8539\n",
      "val Loss: 15.3826 Acc: 0.7029\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 5.4444 Acc: 0.8529\n",
      "val Loss: 15.3465 Acc: 0.7059\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 4.5774 Acc: 0.8618\n",
      "val Loss: 15.3312 Acc: 0.7039\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 4.9043 Acc: 0.8676\n",
      "val Loss: 15.4073 Acc: 0.7000\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 5.3425 Acc: 0.8569\n",
      "val Loss: 15.3582 Acc: 0.7039\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 5.9207 Acc: 0.8451\n",
      "val Loss: 15.5747 Acc: 0.7000\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 5.8215 Acc: 0.8412\n",
      "val Loss: 15.4839 Acc: 0.7088\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 4.9744 Acc: 0.8725\n",
      "val Loss: 15.2095 Acc: 0.7039\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 4.6299 Acc: 0.8676\n",
      "val Loss: 15.2255 Acc: 0.7049\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 6.0631 Acc: 0.8392\n",
      "val Loss: 15.3773 Acc: 0.7010\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 4.7237 Acc: 0.8706\n",
      "val Loss: 15.3134 Acc: 0.7078\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 5.2388 Acc: 0.8578\n",
      "val Loss: 15.3396 Acc: 0.7088\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 6.0456 Acc: 0.8412\n",
      "val Loss: 15.2373 Acc: 0.7039\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 5.0060 Acc: 0.8627\n",
      "val Loss: 15.1001 Acc: 0.7039\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 5.5785 Acc: 0.8402\n",
      "val Loss: 15.3851 Acc: 0.7088\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 5.5451 Acc: 0.8431\n",
      "val Loss: 15.3764 Acc: 0.7098\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 3.7711 Acc: 0.8696\n",
      "val Loss: 15.4135 Acc: 0.7039\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 5.4798 Acc: 0.8422\n",
      "val Loss: 15.4499 Acc: 0.7029\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 4.7428 Acc: 0.8676\n",
      "val Loss: 15.2288 Acc: 0.7137\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 4.8303 Acc: 0.8716\n",
      "val Loss: 15.3016 Acc: 0.7088\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 3.7512 Acc: 0.8725\n",
      "val Loss: 15.2326 Acc: 0.7069\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 5.8750 Acc: 0.8284\n",
      "val Loss: 15.4338 Acc: 0.7029\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 5.5315 Acc: 0.8422\n",
      "val Loss: 15.4920 Acc: 0.7069\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 5.2989 Acc: 0.8480\n",
      "val Loss: 15.3185 Acc: 0.7069\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 5.1872 Acc: 0.8510\n",
      "val Loss: 15.3828 Acc: 0.7049\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 4.7021 Acc: 0.8627\n",
      "val Loss: 15.3489 Acc: 0.7088\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 4.8330 Acc: 0.8598\n",
      "val Loss: 15.5146 Acc: 0.7020\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 5.1134 Acc: 0.8549\n",
      "val Loss: 15.3411 Acc: 0.7049\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 5.2215 Acc: 0.8598\n",
      "val Loss: 15.2312 Acc: 0.7010\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 5.5586 Acc: 0.8520\n",
      "val Loss: 15.3486 Acc: 0.7029\n",
      "\n",
      "Training complete in 30m 15s\n",
      "Best val Acc: 0.713725\n"
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=1, momentum=0.9)\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=num_epochs/4, gamma=0.1)\n",
    "\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train Loss: 5.5586 Acc: 0.8520\n",
    "\n",
    "val Loss: 15.3486 Acc: 0.7029\n",
    "\n",
    "Training complete in 30m 15s\n",
    "\n",
    "Best val Acc: 0.713725"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alDj5iuJgeMz"
   },
   "source": [
    "# lr =0.1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "8M8yFLsLgfNV",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "dcfac1e9-c4a5-45c6-daf7-55dc601853c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 5.2455 Acc: 0.0422\n",
      "val Loss: 4.9815 Acc: 0.2098\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 4.5829 Acc: 0.2402\n",
      "val Loss: 3.6812 Acc: 0.3422\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 3.6361 Acc: 0.3667\n",
      "val Loss: 3.7425 Acc: 0.3667\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 3.4495 Acc: 0.4039\n",
      "val Loss: 3.5973 Acc: 0.4216\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 3.4332 Acc: 0.4461\n",
      "val Loss: 3.4656 Acc: 0.4108\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 2.4889 Acc: 0.5176\n",
      "val Loss: 2.8774 Acc: 0.4833\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 2.6699 Acc: 0.5157\n",
      "val Loss: 2.7427 Acc: 0.4980\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 2.4367 Acc: 0.5480\n",
      "val Loss: 2.7909 Acc: 0.5343\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 2.1687 Acc: 0.5961\n",
      "val Loss: 2.8508 Acc: 0.4990\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 2.1464 Acc: 0.5961\n",
      "val Loss: 2.2896 Acc: 0.5490\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 1.9754 Acc: 0.5912\n",
      "val Loss: 2.7311 Acc: 0.5284\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 2.2536 Acc: 0.5755\n",
      "val Loss: 2.7619 Acc: 0.5176\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 2.3567 Acc: 0.5804\n",
      "val Loss: 2.8120 Acc: 0.5392\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 2.0157 Acc: 0.6275\n",
      "val Loss: 2.3458 Acc: 0.5657\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 1.9724 Acc: 0.6353\n",
      "val Loss: 2.4880 Acc: 0.5539\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 1.8852 Acc: 0.6441\n",
      "val Loss: 2.2800 Acc: 0.5716\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 1.6420 Acc: 0.6765\n",
      "val Loss: 2.2832 Acc: 0.5765\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 1.7024 Acc: 0.6676\n",
      "val Loss: 2.4427 Acc: 0.5676\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 1.6913 Acc: 0.6676\n",
      "val Loss: 2.2278 Acc: 0.5873\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 1.7224 Acc: 0.6725\n",
      "val Loss: 2.5058 Acc: 0.6010\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 1.6976 Acc: 0.6618\n",
      "val Loss: 2.6527 Acc: 0.5431\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 1.6371 Acc: 0.6696\n",
      "val Loss: 2.4550 Acc: 0.5618\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 1.5568 Acc: 0.7137\n",
      "val Loss: 2.3218 Acc: 0.6049\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 1.5735 Acc: 0.6784\n",
      "val Loss: 2.3805 Acc: 0.6049\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 1.4634 Acc: 0.6951\n",
      "val Loss: 2.4207 Acc: 0.5863\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 1.3724 Acc: 0.7157\n",
      "val Loss: 2.3159 Acc: 0.6078\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 1.4254 Acc: 0.7088\n",
      "val Loss: 2.2028 Acc: 0.5971\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 1.6399 Acc: 0.6902\n",
      "val Loss: 2.2439 Acc: 0.6088\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 1.5854 Acc: 0.6951\n",
      "val Loss: 2.7112 Acc: 0.5843\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 1.8061 Acc: 0.6637\n",
      "val Loss: 2.2910 Acc: 0.6020\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 1.6989 Acc: 0.7078\n",
      "val Loss: 2.6074 Acc: 0.5833\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 1.7526 Acc: 0.6941\n",
      "val Loss: 2.3651 Acc: 0.6039\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 1.5685 Acc: 0.6765\n",
      "val Loss: 2.0003 Acc: 0.6343\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 1.5069 Acc: 0.7118\n",
      "val Loss: 2.5473 Acc: 0.5951\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 1.4393 Acc: 0.7059\n",
      "val Loss: 2.2682 Acc: 0.6118\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 1.5734 Acc: 0.7069\n",
      "val Loss: 2.6118 Acc: 0.5824\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 1.3342 Acc: 0.7147\n",
      "val Loss: 2.3432 Acc: 0.6275\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 1.3995 Acc: 0.7245\n",
      "val Loss: 2.3089 Acc: 0.6118\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 1.3777 Acc: 0.7363\n",
      "val Loss: 2.0903 Acc: 0.6088\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 1.2749 Acc: 0.7461\n",
      "val Loss: 2.0357 Acc: 0.6265\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 1.3483 Acc: 0.7275\n",
      "val Loss: 2.5796 Acc: 0.5971\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 1.5518 Acc: 0.7108\n",
      "val Loss: 2.2768 Acc: 0.6118\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 1.3292 Acc: 0.7216\n",
      "val Loss: 2.1973 Acc: 0.6147\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 1.4754 Acc: 0.7196\n",
      "val Loss: 2.2024 Acc: 0.6402\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 1.2699 Acc: 0.7520\n",
      "val Loss: 2.5457 Acc: 0.5941\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 1.3458 Acc: 0.7431\n",
      "val Loss: 2.3597 Acc: 0.6324\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 1.4225 Acc: 0.7402\n",
      "val Loss: 2.1955 Acc: 0.6059\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 1.3540 Acc: 0.7363\n",
      "val Loss: 2.4071 Acc: 0.6069\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 1.5525 Acc: 0.7147\n",
      "val Loss: 2.6040 Acc: 0.5951\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 1.7087 Acc: 0.6814\n",
      "val Loss: 2.4317 Acc: 0.6196\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 1.1477 Acc: 0.7794\n",
      "val Loss: 1.9256 Acc: 0.6686\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.7084 Acc: 0.8578\n",
      "val Loss: 1.7110 Acc: 0.6892\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.6796 Acc: 0.8559\n",
      "val Loss: 1.6776 Acc: 0.6941\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.7114 Acc: 0.8324\n",
      "val Loss: 1.6746 Acc: 0.6980\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.5975 Acc: 0.8559\n",
      "val Loss: 1.6496 Acc: 0.6912\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.7049 Acc: 0.8441\n",
      "val Loss: 1.6266 Acc: 0.6961\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.6024 Acc: 0.8676\n",
      "val Loss: 1.6074 Acc: 0.6922\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.6648 Acc: 0.8490\n",
      "val Loss: 1.6287 Acc: 0.7108\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.6928 Acc: 0.8343\n",
      "val Loss: 1.6488 Acc: 0.7049\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.6402 Acc: 0.8578\n",
      "val Loss: 1.6271 Acc: 0.7078\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.7681 Acc: 0.8510\n",
      "val Loss: 1.6295 Acc: 0.7000\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.6590 Acc: 0.8471\n",
      "val Loss: 1.6311 Acc: 0.7059\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.6603 Acc: 0.8510\n",
      "val Loss: 1.6172 Acc: 0.7049\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.6428 Acc: 0.8451\n",
      "val Loss: 1.5821 Acc: 0.6980\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.7304 Acc: 0.8422\n",
      "val Loss: 1.5935 Acc: 0.6931\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.6024 Acc: 0.8578\n",
      "val Loss: 1.6182 Acc: 0.6892\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.6060 Acc: 0.8569\n",
      "val Loss: 1.6315 Acc: 0.6971\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.6003 Acc: 0.8569\n",
      "val Loss: 1.6270 Acc: 0.7000\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.5918 Acc: 0.8598\n",
      "val Loss: 1.5945 Acc: 0.7118\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.6251 Acc: 0.8510\n",
      "val Loss: 1.5874 Acc: 0.7147\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.5694 Acc: 0.8686\n",
      "val Loss: 1.5742 Acc: 0.7147\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.4993 Acc: 0.8716\n",
      "val Loss: 1.5792 Acc: 0.7196\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.5812 Acc: 0.8549\n",
      "val Loss: 1.5863 Acc: 0.7088\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.5760 Acc: 0.8637\n",
      "val Loss: 1.5811 Acc: 0.7098\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.5332 Acc: 0.8647\n",
      "val Loss: 1.5968 Acc: 0.7010\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.5620 Acc: 0.8706\n",
      "val Loss: 1.5437 Acc: 0.7069\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.6312 Acc: 0.8529\n",
      "val Loss: 1.5620 Acc: 0.6980\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.5165 Acc: 0.8725\n",
      "val Loss: 1.6121 Acc: 0.6941\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.6313 Acc: 0.8637\n",
      "val Loss: 1.5966 Acc: 0.7000\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.5589 Acc: 0.8549\n",
      "val Loss: 1.5919 Acc: 0.7010\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.6796 Acc: 0.8520\n",
      "val Loss: 1.5937 Acc: 0.6931\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.6664 Acc: 0.8471\n",
      "val Loss: 1.5613 Acc: 0.6961\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.6032 Acc: 0.8510\n",
      "val Loss: 1.5536 Acc: 0.7020\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.5856 Acc: 0.8588\n",
      "val Loss: 1.5673 Acc: 0.7039\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.6295 Acc: 0.8520\n",
      "val Loss: 1.5688 Acc: 0.6892\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.6868 Acc: 0.8500\n",
      "val Loss: 1.5796 Acc: 0.6971\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.6442 Acc: 0.8618\n",
      "val Loss: 1.5802 Acc: 0.7020\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.6562 Acc: 0.8471\n",
      "val Loss: 1.5734 Acc: 0.6912\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.5720 Acc: 0.8559\n",
      "val Loss: 1.5449 Acc: 0.6882\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.5997 Acc: 0.8471\n",
      "val Loss: 1.5630 Acc: 0.6990\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.6151 Acc: 0.8480\n",
      "val Loss: 1.5420 Acc: 0.7088\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.6213 Acc: 0.8510\n",
      "val Loss: 1.5622 Acc: 0.7049\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.6422 Acc: 0.8578\n",
      "val Loss: 1.5831 Acc: 0.6980\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.5669 Acc: 0.8588\n",
      "val Loss: 1.5537 Acc: 0.6931\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.5478 Acc: 0.8539\n",
      "val Loss: 1.5450 Acc: 0.6951\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.6410 Acc: 0.8598\n",
      "val Loss: 1.5412 Acc: 0.7010\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.6183 Acc: 0.8618\n",
      "val Loss: 1.5347 Acc: 0.7029\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.5491 Acc: 0.8657\n",
      "val Loss: 1.5644 Acc: 0.6980\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.5684 Acc: 0.8598\n",
      "val Loss: 1.5554 Acc: 0.7020\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.5845 Acc: 0.8637\n",
      "val Loss: 1.5398 Acc: 0.7029\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.6268 Acc: 0.8667\n",
      "val Loss: 1.5328 Acc: 0.7020\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.6211 Acc: 0.8598\n",
      "val Loss: 1.5340 Acc: 0.6961\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.5363 Acc: 0.8735\n",
      "val Loss: 1.5103 Acc: 0.7039\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.5661 Acc: 0.8657\n",
      "val Loss: 1.5194 Acc: 0.6971\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.4331 Acc: 0.8912\n",
      "val Loss: 1.5031 Acc: 0.7039\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.6068 Acc: 0.8490\n",
      "val Loss: 1.5147 Acc: 0.7039\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.5676 Acc: 0.8618\n",
      "val Loss: 1.5175 Acc: 0.7059\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.6568 Acc: 0.8529\n",
      "val Loss: 1.5119 Acc: 0.7029\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.5977 Acc: 0.8510\n",
      "val Loss: 1.5005 Acc: 0.7039\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.5477 Acc: 0.8735\n",
      "val Loss: 1.5153 Acc: 0.7010\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.6054 Acc: 0.8735\n",
      "val Loss: 1.5073 Acc: 0.7069\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.4691 Acc: 0.8833\n",
      "val Loss: 1.4959 Acc: 0.7127\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.5704 Acc: 0.8578\n",
      "val Loss: 1.5048 Acc: 0.7049\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.6069 Acc: 0.8696\n",
      "val Loss: 1.5031 Acc: 0.7059\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.5826 Acc: 0.8686\n",
      "val Loss: 1.5096 Acc: 0.7029\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.5711 Acc: 0.8696\n",
      "val Loss: 1.5060 Acc: 0.7010\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.5742 Acc: 0.8676\n",
      "val Loss: 1.5050 Acc: 0.7039\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.5321 Acc: 0.8686\n",
      "val Loss: 1.4958 Acc: 0.7069\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.6104 Acc: 0.8618\n",
      "val Loss: 1.5155 Acc: 0.6990\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.5251 Acc: 0.8686\n",
      "val Loss: 1.5030 Acc: 0.7000\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.5919 Acc: 0.8647\n",
      "val Loss: 1.5093 Acc: 0.6971\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.5278 Acc: 0.8686\n",
      "val Loss: 1.5120 Acc: 0.7049\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.5378 Acc: 0.8637\n",
      "val Loss: 1.5062 Acc: 0.7029\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.5559 Acc: 0.8696\n",
      "val Loss: 1.4941 Acc: 0.7098\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.6208 Acc: 0.8637\n",
      "val Loss: 1.5007 Acc: 0.7049\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.6169 Acc: 0.8588\n",
      "val Loss: 1.5137 Acc: 0.7049\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.5075 Acc: 0.8804\n",
      "val Loss: 1.5128 Acc: 0.7069\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.5570 Acc: 0.8706\n",
      "val Loss: 1.4989 Acc: 0.7118\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.5299 Acc: 0.8686\n",
      "val Loss: 1.5080 Acc: 0.7059\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.5600 Acc: 0.8549\n",
      "val Loss: 1.5264 Acc: 0.6990\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.5410 Acc: 0.8686\n",
      "val Loss: 1.5024 Acc: 0.7020\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.6122 Acc: 0.8598\n",
      "val Loss: 1.5113 Acc: 0.6971\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.5720 Acc: 0.8608\n",
      "val Loss: 1.5005 Acc: 0.7049\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.5546 Acc: 0.8804\n",
      "val Loss: 1.4999 Acc: 0.6990\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.5898 Acc: 0.8578\n",
      "val Loss: 1.5017 Acc: 0.7069\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.6647 Acc: 0.8510\n",
      "val Loss: 1.5066 Acc: 0.7010\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.5026 Acc: 0.8725\n",
      "val Loss: 1.4940 Acc: 0.7059\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.4445 Acc: 0.8941\n",
      "val Loss: 1.4939 Acc: 0.7010\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.5201 Acc: 0.8745\n",
      "val Loss: 1.5025 Acc: 0.7059\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.5408 Acc: 0.8676\n",
      "val Loss: 1.4917 Acc: 0.7069\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.5428 Acc: 0.8833\n",
      "val Loss: 1.4989 Acc: 0.7010\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.5479 Acc: 0.8735\n",
      "val Loss: 1.5049 Acc: 0.6971\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.5187 Acc: 0.8706\n",
      "val Loss: 1.5057 Acc: 0.6980\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.6338 Acc: 0.8569\n",
      "val Loss: 1.5090 Acc: 0.6961\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.5146 Acc: 0.8706\n",
      "val Loss: 1.4962 Acc: 0.7010\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.5069 Acc: 0.8745\n",
      "val Loss: 1.4916 Acc: 0.6990\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.5373 Acc: 0.8775\n",
      "val Loss: 1.4914 Acc: 0.7059\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.5316 Acc: 0.8706\n",
      "val Loss: 1.4886 Acc: 0.7049\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.6035 Acc: 0.8529\n",
      "val Loss: 1.5047 Acc: 0.6980\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.5239 Acc: 0.8716\n",
      "val Loss: 1.5067 Acc: 0.7049\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.5629 Acc: 0.8696\n",
      "val Loss: 1.4961 Acc: 0.7039\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.5218 Acc: 0.8755\n",
      "val Loss: 1.5017 Acc: 0.7059\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.6416 Acc: 0.8422\n",
      "val Loss: 1.5090 Acc: 0.7000\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.5147 Acc: 0.8696\n",
      "val Loss: 1.4952 Acc: 0.7069\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.4612 Acc: 0.8755\n",
      "val Loss: 1.4981 Acc: 0.7039\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.5991 Acc: 0.8588\n",
      "val Loss: 1.4984 Acc: 0.7078\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.5171 Acc: 0.8696\n",
      "val Loss: 1.4879 Acc: 0.7020\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.5949 Acc: 0.8569\n",
      "val Loss: 1.5018 Acc: 0.7020\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.5599 Acc: 0.8676\n",
      "val Loss: 1.5123 Acc: 0.7020\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.5655 Acc: 0.8716\n",
      "val Loss: 1.5098 Acc: 0.7029\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.5090 Acc: 0.8794\n",
      "val Loss: 1.4990 Acc: 0.7020\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.5001 Acc: 0.8794\n",
      "val Loss: 1.4908 Acc: 0.7108\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.4067 Acc: 0.8892\n",
      "val Loss: 1.4772 Acc: 0.7108\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.5048 Acc: 0.8794\n",
      "val Loss: 1.4968 Acc: 0.7020\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.5120 Acc: 0.8775\n",
      "val Loss: 1.5003 Acc: 0.7020\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.5869 Acc: 0.8549\n",
      "val Loss: 1.4920 Acc: 0.7059\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.5480 Acc: 0.8657\n",
      "val Loss: 1.5036 Acc: 0.7049\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.5164 Acc: 0.8755\n",
      "val Loss: 1.4948 Acc: 0.7069\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.4882 Acc: 0.8814\n",
      "val Loss: 1.4913 Acc: 0.7029\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.5896 Acc: 0.8588\n",
      "val Loss: 1.4859 Acc: 0.7059\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.5044 Acc: 0.8725\n",
      "val Loss: 1.4881 Acc: 0.7127\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.5402 Acc: 0.8657\n",
      "val Loss: 1.5026 Acc: 0.7049\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.4524 Acc: 0.8833\n",
      "val Loss: 1.4947 Acc: 0.7049\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.5888 Acc: 0.8520\n",
      "val Loss: 1.5018 Acc: 0.7039\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.5951 Acc: 0.8637\n",
      "val Loss: 1.5009 Acc: 0.7029\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.5763 Acc: 0.8647\n",
      "val Loss: 1.4939 Acc: 0.7039\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.5286 Acc: 0.8794\n",
      "val Loss: 1.5015 Acc: 0.7098\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.5447 Acc: 0.8676\n",
      "val Loss: 1.5028 Acc: 0.7108\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.5568 Acc: 0.8784\n",
      "val Loss: 1.4962 Acc: 0.7049\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.4977 Acc: 0.8824\n",
      "val Loss: 1.4974 Acc: 0.7000\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.5154 Acc: 0.8716\n",
      "val Loss: 1.5044 Acc: 0.6980\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.6214 Acc: 0.8549\n",
      "val Loss: 1.4992 Acc: 0.7049\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.5736 Acc: 0.8608\n",
      "val Loss: 1.5058 Acc: 0.7020\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.5368 Acc: 0.8686\n",
      "val Loss: 1.5035 Acc: 0.6980\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.5120 Acc: 0.8775\n",
      "val Loss: 1.4917 Acc: 0.7010\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.4958 Acc: 0.8824\n",
      "val Loss: 1.4947 Acc: 0.7069\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.6530 Acc: 0.8510\n",
      "val Loss: 1.4992 Acc: 0.7020\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.4873 Acc: 0.8765\n",
      "val Loss: 1.4942 Acc: 0.7059\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.5953 Acc: 0.8569\n",
      "val Loss: 1.5054 Acc: 0.7010\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.4755 Acc: 0.8892\n",
      "val Loss: 1.4969 Acc: 0.7049\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.5597 Acc: 0.8667\n",
      "val Loss: 1.4944 Acc: 0.7049\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.5653 Acc: 0.8676\n",
      "val Loss: 1.4865 Acc: 0.7108\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.6173 Acc: 0.8618\n",
      "val Loss: 1.5117 Acc: 0.7000\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.5244 Acc: 0.8843\n",
      "val Loss: 1.4948 Acc: 0.7039\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.5651 Acc: 0.8618\n",
      "val Loss: 1.4955 Acc: 0.7029\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.5045 Acc: 0.8745\n",
      "val Loss: 1.4939 Acc: 0.7059\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.5550 Acc: 0.8735\n",
      "val Loss: 1.4982 Acc: 0.7000\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.5780 Acc: 0.8618\n",
      "val Loss: 1.5114 Acc: 0.7000\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.4735 Acc: 0.8784\n",
      "val Loss: 1.4975 Acc: 0.7069\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.5293 Acc: 0.8686\n",
      "val Loss: 1.5026 Acc: 0.7039\n",
      "\n",
      "Training complete in 29m 55s\n",
      "Best val Acc: 0.719608\n"
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=num_epochs/4, gamma=0.1)\n",
    "\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train Loss: 0.5293 Acc: 0.8686\n",
    "\n",
    "val Loss: 1.5026 Acc: 0.7039\n",
    "\n",
    "Training complete in 29m 55s\n",
    "\n",
    "Best val Acc: 0.719608"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcDul6xlgiVH"
   },
   "source": [
    "# lr = 0.01 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "XkY6SXligp3p",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "58d8c7eb-fdf2-4a25-d3a6-e7466f570739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 4.6926 Acc: 0.0108\n",
      "val Loss: 4.4088 Acc: 0.0667\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 4.3628 Acc: 0.0882\n",
      "val Loss: 3.9430 Acc: 0.2676\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 3.9306 Acc: 0.2118\n",
      "val Loss: 3.5473 Acc: 0.3490\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 3.5825 Acc: 0.3186\n",
      "val Loss: 3.2037 Acc: 0.4304\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 3.2529 Acc: 0.3794\n",
      "val Loss: 2.9085 Acc: 0.5000\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 2.9850 Acc: 0.4333\n",
      "val Loss: 2.6750 Acc: 0.5422\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 2.7588 Acc: 0.4725\n",
      "val Loss: 2.4994 Acc: 0.5637\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 2.5847 Acc: 0.5275\n",
      "val Loss: 2.3509 Acc: 0.5578\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 2.4240 Acc: 0.5373\n",
      "val Loss: 2.2244 Acc: 0.5843\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 2.2634 Acc: 0.5676\n",
      "val Loss: 2.1059 Acc: 0.6010\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 2.1561 Acc: 0.6020\n",
      "val Loss: 2.0245 Acc: 0.6108\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 2.1036 Acc: 0.5902\n",
      "val Loss: 1.9290 Acc: 0.6275\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 1.9599 Acc: 0.6245\n",
      "val Loss: 1.8514 Acc: 0.6441\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 1.9159 Acc: 0.6304\n",
      "val Loss: 1.7893 Acc: 0.6588\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 1.8715 Acc: 0.6451\n",
      "val Loss: 1.7430 Acc: 0.6588\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 1.7489 Acc: 0.6578\n",
      "val Loss: 1.6871 Acc: 0.6706\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 1.6950 Acc: 0.6667\n",
      "val Loss: 1.6591 Acc: 0.6569\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 1.6599 Acc: 0.6794\n",
      "val Loss: 1.6032 Acc: 0.6873\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 1.5684 Acc: 0.6922\n",
      "val Loss: 1.5751 Acc: 0.6892\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 1.5668 Acc: 0.6794\n",
      "val Loss: 1.5475 Acc: 0.6873\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 1.5169 Acc: 0.7000\n",
      "val Loss: 1.5217 Acc: 0.6882\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 1.4223 Acc: 0.7186\n",
      "val Loss: 1.4878 Acc: 0.6882\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 1.5147 Acc: 0.6863\n",
      "val Loss: 1.4927 Acc: 0.6784\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 1.4208 Acc: 0.7225\n",
      "val Loss: 1.4542 Acc: 0.6990\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 1.4092 Acc: 0.7157\n",
      "val Loss: 1.4363 Acc: 0.7069\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 1.3448 Acc: 0.7363\n",
      "val Loss: 1.4038 Acc: 0.7088\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 1.3254 Acc: 0.7363\n",
      "val Loss: 1.3824 Acc: 0.7059\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 1.3965 Acc: 0.7098\n",
      "val Loss: 1.3767 Acc: 0.7010\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 1.3063 Acc: 0.7216\n",
      "val Loss: 1.3778 Acc: 0.6980\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 1.2589 Acc: 0.7412\n",
      "val Loss: 1.3544 Acc: 0.7000\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 1.2798 Acc: 0.7255\n",
      "val Loss: 1.3406 Acc: 0.7108\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 1.2664 Acc: 0.7382\n",
      "val Loss: 1.3352 Acc: 0.7118\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 1.2268 Acc: 0.7461\n",
      "val Loss: 1.3057 Acc: 0.7157\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 1.2213 Acc: 0.7480\n",
      "val Loss: 1.3016 Acc: 0.7118\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 1.2071 Acc: 0.7500\n",
      "val Loss: 1.2887 Acc: 0.7167\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 1.1172 Acc: 0.7686\n",
      "val Loss: 1.3019 Acc: 0.7039\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 1.1304 Acc: 0.7657\n",
      "val Loss: 1.2739 Acc: 0.7157\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 1.1128 Acc: 0.7657\n",
      "val Loss: 1.2741 Acc: 0.7157\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 1.1413 Acc: 0.7618\n",
      "val Loss: 1.2822 Acc: 0.7147\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 1.2083 Acc: 0.7412\n",
      "val Loss: 1.2743 Acc: 0.7059\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 1.1341 Acc: 0.7627\n",
      "val Loss: 1.2611 Acc: 0.7137\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 1.0500 Acc: 0.7775\n",
      "val Loss: 1.2482 Acc: 0.7127\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 1.0886 Acc: 0.7598\n",
      "val Loss: 1.2427 Acc: 0.7127\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 1.1148 Acc: 0.7539\n",
      "val Loss: 1.2461 Acc: 0.7118\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.9932 Acc: 0.7971\n",
      "val Loss: 1.2431 Acc: 0.7108\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 1.0697 Acc: 0.7843\n",
      "val Loss: 1.2489 Acc: 0.7029\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 1.1038 Acc: 0.7794\n",
      "val Loss: 1.2310 Acc: 0.7176\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 1.0186 Acc: 0.7863\n",
      "val Loss: 1.2303 Acc: 0.7069\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.9876 Acc: 0.7912\n",
      "val Loss: 1.2155 Acc: 0.7167\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.9808 Acc: 0.7873\n",
      "val Loss: 1.2126 Acc: 0.7098\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.9328 Acc: 0.8088\n",
      "val Loss: 1.2034 Acc: 0.7127\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.9754 Acc: 0.8078\n",
      "val Loss: 1.1973 Acc: 0.7176\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.9635 Acc: 0.7882\n",
      "val Loss: 1.1944 Acc: 0.7157\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.9730 Acc: 0.8078\n",
      "val Loss: 1.1943 Acc: 0.7235\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.9065 Acc: 0.8284\n",
      "val Loss: 1.1920 Acc: 0.7235\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.9799 Acc: 0.8049\n",
      "val Loss: 1.1990 Acc: 0.7167\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 1.0154 Acc: 0.7863\n",
      "val Loss: 1.1868 Acc: 0.7225\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.9097 Acc: 0.8206\n",
      "val Loss: 1.1819 Acc: 0.7245\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.9319 Acc: 0.8127\n",
      "val Loss: 1.1866 Acc: 0.7206\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.9371 Acc: 0.8118\n",
      "val Loss: 1.1892 Acc: 0.7186\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.9700 Acc: 0.8010\n",
      "val Loss: 1.1910 Acc: 0.7186\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.9042 Acc: 0.8167\n",
      "val Loss: 1.1800 Acc: 0.7284\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.9490 Acc: 0.8157\n",
      "val Loss: 1.1814 Acc: 0.7284\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.9360 Acc: 0.8078\n",
      "val Loss: 1.1774 Acc: 0.7225\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.9655 Acc: 0.8039\n",
      "val Loss: 1.1892 Acc: 0.7225\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.9604 Acc: 0.8020\n",
      "val Loss: 1.1868 Acc: 0.7206\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.9058 Acc: 0.8137\n",
      "val Loss: 1.1808 Acc: 0.7255\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.9055 Acc: 0.8186\n",
      "val Loss: 1.1807 Acc: 0.7245\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.8996 Acc: 0.8118\n",
      "val Loss: 1.1844 Acc: 0.7206\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.9012 Acc: 0.8157\n",
      "val Loss: 1.1781 Acc: 0.7265\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.9670 Acc: 0.7980\n",
      "val Loss: 1.1800 Acc: 0.7294\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.8682 Acc: 0.8324\n",
      "val Loss: 1.1771 Acc: 0.7275\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.9032 Acc: 0.8235\n",
      "val Loss: 1.1745 Acc: 0.7304\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.9253 Acc: 0.8206\n",
      "val Loss: 1.1835 Acc: 0.7245\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.9488 Acc: 0.7961\n",
      "val Loss: 1.1810 Acc: 0.7186\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.9227 Acc: 0.8127\n",
      "val Loss: 1.1700 Acc: 0.7284\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.9375 Acc: 0.8147\n",
      "val Loss: 1.1814 Acc: 0.7284\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.9306 Acc: 0.8108\n",
      "val Loss: 1.1725 Acc: 0.7245\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.9475 Acc: 0.8108\n",
      "val Loss: 1.1798 Acc: 0.7225\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.8984 Acc: 0.8108\n",
      "val Loss: 1.1718 Acc: 0.7206\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.9088 Acc: 0.8078\n",
      "val Loss: 1.1760 Acc: 0.7206\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.9571 Acc: 0.7971\n",
      "val Loss: 1.1798 Acc: 0.7245\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.9388 Acc: 0.8078\n",
      "val Loss: 1.1865 Acc: 0.7216\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.9780 Acc: 0.7990\n",
      "val Loss: 1.1852 Acc: 0.7186\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.9051 Acc: 0.8176\n",
      "val Loss: 1.1804 Acc: 0.7235\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.8998 Acc: 0.8186\n",
      "val Loss: 1.1783 Acc: 0.7255\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.9033 Acc: 0.8157\n",
      "val Loss: 1.1731 Acc: 0.7235\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.9631 Acc: 0.7902\n",
      "val Loss: 1.1810 Acc: 0.7255\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.9222 Acc: 0.8176\n",
      "val Loss: 1.1774 Acc: 0.7255\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.9345 Acc: 0.8010\n",
      "val Loss: 1.1751 Acc: 0.7245\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.9582 Acc: 0.7941\n",
      "val Loss: 1.1727 Acc: 0.7235\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.9072 Acc: 0.8255\n",
      "val Loss: 1.1698 Acc: 0.7206\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.9068 Acc: 0.8304\n",
      "val Loss: 1.1712 Acc: 0.7225\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.8776 Acc: 0.8176\n",
      "val Loss: 1.1807 Acc: 0.7206\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.9439 Acc: 0.8039\n",
      "val Loss: 1.1775 Acc: 0.7216\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.9171 Acc: 0.7990\n",
      "val Loss: 1.1774 Acc: 0.7167\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.8996 Acc: 0.8069\n",
      "val Loss: 1.1650 Acc: 0.7245\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.9363 Acc: 0.8167\n",
      "val Loss: 1.1689 Acc: 0.7245\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.9021 Acc: 0.8225\n",
      "val Loss: 1.1745 Acc: 0.7196\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.9062 Acc: 0.8225\n",
      "val Loss: 1.1796 Acc: 0.7265\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.9153 Acc: 0.8196\n",
      "val Loss: 1.1730 Acc: 0.7176\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.9141 Acc: 0.8196\n",
      "val Loss: 1.1709 Acc: 0.7225\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.8854 Acc: 0.8225\n",
      "val Loss: 1.1669 Acc: 0.7225\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.8807 Acc: 0.8118\n",
      "val Loss: 1.1719 Acc: 0.7216\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.9092 Acc: 0.8029\n",
      "val Loss: 1.1698 Acc: 0.7216\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.9337 Acc: 0.8127\n",
      "val Loss: 1.1709 Acc: 0.7245\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.8336 Acc: 0.8333\n",
      "val Loss: 1.1634 Acc: 0.7235\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.9286 Acc: 0.8039\n",
      "val Loss: 1.1778 Acc: 0.7275\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.8808 Acc: 0.8118\n",
      "val Loss: 1.1671 Acc: 0.7245\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.9098 Acc: 0.8010\n",
      "val Loss: 1.1678 Acc: 0.7245\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.8544 Acc: 0.8382\n",
      "val Loss: 1.1662 Acc: 0.7255\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.8351 Acc: 0.8451\n",
      "val Loss: 1.1673 Acc: 0.7225\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.8876 Acc: 0.8206\n",
      "val Loss: 1.1619 Acc: 0.7304\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.9250 Acc: 0.8127\n",
      "val Loss: 1.1697 Acc: 0.7235\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.9301 Acc: 0.7863\n",
      "val Loss: 1.1729 Acc: 0.7196\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.9261 Acc: 0.7990\n",
      "val Loss: 1.1700 Acc: 0.7206\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.8985 Acc: 0.8235\n",
      "val Loss: 1.1739 Acc: 0.7235\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.8853 Acc: 0.8265\n",
      "val Loss: 1.1678 Acc: 0.7225\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.9076 Acc: 0.8206\n",
      "val Loss: 1.1739 Acc: 0.7255\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.8414 Acc: 0.8304\n",
      "val Loss: 1.1693 Acc: 0.7275\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.8755 Acc: 0.8353\n",
      "val Loss: 1.1647 Acc: 0.7235\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.9195 Acc: 0.8069\n",
      "val Loss: 1.1703 Acc: 0.7216\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.8983 Acc: 0.8137\n",
      "val Loss: 1.1683 Acc: 0.7216\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.9272 Acc: 0.8098\n",
      "val Loss: 1.1710 Acc: 0.7245\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.8693 Acc: 0.8225\n",
      "val Loss: 1.1704 Acc: 0.7275\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.8828 Acc: 0.8304\n",
      "val Loss: 1.1648 Acc: 0.7235\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.8912 Acc: 0.8186\n",
      "val Loss: 1.1763 Acc: 0.7196\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.8802 Acc: 0.8127\n",
      "val Loss: 1.1696 Acc: 0.7265\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.9485 Acc: 0.8098\n",
      "val Loss: 1.1711 Acc: 0.7255\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.8769 Acc: 0.8245\n",
      "val Loss: 1.1677 Acc: 0.7216\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.9148 Acc: 0.8157\n",
      "val Loss: 1.1702 Acc: 0.7245\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.8940 Acc: 0.8137\n",
      "val Loss: 1.1714 Acc: 0.7255\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.8580 Acc: 0.8275\n",
      "val Loss: 1.1667 Acc: 0.7245\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.9086 Acc: 0.8029\n",
      "val Loss: 1.1698 Acc: 0.7235\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.8925 Acc: 0.8196\n",
      "val Loss: 1.1707 Acc: 0.7196\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.9170 Acc: 0.8118\n",
      "val Loss: 1.1720 Acc: 0.7225\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.9270 Acc: 0.7882\n",
      "val Loss: 1.1723 Acc: 0.7186\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.8885 Acc: 0.8255\n",
      "val Loss: 1.1738 Acc: 0.7186\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.9095 Acc: 0.8137\n",
      "val Loss: 1.1723 Acc: 0.7225\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.9408 Acc: 0.7961\n",
      "val Loss: 1.1787 Acc: 0.7186\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.9079 Acc: 0.8147\n",
      "val Loss: 1.1740 Acc: 0.7206\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.9011 Acc: 0.8196\n",
      "val Loss: 1.1728 Acc: 0.7206\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.8939 Acc: 0.8216\n",
      "val Loss: 1.1611 Acc: 0.7245\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.9322 Acc: 0.8049\n",
      "val Loss: 1.1729 Acc: 0.7245\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.9276 Acc: 0.8088\n",
      "val Loss: 1.1798 Acc: 0.7216\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.9110 Acc: 0.8206\n",
      "val Loss: 1.1763 Acc: 0.7235\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.9226 Acc: 0.8049\n",
      "val Loss: 1.1733 Acc: 0.7265\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.8802 Acc: 0.8167\n",
      "val Loss: 1.1673 Acc: 0.7225\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.8963 Acc: 0.8118\n",
      "val Loss: 1.1649 Acc: 0.7216\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.8672 Acc: 0.8118\n",
      "val Loss: 1.1648 Acc: 0.7255\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.9134 Acc: 0.8039\n",
      "val Loss: 1.1722 Acc: 0.7245\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.8595 Acc: 0.8382\n",
      "val Loss: 1.1666 Acc: 0.7235\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.8823 Acc: 0.8186\n",
      "val Loss: 1.1596 Acc: 0.7255\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.9335 Acc: 0.8137\n",
      "val Loss: 1.1654 Acc: 0.7216\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.9264 Acc: 0.8147\n",
      "val Loss: 1.1736 Acc: 0.7255\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.9061 Acc: 0.8265\n",
      "val Loss: 1.1751 Acc: 0.7255\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.9149 Acc: 0.8167\n",
      "val Loss: 1.1746 Acc: 0.7167\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.8954 Acc: 0.8196\n",
      "val Loss: 1.1703 Acc: 0.7235\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.8631 Acc: 0.8275\n",
      "val Loss: 1.1650 Acc: 0.7245\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.8323 Acc: 0.8422\n",
      "val Loss: 1.1604 Acc: 0.7206\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.8694 Acc: 0.8186\n",
      "val Loss: 1.1625 Acc: 0.7255\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.8890 Acc: 0.8333\n",
      "val Loss: 1.1728 Acc: 0.7245\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.9338 Acc: 0.7971\n",
      "val Loss: 1.1683 Acc: 0.7196\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.8871 Acc: 0.8059\n",
      "val Loss: 1.1739 Acc: 0.7167\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.8997 Acc: 0.8235\n",
      "val Loss: 1.1724 Acc: 0.7206\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.8475 Acc: 0.8304\n",
      "val Loss: 1.1645 Acc: 0.7235\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.8665 Acc: 0.8245\n",
      "val Loss: 1.1738 Acc: 0.7255\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.9120 Acc: 0.8118\n",
      "val Loss: 1.1634 Acc: 0.7235\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.9399 Acc: 0.8176\n",
      "val Loss: 1.1698 Acc: 0.7235\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.9563 Acc: 0.8098\n",
      "val Loss: 1.1801 Acc: 0.7235\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.8548 Acc: 0.8363\n",
      "val Loss: 1.1689 Acc: 0.7196\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.9037 Acc: 0.8108\n",
      "val Loss: 1.1657 Acc: 0.7235\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.8846 Acc: 0.8078\n",
      "val Loss: 1.1606 Acc: 0.7235\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.9207 Acc: 0.7990\n",
      "val Loss: 1.1657 Acc: 0.7245\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.8739 Acc: 0.8118\n",
      "val Loss: 1.1633 Acc: 0.7265\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.9007 Acc: 0.8078\n",
      "val Loss: 1.1759 Acc: 0.7216\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.8802 Acc: 0.8167\n",
      "val Loss: 1.1696 Acc: 0.7235\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.8298 Acc: 0.8363\n",
      "val Loss: 1.1662 Acc: 0.7304\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.8655 Acc: 0.8206\n",
      "val Loss: 1.1637 Acc: 0.7284\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.8994 Acc: 0.8206\n",
      "val Loss: 1.1663 Acc: 0.7225\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.8935 Acc: 0.8304\n",
      "val Loss: 1.1684 Acc: 0.7265\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.8270 Acc: 0.8314\n",
      "val Loss: 1.1630 Acc: 0.7235\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 0.8973 Acc: 0.8167\n",
      "val Loss: 1.1759 Acc: 0.7206\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 0.8922 Acc: 0.8167\n",
      "val Loss: 1.1668 Acc: 0.7196\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 0.8568 Acc: 0.8265\n",
      "val Loss: 1.1646 Acc: 0.7255\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 0.9103 Acc: 0.8137\n",
      "val Loss: 1.1712 Acc: 0.7235\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 0.8948 Acc: 0.8176\n",
      "val Loss: 1.1700 Acc: 0.7245\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 0.8547 Acc: 0.8294\n",
      "val Loss: 1.1671 Acc: 0.7265\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 0.9107 Acc: 0.8157\n",
      "val Loss: 1.1699 Acc: 0.7216\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 0.9638 Acc: 0.7922\n",
      "val Loss: 1.1685 Acc: 0.7255\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 0.9201 Acc: 0.8059\n",
      "val Loss: 1.1729 Acc: 0.7196\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 0.9033 Acc: 0.8196\n",
      "val Loss: 1.1696 Acc: 0.7216\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 0.9001 Acc: 0.8275\n",
      "val Loss: 1.1660 Acc: 0.7216\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 0.9033 Acc: 0.7961\n",
      "val Loss: 1.1713 Acc: 0.7206\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 0.8914 Acc: 0.8265\n",
      "val Loss: 1.1610 Acc: 0.7216\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 0.9145 Acc: 0.8069\n",
      "val Loss: 1.1698 Acc: 0.7225\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 0.8866 Acc: 0.8127\n",
      "val Loss: 1.1670 Acc: 0.7255\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 0.8798 Acc: 0.8186\n",
      "val Loss: 1.1681 Acc: 0.7245\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 0.8686 Acc: 0.8235\n",
      "val Loss: 1.1691 Acc: 0.7235\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 0.9362 Acc: 0.8127\n",
      "val Loss: 1.1695 Acc: 0.7255\n",
      "\n",
      "Training complete in 29m 38s\n",
      "Best val Acc: 0.730392\n"
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=num_epochs/4, gamma=0.1)\n",
    "\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train Loss: 0.9362 Acc: 0.8127\n",
    "\n",
    "val Loss: 1.1695 Acc: 0.7255\n",
    "\n",
    "Training complete in 29m 38s\n",
    "\n",
    "Best val Acc: 0.730392"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbPAs3JSgsxU"
   },
   "source": [
    "# lr = 0.001 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "BX829xQKgt1C",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5a8f0d60-25b0-4cb4-dae0-13a8a0d5c45a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 4.6687 Acc: 0.0069\n",
      "val Loss: 4.6127 Acc: 0.0127\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 4.6069 Acc: 0.0147\n",
      "val Loss: 4.5425 Acc: 0.0392\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 4.5551 Acc: 0.0304\n",
      "val Loss: 4.4865 Acc: 0.0676\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 4.4918 Acc: 0.0569\n",
      "val Loss: 4.4352 Acc: 0.0951\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 4.4433 Acc: 0.0971\n",
      "val Loss: 4.3834 Acc: 0.1343\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 4.3901 Acc: 0.1363\n",
      "val Loss: 4.3307 Acc: 0.1618\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 4.3572 Acc: 0.1422\n",
      "val Loss: 4.2797 Acc: 0.1971\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 4.2864 Acc: 0.2127\n",
      "val Loss: 4.2260 Acc: 0.2520\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 4.2569 Acc: 0.2275\n",
      "val Loss: 4.1797 Acc: 0.2833\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 4.2080 Acc: 0.2520\n",
      "val Loss: 4.1293 Acc: 0.3020\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 4.1583 Acc: 0.2892\n",
      "val Loss: 4.0814 Acc: 0.3549\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 4.1085 Acc: 0.3225\n",
      "val Loss: 4.0344 Acc: 0.3608\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 4.0618 Acc: 0.3627\n",
      "val Loss: 3.9898 Acc: 0.3735\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 4.0281 Acc: 0.3686\n",
      "val Loss: 3.9476 Acc: 0.3961\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 3.9871 Acc: 0.3745\n",
      "val Loss: 3.8957 Acc: 0.4167\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 3.9383 Acc: 0.4059\n",
      "val Loss: 3.8522 Acc: 0.4196\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 3.8859 Acc: 0.4245\n",
      "val Loss: 3.8110 Acc: 0.4314\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 3.8683 Acc: 0.3971\n",
      "val Loss: 3.7676 Acc: 0.4412\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 3.8032 Acc: 0.4255\n",
      "val Loss: 3.7255 Acc: 0.4578\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 3.7762 Acc: 0.4686\n",
      "val Loss: 3.6881 Acc: 0.4549\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 3.7204 Acc: 0.4686\n",
      "val Loss: 3.6504 Acc: 0.4618\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 3.6982 Acc: 0.4647\n",
      "val Loss: 3.6123 Acc: 0.4745\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 3.6590 Acc: 0.4618\n",
      "val Loss: 3.5712 Acc: 0.4618\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 3.6119 Acc: 0.4824\n",
      "val Loss: 3.5305 Acc: 0.4873\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 3.5621 Acc: 0.4922\n",
      "val Loss: 3.4966 Acc: 0.4961\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 3.5384 Acc: 0.5029\n",
      "val Loss: 3.4563 Acc: 0.4922\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 3.4875 Acc: 0.5284\n",
      "val Loss: 3.4180 Acc: 0.5098\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 3.4578 Acc: 0.5294\n",
      "val Loss: 3.3975 Acc: 0.4912\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 3.4436 Acc: 0.5157\n",
      "val Loss: 3.3484 Acc: 0.5069\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 3.3941 Acc: 0.5294\n",
      "val Loss: 3.3161 Acc: 0.5225\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 3.3690 Acc: 0.5373\n",
      "val Loss: 3.2943 Acc: 0.5108\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 3.3278 Acc: 0.5265\n",
      "val Loss: 3.2406 Acc: 0.5265\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 3.2874 Acc: 0.5510\n",
      "val Loss: 3.2123 Acc: 0.5392\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 3.2707 Acc: 0.5637\n",
      "val Loss: 3.1849 Acc: 0.5324\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 3.2700 Acc: 0.5451\n",
      "val Loss: 3.1568 Acc: 0.5392\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 3.2036 Acc: 0.5569\n",
      "val Loss: 3.1235 Acc: 0.5353\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 3.1935 Acc: 0.5471\n",
      "val Loss: 3.0960 Acc: 0.5529\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 3.1443 Acc: 0.5706\n",
      "val Loss: 3.0706 Acc: 0.5539\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 3.1619 Acc: 0.5529\n",
      "val Loss: 3.0485 Acc: 0.5539\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 3.0979 Acc: 0.5735\n",
      "val Loss: 3.0122 Acc: 0.5529\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 3.0704 Acc: 0.5814\n",
      "val Loss: 2.9859 Acc: 0.5569\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 3.0452 Acc: 0.5676\n",
      "val Loss: 2.9480 Acc: 0.5637\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 3.0339 Acc: 0.5941\n",
      "val Loss: 2.9375 Acc: 0.5627\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 2.9701 Acc: 0.6069\n",
      "val Loss: 2.9033 Acc: 0.5657\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 2.9641 Acc: 0.5775\n",
      "val Loss: 2.8855 Acc: 0.5657\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 2.9377 Acc: 0.5990\n",
      "val Loss: 2.8615 Acc: 0.5696\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 2.9383 Acc: 0.5784\n",
      "val Loss: 2.8332 Acc: 0.5667\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 2.9007 Acc: 0.5931\n",
      "val Loss: 2.8191 Acc: 0.5637\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 2.8533 Acc: 0.5971\n",
      "val Loss: 2.7851 Acc: 0.5745\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 2.8453 Acc: 0.6069\n",
      "val Loss: 2.7554 Acc: 0.5873\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 2.8042 Acc: 0.6157\n",
      "val Loss: 2.7524 Acc: 0.5951\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 2.8167 Acc: 0.5980\n",
      "val Loss: 2.7583 Acc: 0.5873\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 2.7930 Acc: 0.6343\n",
      "val Loss: 2.7552 Acc: 0.5843\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 2.8004 Acc: 0.6118\n",
      "val Loss: 2.7508 Acc: 0.5922\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 2.8341 Acc: 0.5853\n",
      "val Loss: 2.7463 Acc: 0.5892\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 2.8281 Acc: 0.6069\n",
      "val Loss: 2.7473 Acc: 0.5863\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 2.7951 Acc: 0.6284\n",
      "val Loss: 2.7449 Acc: 0.5873\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 2.8055 Acc: 0.6167\n",
      "val Loss: 2.7385 Acc: 0.5922\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 2.8076 Acc: 0.5971\n",
      "val Loss: 2.7370 Acc: 0.5922\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 2.8208 Acc: 0.5775\n",
      "val Loss: 2.7471 Acc: 0.5902\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 2.7881 Acc: 0.6206\n",
      "val Loss: 2.7327 Acc: 0.5961\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 2.7879 Acc: 0.6284\n",
      "val Loss: 2.7406 Acc: 0.5863\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 2.7715 Acc: 0.6206\n",
      "val Loss: 2.7338 Acc: 0.5873\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 2.7588 Acc: 0.6490\n",
      "val Loss: 2.7250 Acc: 0.5941\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 2.8009 Acc: 0.6108\n",
      "val Loss: 2.7369 Acc: 0.5873\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 2.8033 Acc: 0.6088\n",
      "val Loss: 2.7339 Acc: 0.5912\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 2.7586 Acc: 0.6167\n",
      "val Loss: 2.7206 Acc: 0.5931\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 2.7734 Acc: 0.6294\n",
      "val Loss: 2.7212 Acc: 0.5882\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 2.7606 Acc: 0.6157\n",
      "val Loss: 2.7164 Acc: 0.5931\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 2.7718 Acc: 0.6275\n",
      "val Loss: 2.7209 Acc: 0.5902\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 2.7595 Acc: 0.6196\n",
      "val Loss: 2.7149 Acc: 0.5892\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 2.8037 Acc: 0.5980\n",
      "val Loss: 2.7170 Acc: 0.5961\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 2.7776 Acc: 0.6088\n",
      "val Loss: 2.7045 Acc: 0.5912\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 2.7942 Acc: 0.5971\n",
      "val Loss: 2.7153 Acc: 0.5931\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 2.7558 Acc: 0.6137\n",
      "val Loss: 2.7075 Acc: 0.5912\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 2.7461 Acc: 0.6235\n",
      "val Loss: 2.7039 Acc: 0.5902\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 2.7597 Acc: 0.6275\n",
      "val Loss: 2.7050 Acc: 0.5892\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 2.7556 Acc: 0.6127\n",
      "val Loss: 2.7183 Acc: 0.5863\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 2.7645 Acc: 0.6167\n",
      "val Loss: 2.6960 Acc: 0.5931\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 2.7434 Acc: 0.6255\n",
      "val Loss: 2.6971 Acc: 0.5951\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 2.7386 Acc: 0.6216\n",
      "val Loss: 2.6793 Acc: 0.6029\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 2.7512 Acc: 0.6078\n",
      "val Loss: 2.6912 Acc: 0.5902\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 2.7586 Acc: 0.6324\n",
      "val Loss: 2.6815 Acc: 0.5941\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 2.7163 Acc: 0.6314\n",
      "val Loss: 2.6797 Acc: 0.5971\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 2.7954 Acc: 0.6069\n",
      "val Loss: 2.6974 Acc: 0.5922\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 2.7336 Acc: 0.6471\n",
      "val Loss: 2.6847 Acc: 0.5892\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 2.7311 Acc: 0.6206\n",
      "val Loss: 2.6867 Acc: 0.5922\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 2.7241 Acc: 0.6127\n",
      "val Loss: 2.6854 Acc: 0.5902\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 2.7265 Acc: 0.6392\n",
      "val Loss: 2.6839 Acc: 0.5951\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 2.7398 Acc: 0.6176\n",
      "val Loss: 2.6842 Acc: 0.5931\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 2.7256 Acc: 0.6245\n",
      "val Loss: 2.6752 Acc: 0.5971\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 2.7315 Acc: 0.6196\n",
      "val Loss: 2.6636 Acc: 0.5971\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 2.7087 Acc: 0.6127\n",
      "val Loss: 2.6708 Acc: 0.5941\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 2.7265 Acc: 0.6176\n",
      "val Loss: 2.6681 Acc: 0.5961\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 2.7467 Acc: 0.6255\n",
      "val Loss: 2.6666 Acc: 0.5980\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 2.6931 Acc: 0.6225\n",
      "val Loss: 2.6661 Acc: 0.5941\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 2.7009 Acc: 0.6265\n",
      "val Loss: 2.6595 Acc: 0.5971\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 2.7558 Acc: 0.6069\n",
      "val Loss: 2.6627 Acc: 0.5980\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 2.6984 Acc: 0.6216\n",
      "val Loss: 2.6580 Acc: 0.6010\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 2.7017 Acc: 0.6402\n",
      "val Loss: 2.6595 Acc: 0.5971\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 2.7145 Acc: 0.6275\n",
      "val Loss: 2.6556 Acc: 0.5931\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 2.7395 Acc: 0.6108\n",
      "val Loss: 2.6605 Acc: 0.5941\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 2.6864 Acc: 0.6363\n",
      "val Loss: 2.6467 Acc: 0.6020\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 2.7123 Acc: 0.6265\n",
      "val Loss: 2.6595 Acc: 0.5980\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 2.6781 Acc: 0.6461\n",
      "val Loss: 2.6561 Acc: 0.6020\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 2.7005 Acc: 0.6245\n",
      "val Loss: 2.6492 Acc: 0.5961\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 2.7155 Acc: 0.6206\n",
      "val Loss: 2.6526 Acc: 0.5902\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 2.7324 Acc: 0.6108\n",
      "val Loss: 2.6638 Acc: 0.5863\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 2.7223 Acc: 0.6216\n",
      "val Loss: 2.6512 Acc: 0.5931\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 2.7053 Acc: 0.6284\n",
      "val Loss: 2.6521 Acc: 0.5941\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 2.7067 Acc: 0.6196\n",
      "val Loss: 2.6523 Acc: 0.5980\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 2.7075 Acc: 0.6088\n",
      "val Loss: 2.6534 Acc: 0.5971\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 2.6986 Acc: 0.6314\n",
      "val Loss: 2.6625 Acc: 0.5902\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 2.6821 Acc: 0.6206\n",
      "val Loss: 2.6580 Acc: 0.5961\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 2.7016 Acc: 0.6147\n",
      "val Loss: 2.6520 Acc: 0.5980\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 2.7224 Acc: 0.6137\n",
      "val Loss: 2.6512 Acc: 0.5961\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 2.7334 Acc: 0.6010\n",
      "val Loss: 2.6567 Acc: 0.5951\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 2.7086 Acc: 0.6127\n",
      "val Loss: 2.6566 Acc: 0.5951\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 2.6678 Acc: 0.6569\n",
      "val Loss: 2.6516 Acc: 0.6000\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 2.7324 Acc: 0.6265\n",
      "val Loss: 2.6598 Acc: 0.5941\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 2.7168 Acc: 0.6059\n",
      "val Loss: 2.6579 Acc: 0.5931\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 2.6635 Acc: 0.6441\n",
      "val Loss: 2.6389 Acc: 0.5961\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 2.6846 Acc: 0.6412\n",
      "val Loss: 2.6437 Acc: 0.5971\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 2.7279 Acc: 0.6118\n",
      "val Loss: 2.6650 Acc: 0.5922\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 2.7100 Acc: 0.6186\n",
      "val Loss: 2.6565 Acc: 0.5971\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 2.7064 Acc: 0.6206\n",
      "val Loss: 2.6519 Acc: 0.6010\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 2.7208 Acc: 0.6049\n",
      "val Loss: 2.6548 Acc: 0.5961\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 2.7386 Acc: 0.6284\n",
      "val Loss: 2.6663 Acc: 0.5951\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 2.7243 Acc: 0.6284\n",
      "val Loss: 2.6541 Acc: 0.5990\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 2.7073 Acc: 0.6186\n",
      "val Loss: 2.6575 Acc: 0.5980\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 2.7123 Acc: 0.6275\n",
      "val Loss: 2.6538 Acc: 0.5990\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 2.6883 Acc: 0.6324\n",
      "val Loss: 2.6409 Acc: 0.5971\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 2.6914 Acc: 0.6255\n",
      "val Loss: 2.6336 Acc: 0.6029\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 2.7278 Acc: 0.6245\n",
      "val Loss: 2.6423 Acc: 0.6010\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 2.7009 Acc: 0.6314\n",
      "val Loss: 2.6441 Acc: 0.6039\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 2.7330 Acc: 0.6088\n",
      "val Loss: 2.6504 Acc: 0.5971\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 2.6832 Acc: 0.6255\n",
      "val Loss: 2.6492 Acc: 0.5980\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 2.7257 Acc: 0.6078\n",
      "val Loss: 2.6520 Acc: 0.6020\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 2.6855 Acc: 0.6284\n",
      "val Loss: 2.6453 Acc: 0.5971\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 2.6808 Acc: 0.6333\n",
      "val Loss: 2.6494 Acc: 0.6029\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 2.6945 Acc: 0.6333\n",
      "val Loss: 2.6469 Acc: 0.5961\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 2.6586 Acc: 0.6353\n",
      "val Loss: 2.6393 Acc: 0.6029\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 2.7237 Acc: 0.6216\n",
      "val Loss: 2.6525 Acc: 0.5961\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 2.7061 Acc: 0.6098\n",
      "val Loss: 2.6494 Acc: 0.5941\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 2.7075 Acc: 0.6176\n",
      "val Loss: 2.6412 Acc: 0.6059\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 2.7077 Acc: 0.6167\n",
      "val Loss: 2.6419 Acc: 0.6000\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 2.6525 Acc: 0.6373\n",
      "val Loss: 2.6476 Acc: 0.5990\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 2.6854 Acc: 0.6294\n",
      "val Loss: 2.6374 Acc: 0.5971\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 2.6997 Acc: 0.6157\n",
      "val Loss: 2.6309 Acc: 0.6010\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 2.7136 Acc: 0.6255\n",
      "val Loss: 2.6486 Acc: 0.5980\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 2.7124 Acc: 0.6098\n",
      "val Loss: 2.6431 Acc: 0.6039\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 2.7193 Acc: 0.6167\n",
      "val Loss: 2.6450 Acc: 0.6010\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 2.6905 Acc: 0.6255\n",
      "val Loss: 2.6432 Acc: 0.6029\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 2.6648 Acc: 0.6324\n",
      "val Loss: 2.6431 Acc: 0.5971\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 2.6759 Acc: 0.6392\n",
      "val Loss: 2.6417 Acc: 0.6000\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 2.6936 Acc: 0.6304\n",
      "val Loss: 2.6435 Acc: 0.6010\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 2.7355 Acc: 0.6069\n",
      "val Loss: 2.6465 Acc: 0.5961\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 2.6610 Acc: 0.6637\n",
      "val Loss: 2.6417 Acc: 0.6059\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 2.6441 Acc: 0.6480\n",
      "val Loss: 2.6327 Acc: 0.6039\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 2.6782 Acc: 0.6284\n",
      "val Loss: 2.6445 Acc: 0.6039\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 2.7013 Acc: 0.6294\n",
      "val Loss: 2.6396 Acc: 0.6039\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 2.7098 Acc: 0.6235\n",
      "val Loss: 2.6465 Acc: 0.5990\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 2.7214 Acc: 0.6059\n",
      "val Loss: 2.6521 Acc: 0.5951\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 2.7269 Acc: 0.6118\n",
      "val Loss: 2.6412 Acc: 0.6010\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 2.6935 Acc: 0.6284\n",
      "val Loss: 2.6509 Acc: 0.5961\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 2.6951 Acc: 0.6176\n",
      "val Loss: 2.6489 Acc: 0.5980\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 2.6947 Acc: 0.6265\n",
      "val Loss: 2.6386 Acc: 0.6039\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 2.7090 Acc: 0.6147\n",
      "val Loss: 2.6491 Acc: 0.6000\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 2.6714 Acc: 0.6333\n",
      "val Loss: 2.6379 Acc: 0.6059\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 2.6815 Acc: 0.6451\n",
      "val Loss: 2.6338 Acc: 0.6059\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 2.6850 Acc: 0.6098\n",
      "val Loss: 2.6535 Acc: 0.6010\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 2.6837 Acc: 0.6353\n",
      "val Loss: 2.6415 Acc: 0.6010\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 2.7121 Acc: 0.6176\n",
      "val Loss: 2.6550 Acc: 0.5980\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 2.7047 Acc: 0.6216\n",
      "val Loss: 2.6392 Acc: 0.5971\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 2.7160 Acc: 0.6333\n",
      "val Loss: 2.6438 Acc: 0.5980\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 2.7043 Acc: 0.6235\n",
      "val Loss: 2.6480 Acc: 0.5961\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 2.7232 Acc: 0.6098\n",
      "val Loss: 2.6472 Acc: 0.5961\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 2.6813 Acc: 0.6373\n",
      "val Loss: 2.6407 Acc: 0.6000\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 2.7038 Acc: 0.6176\n",
      "val Loss: 2.6449 Acc: 0.6000\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 2.6988 Acc: 0.6235\n",
      "val Loss: 2.6498 Acc: 0.5980\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 2.6619 Acc: 0.6265\n",
      "val Loss: 2.6369 Acc: 0.6039\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 2.7149 Acc: 0.6176\n",
      "val Loss: 2.6393 Acc: 0.6010\n",
      "\n",
      "Epoch 182/199\n",
      "----------\n",
      "train Loss: 2.6973 Acc: 0.6137\n",
      "val Loss: 2.6314 Acc: 0.6049\n",
      "\n",
      "Epoch 183/199\n",
      "----------\n",
      "train Loss: 2.6528 Acc: 0.6373\n",
      "val Loss: 2.6367 Acc: 0.6000\n",
      "\n",
      "Epoch 184/199\n",
      "----------\n",
      "train Loss: 2.6941 Acc: 0.6392\n",
      "val Loss: 2.6388 Acc: 0.6069\n",
      "\n",
      "Epoch 185/199\n",
      "----------\n",
      "train Loss: 2.7086 Acc: 0.6314\n",
      "val Loss: 2.6460 Acc: 0.6010\n",
      "\n",
      "Epoch 186/199\n",
      "----------\n",
      "train Loss: 2.7021 Acc: 0.6343\n",
      "val Loss: 2.6456 Acc: 0.6010\n",
      "\n",
      "Epoch 187/199\n",
      "----------\n",
      "train Loss: 2.7087 Acc: 0.6284\n",
      "val Loss: 2.6494 Acc: 0.5990\n",
      "\n",
      "Epoch 188/199\n",
      "----------\n",
      "train Loss: 2.7348 Acc: 0.6069\n",
      "val Loss: 2.6496 Acc: 0.5990\n",
      "\n",
      "Epoch 189/199\n",
      "----------\n",
      "train Loss: 2.6995 Acc: 0.6333\n",
      "val Loss: 2.6492 Acc: 0.5951\n",
      "\n",
      "Epoch 190/199\n",
      "----------\n",
      "train Loss: 2.6990 Acc: 0.6216\n",
      "val Loss: 2.6451 Acc: 0.5961\n",
      "\n",
      "Epoch 191/199\n",
      "----------\n",
      "train Loss: 2.6815 Acc: 0.6314\n",
      "val Loss: 2.6436 Acc: 0.6020\n",
      "\n",
      "Epoch 192/199\n",
      "----------\n",
      "train Loss: 2.7088 Acc: 0.6029\n",
      "val Loss: 2.6392 Acc: 0.5990\n",
      "\n",
      "Epoch 193/199\n",
      "----------\n",
      "train Loss: 2.6754 Acc: 0.6333\n",
      "val Loss: 2.6495 Acc: 0.6000\n",
      "\n",
      "Epoch 194/199\n",
      "----------\n",
      "train Loss: 2.6743 Acc: 0.6480\n",
      "val Loss: 2.6417 Acc: 0.5990\n",
      "\n",
      "Epoch 195/199\n",
      "----------\n",
      "train Loss: 2.7111 Acc: 0.6304\n",
      "val Loss: 2.6440 Acc: 0.5990\n",
      "\n",
      "Epoch 196/199\n",
      "----------\n",
      "train Loss: 2.6773 Acc: 0.6510\n",
      "val Loss: 2.6431 Acc: 0.6000\n",
      "\n",
      "Epoch 197/199\n",
      "----------\n",
      "train Loss: 2.6656 Acc: 0.6235\n",
      "val Loss: 2.6420 Acc: 0.5990\n",
      "\n",
      "Epoch 198/199\n",
      "----------\n",
      "train Loss: 2.6990 Acc: 0.6235\n",
      "val Loss: 2.6395 Acc: 0.5971\n",
      "\n",
      "Epoch 199/199\n",
      "----------\n",
      "train Loss: 2.6864 Acc: 0.6255\n",
      "val Loss: 2.6368 Acc: 0.5980\n",
      "\n",
      "Training complete in 29m 30s\n",
      "Best val Acc: 0.606863\n"
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=num_epochs/4, gamma=0.1)\n",
    "\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train Loss: 2.6864 Acc: 0.6255\n",
    "\n",
    "val Loss: 2.6368 Acc: 0.5980\n",
    "\n",
    "Training complete in 29m 30s\n",
    "\n",
    "Best val Acc: 0.606863"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# With  a learning rate of 1, the training accuracy after 200 epochs is  0.8520 and the  validation accuracy is 0.7029.\n",
    "\n",
    "# With  a learning rate of 0.1, the training accuracy after 200 epochs is  0.8686 and the  validation accuracy is 0.7039\n",
    "\n",
    "# With  a learning rate of 0.01, the training accuracy after 200 epochs is  0.8127 and the  validation accuracy is 0.7255.\n",
    "\n",
    "# With  a learning rate of 0.001, the training accuracy after 200 epochs is  0.6255 and the  validation accuracy is 0.5980.\n",
    "\n",
    "# The best accuracy on the validation set is the model with a learning rate of 0.01 and  the best accuracu on  the training set is  the model with a learning  rate  of  0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwuBjr6cg-Bs"
   },
   "source": [
    "# b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The  best accuracy on both training and validation set comes from finetuning the base model with  a learning  rate of 0.01 (0.9784  on training and 0.8206 on validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It makes sense that using the model only  as a  base feature  extractor (Q2) leads  to  poor  results because the dataset of the base  model  is  quite different  and therefore  the extracted learnt  features will not  be representative  of the target dataset. This is  why we  need to allow our  network to use this feature extraction  as an initialization but allow it to finetune  the weights to learn a  new  feature extraction by not  freezing  the weights. Learning will take more time and might cause some overfitting problems but it will certainly lead to better performances.\n",
    "\n",
    "# I think there are 2 other ways to improve performances :\n",
    "\n",
    "# - freeze the first few layers of the model will help generalization and save some training time without hurting performances as it has been shown that the first layers of a CNN learn common features across datasets  such  as edges.\n",
    "\n",
    "# - Use P2L approach and use a better base model for feature extraction as ImageNet might not be the best model for this target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "PB1_Yanis_TAZI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
