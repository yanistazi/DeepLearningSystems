{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Deep Reinforcement Learning :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episodic tasks are tasks that last only for a certain amount of time and at the end of the task  the agent get a reward. Chess game is an episodic task for example where basically at the end of  game the reward can be lose or  win the game.\n",
    "\n",
    "## Continuous tasks are tasks in which the agent continuously forever interact with the environment and keep learning and a reward is accumulated overtime. For example learning mathematic can be considered as a continuous taskwhere we keep learning overtime and improving our understanding. Basically, there is no end of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You want to take actions that maximize your reward but at the same time you want to explore certain actions not taken before because they might give us better reward but it may also happen that those actions will lead to bad rewards so the dilemma is exploring environments by taking random actions or exploiting what we have learnt so far and taking the actions that gave us the best reward so far. Maximize the reward while learning the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-greedy policy is trying to find a balance between exploration and exploitation . With probability $\\epsilon$ , you take a random action (exploring the environment) and with probability $1-\\epsilon$ , you select the action leading to the maximum expected returned so far.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\epsilon$ should decay overtime . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indeed, at the beginning of the task, we want large values of $\\epsilon$ so that  we explore and understand  our  environment , but after a certain time , we want to start maximize our rewards because we know a bit more about the environment and therefore we can explore less , this is why we decrease $\\epsilon$. High $\\epsilon$ values means taking lots of random actions  and therefore lots of exploration while low $\\epsilon$ values means that  we take with high probability the actions leading to the maximum expected returned s far so we exploit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Deep-Q learning is a way of achieving Q-learning by learning the  weights of the neural networks that will give us the mapping to Q.Basically, we have replaced an exact value function with a function approximator. They are 2 main improvements of DQN over Q-learning.\n",
    "## First,DQN uses  experience replay buffer : storing at each time step the experience tuple  in replay memory which cut down correlations.\n",
    "## Second, they use 2 separate networks : Q-network and target Q-network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm explained :\n",
    "\n",
    "\n",
    "\n",
    "## Initialization of replay memory D\n",
    "\n",
    "## Initialization of Q with random-weights\n",
    "\n",
    "## For different episodes (1 to M):\n",
    "\n",
    "##    We start by initializing a sequency with  a value s1 \n",
    "    \n",
    "##    For steps 1 to T :\n",
    "    \n",
    "##        With proba $\\epsilon$ (use of $\\epsilon$-greedy policy, the agent takes a random action) (exploration) else\n",
    "        \n",
    "##        the agent takes an action dictated by its behavior policy Q-network : the action  that maximizes the reward  (exploitation).\n",
    "\n",
    "##        Execute selected action to observe the reward associated and now this action leads  to :\n",
    "\n",
    "##        a new state  $s_{t+1}$ composed of previous  state $s_t$, action that you just took $a_t$ and the new image $x_{t+1}$ and transfrom $s_t$  using  $\\phi$ (function of the state)\n",
    "\n",
    " ##       Now, store this transition  : $\\{\\phi_t,a_t,r_t,\\phi_{t+1} \\}$ {old state,action,reward,new state}  in D, the  replay buffer.\n",
    "\n",
    "##        From time to time, sample randomly some mini batch of those stored  transitions from the replay buffer:\n",
    "\n",
    " ##       Set $y_j$ to be either : \n",
    "\n",
    "  ##      -$r_j$ meaning that the task is ending so it's the last reward you can get or:\n",
    "\n",
    "  ##      -$r_j+ \\gamma *$future  reward calculated from  target network.\n",
    "\n",
    "  ##      Then we take  the squared difference between  this $y_j$ and the Q-network value to perform gradient descent and learn the $\\theta$ parameters to update the parameters of the Q  network.\n",
    "\n",
    "  ##      Finally, every C steps we perform  synchronization of  the two Q networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One of the most important elements in DQN is using this target Q network because it is necessary to stabilize learning . The idea is to keep a copy of our neural network and use it for the Q(s’, a’) value in the Bellman equation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay helps by :\n",
    "    \n",
    "## -preventing instability in Q-learning\n",
    "\n",
    "## -reusability of the data to train the Q-network \n",
    "\n",
    "## -removing correlation in input batch samples and helps for generalization (like adding noise) \n",
    "\n",
    "## -prevents overfitting allowing agent to learn from previous versions of the policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The idea behind prioritized experienced replay is to use a clever way of sampling from replay buffer and not to sample randomly from the replay buffer.\n",
    "\n",
    "## Sampling using priority given to certain tuples is more optimal since some tuples might not be associated to the reward because of sparsity.\n",
    "\n",
    "## The information provided by the learners in the replay buffer is skewed and might not help to improve the learning.\n",
    "\n",
    "## Therefore we should replay the experiences providing us with the most surprising outcomes.\n",
    "\n",
    "## We label each experience with a priority to perform sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarities :\n",
    "\n",
    "## 1. They are both trying to learn the weights of a neural network to give them the mapping to Q\n",
    "## 2. They both have an acting part selecting an action a to apply in its environment and they have multiple acotors\n",
    "## 3. They both have  a learning part consisting of sampling data batches from the memory in order  to update the Q-network parameters and they have multiple learners.\n",
    "\n",
    "\n",
    "## Differences :\n",
    "## 1. in ape-X they use a shared centralized replayed  memory and do not sample uniformly but prioritize to sample the most useful data more often\n",
    "## 2. ape-X uses doubleQ-learning with multi-step bootstraps targets fior the learning algorithm\n",
    "## 3. ape-X uses a dueling network architecture as function approximator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
