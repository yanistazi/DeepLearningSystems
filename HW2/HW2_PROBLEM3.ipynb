{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YANIS TAZI HOMEWORK. 2 DEEP LEARNING SYSTEMS\n",
    "#### yt1600@nyu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-adaptation is when neurons depend highly on each other . This is a very important matters because one affected neuron (receiving bad input for example) will affect all the neurons that depend on this one and this is the kind of issue leading to overfitting for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal covariate shift refers to the change in the distribution of network activations due to change in network parameters during training. To reduce this, we can use normalization at each layer so that we achieve fix distribution of inputs for every layer. One of the most common technque is to use Batch normalization.\n",
    "### Internal covariate shift often leads to slow training and can create non convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LeNet 5 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taziy/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/taziy/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/taziy/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/taziy/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/taziy/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/taziy/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import models, layers\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import BatchNormalization, LayerNormalization\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "    from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Standard normalization for input layer and batch normalization for hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Data with standard normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset as train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Set numeric type to float32 from uint8\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize value to [0, 1]\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Standard normalization\n",
    "mean_train = x_train.mean()\n",
    "std_train = x_train.std()\n",
    "x_train -= mean_train\n",
    "x_train /= std_train\n",
    "\n",
    "x_test -= mean_train\n",
    "x_test /= std_train\n",
    "\n",
    "# Transform lables to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Reshape the dataset into 4D array\n",
    "x_train = x_train.reshape(x_train.shape[0], 28,28,1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28,28,1)\n",
    "\n",
    "x_train_std_input = x_train\n",
    "x_test_std_input = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.random.set_seed(17)\n",
    "\n",
    "#Instantiate an empty model\n",
    "model = Sequential()\n",
    "# C1 Convolutional Layer\n",
    "model.add(layers.Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=(28,28,1), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "# S2 Pooling Layer\n",
    "model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "# C3 Convolutional Layer\n",
    "model.add(layers.Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "# S4 Pooling Layer\n",
    "model.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "# C5 Fully Connected Convolutional Layer\n",
    "model.add(layers.Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "#Flatten the CNN output so that we can connect it with fully connected layers\n",
    "model.add(layers.Flatten())\n",
    "# FC6 Fully Connected Layer\n",
    "model.add(layers.Dense(84, activation='tanh'))\n",
    "model.add(BatchNormalization())\n",
    "#Output Layer with softmax activation\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer='SGD', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### While the original paper talks about applying batch norm just before the activation function, it has been found in practice that applying batch norm after the activation yields better results.\n",
    "### Therefore, I apply it after activation for the hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 6)         24        \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 27, 27, 6)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 27, 27, 6)         24        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 23, 23, 16)        2416      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 23, 23, 16)        64        \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 11, 11, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 11, 11, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 120)         48120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 7, 7, 120)         480       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5880)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 84)                494004    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 84)                336       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 546,538\n",
      "Trainable params: 546,042\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.2075 - accuracy: 0.9425 - val_loss: 0.0996 - val_accuracy: 0.9736\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0860 - accuracy: 0.9778 - val_loss: 0.0716 - val_accuracy: 0.9795\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0634 - accuracy: 0.9839 - val_loss: 0.0545 - val_accuracy: 0.9848\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0514 - accuracy: 0.9867 - val_loss: 0.0542 - val_accuracy: 0.9838\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0436 - accuracy: 0.9889 - val_loss: 0.0437 - val_accuracy: 0.9871\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0375 - accuracy: 0.9908 - val_loss: 0.0445 - val_accuracy: 0.9865\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0333 - accuracy: 0.9919 - val_loss: 0.0407 - val_accuracy: 0.9885\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0297 - accuracy: 0.9929 - val_loss: 0.0359 - val_accuracy: 0.9891\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0265 - accuracy: 0.9938 - val_loss: 0.0359 - val_accuracy: 0.9889\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0238 - accuracy: 0.9945 - val_loss: 0.0329 - val_accuracy: 0.9901\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0223 - accuracy: 0.9952 - val_loss: 0.0340 - val_accuracy: 0.9898\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0203 - accuracy: 0.9959 - val_loss: 0.0306 - val_accuracy: 0.9911\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0184 - accuracy: 0.9962 - val_loss: 0.0303 - val_accuracy: 0.9908\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0171 - accuracy: 0.9966 - val_loss: 0.0314 - val_accuracy: 0.9901\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0160 - accuracy: 0.9968 - val_loss: 0.0288 - val_accuracy: 0.9909\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0149 - accuracy: 0.9973 - val_loss: 0.0317 - val_accuracy: 0.9907\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0139 - accuracy: 0.9976 - val_loss: 0.0294 - val_accuracy: 0.9913\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0131 - accuracy: 0.9978 - val_loss: 0.0300 - val_accuracy: 0.9909\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0123 - accuracy: 0.9979 - val_loss: 0.0275 - val_accuracy: 0.9917\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0113 - accuracy: 0.9983 - val_loss: 0.0268 - val_accuracy: 0.9919\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x=x_train_std_input,y=y_train, epochs=20, batch_size=128, validation_data=(x_test_std_input, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model1_standnorm_input_batchnorm_hidden.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('model1_standnorm_input_batchnorm_hidden.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization\n",
      "\n",
      "Gamma :          [1.0001033544540405, 1.0001364946365356, 1.00005304813385, 1.0000249147415161, 1.0000289678573608, 1.000061273574829]\n",
      "\n",
      "Beta :          [-3.193265651901811e-09, -3.1261129240789387e-09, 1.0744865441836282e-08, -1.8673049773099137e-09, -2.641025842464728e-09, -6.1343565782578935e-09]\n",
      "\n",
      "####################################################################################################################################################################################################################################\n",
      "\n",
      "batch_normalization_1\n",
      "\n",
      "Gamma :          [1.080140471458435, 1.1022557020187378, 1.0472339391708374, 1.026122808456421, 1.024849772453308, 1.055171012878418]\n",
      "\n",
      "Beta :          [-0.07621415704488754, -0.02155950292944908, 0.22738882899284363, -0.011400452814996243, 0.10398653149604797, -0.0037515745498239994]\n",
      "\n",
      "####################################################################################################################################################################################################################################\n",
      "\n",
      "batch_normalization_2\n",
      "\n",
      "Gamma :          [1.0000356435775757, 1.0000033378601074, 1.0000083446502686, 1.0000169277191162, 1.0000183582305908, 1.0000218152999878, 1.000011682510376, 1.0000137090682983, 1.0000096559524536, 1.0000061988830566, 1.0000522136688232, 1.0000017881393433, 1.0000300407409668, 1.0000131130218506, 1.0000200271606445, 1.0000075101852417]\n",
      "\n",
      "Beta :          [-2.463348858228187e-09, 2.005402288673963e-09, 1.6207137021329032e-10, 1.5322434432363252e-09, -1.3839591694875253e-09, -3.1252422871830277e-09, -3.3732927562368786e-10, -3.91772919661193e-10, 4.1625791702415427e-10, 1.1608524558281985e-10, 1.0016080187469356e-09, -1.153854012336808e-09, 1.8028366577382826e-09, 2.276902222320132e-09, 3.3491312501077175e-10, -4.598384728549121e-10]\n",
      "\n",
      "####################################################################################################################################################################################################################################\n",
      "\n",
      "batch_normalization_3\n",
      "\n",
      "Gamma :          [1.0338406562805176, 1.006487250328064, 1.0142003297805786, 1.017224907875061, 1.0104438066482544, 1.0181764364242554, 1.0087862014770508, 1.0160397291183472, 1.005645751953125, 1.0070059299468994, 1.047123670578003, 0.9986793994903564, 1.0332375764846802, 1.0144470930099487, 1.0219016075134277, 1.0124233961105347]\n",
      "\n",
      "Beta :          [-0.002855873666703701, 0.001166807720437646, -0.004663181956857443, 0.004320188425481319, -0.009904230013489723, -0.011273208074271679, 0.007533228490501642, -0.013439025729894638, -0.005260499194264412, 0.008676453493535519, -0.0031983396038413048, 0.0031105198431760073, 0.012730601243674755, 0.0038798220921307802, -0.008165532723069191, 0.002551551442593336]\n",
      "\n",
      "####################################################################################################################################################################################################################################\n",
      "\n",
      "batch_normalization_4\n",
      "\n",
      "Gamma :          [1.0056623220443726, 1.0002540349960327, 1.0018022060394287, 1.0066707134246826, 1.0092597007751465, 1.0071388483047485, 1.0072309970855713, 0.9957995414733887, 1.0101786851882935, 1.0164235830307007, 1.0066404342651367, 1.0052077770233154, 1.0055514574050903, 1.0036792755126953, 1.0043503046035767, 1.0096917152404785, 1.006450891494751, 1.0025393962860107, 1.0074964761734009, 1.0047550201416016, 1.000993013381958, 0.9988912343978882, 1.0064479112625122, 0.9974077939987183, 1.0029772520065308, 1.0045174360275269, 1.0033620595932007, 1.0078637599945068, 1.0041853189468384, 1.002315878868103, 1.0109084844589233, 1.0090458393096924, 1.0084309577941895, 1.0023661851882935, 1.0066691637039185, 1.0024234056472778, 1.003269076347351, 1.005169153213501, 1.005933165550232, 0.9983053803443909, 1.0032063722610474, 0.9978740215301514, 1.0113455057144165, 1.008239507675171, 1.0073041915893555, 1.002217411994934, 1.002747654914856, 0.9994053840637207, 0.9990352392196655, 1.003086805343628, 0.9998452067375183, 1.0092865228652954, 0.9984353184700012, 1.0023771524429321, 1.0014634132385254, 1.0023584365844727, 1.0083787441253662, 1.013611912727356, 0.9999356269836426, 1.0093258619308472, 1.0075217485427856, 1.0061134099960327, 1.0038620233535767, 1.0156421661376953, 1.0063326358795166, 1.0024514198303223, 1.0096383094787598, 1.0029706954956055, 1.0026702880859375, 1.0199156999588013, 1.0051530599594116, 1.0050839185714722, 1.0054091215133667, 1.0023939609527588, 0.9996649026870728, 1.013514518737793, 1.0103176832199097, 1.0138578414916992, 0.9989141225814819, 1.0042575597763062, 0.9978541135787964, 1.0094982385635376, 0.9994216561317444, 1.0033928155899048, 1.001430630683899, 1.0026663541793823, 1.0066559314727783, 1.0063748359680176, 1.002204418182373, 1.007834792137146, 0.9986542463302612, 1.001767873764038, 1.0049982070922852, 1.0009055137634277, 1.0031565427780151, 1.002622127532959, 1.002833366394043, 1.0056177377700806, 1.0114803314208984, 0.9991413950920105, 0.9986771941184998, 1.005359172821045, 1.0078128576278687, 1.0053293704986572, 1.001425862312317, 1.0197887420654297, 1.0087956190109253, 1.0069087743759155, 1.0082004070281982, 1.008539080619812, 1.0163979530334473, 1.0013022422790527, 0.9979588985443115, 1.0011752843856812, 1.0037761926651, 1.0104435682296753, 0.9994767904281616, 0.9976818561553955, 1.0070868730545044, 1.0001554489135742]\n",
      "\n",
      "Beta :          [0.001124291098676622, 0.0005901519907638431, -0.002198457717895508, 0.00010894873412325978, -0.0019466871162876487, -0.0005352898151613772, -0.0033958384301513433, -0.0010184940183535218, -0.003453400218859315, 0.0015058420831337571, -0.0014921720139682293, 0.001248329528607428, -0.0004696552350651473, 2.4880162527551875e-05, 0.003014389891177416, -0.0017457314534112811, -0.0007751599187031388, 0.0025623925030231476, -0.0015817326493561268, 0.002309000352397561, -0.001256747404113412, 0.0020975738298147917, 0.0020140092819929123, -0.0005003346013836563, 0.0015787516022101045, -0.00043038136209361255, -0.0015560592291876674, 0.0005733513389714062, -0.0025358612183481455, 5.677256194758229e-05, 0.004662738647311926, -0.00040494761196896434, -0.00019614186021499336, -0.0009334749775007367, 0.0007417193264700472, 0.0010360779706388712, -0.0031069170217961073, -0.0001801229373086244, -0.00023045636771712452, 0.0010385994100943208, 0.0006547464872710407, -0.001505096210166812, -0.0013453575083985925, -0.0017636660486459732, -0.0017419439973309636, 0.002555257175117731, 0.0019506033277139068, -0.0007243838626891375, -0.0023608196061104536, 0.0013325171312317252, -0.00029090323369018734, -5.2844512538285926e-05, -0.001144959358498454, 0.001336176646873355, 0.0013341348385438323, 0.0018369388999417424, 0.0001850294938776642, -0.002749239094555378, -0.0012618439504876733, 0.0018150972900912166, -0.0007615818758495152, 0.00012803601566702127, -0.0023665456101298332, 0.00011361933866282925, -0.0007389850215986371, 0.00048394594341516495, -0.0003616343019530177, -0.0005119805573485792, -0.00029270321829244494, -0.00016640231478959322, 0.0020666508935391903, 0.0005867743166163564, -0.0008726856322027743, -0.0005122057627886534, -0.001182940904982388, -0.0011479889508336782, -0.0019359468715265393, 0.0029170888010412455, -0.0020914669148623943, 0.0008942880085669458, -0.0012710365699604154, -0.0011192521778866649, -9.706370474305004e-05, -0.0008925693691708148, -0.001172817312180996, -0.0008061046828515828, 0.0021084684412926435, 0.0013441269984468818, 0.0020949963945895433, 0.0008994786185212433, -0.0008241339819505811, 0.00014087320596445352, 0.0008310443954542279, -2.537005093472544e-05, 0.0010218500392511487, 0.0011768750846385956, -0.0028767904732376337, -0.0002652867406141013, 4.538970097200945e-05, -0.00208503776229918, -0.0010540963849052787, -0.001366359880194068, -0.0014983313158154488, -0.002451713662594557, -0.0005831843591295183, 0.0027133007533848286, -0.0011870344169437885, 0.0027340007945895195, 0.0021771600004285574, -0.0011616094270721078, 0.0002692993148230016, -0.004542726557701826, 0.0011777520412579179, 0.0014645763440057635, 9.493681136518717e-05, -0.002649039961397648, 0.0027524407487362623, -0.0003103430208284408, -0.0007679365808144212, 0.0005505988374352455]\n",
      "\n",
      "####################################################################################################################################################################################################################################\n",
      "\n",
      "batch_normalization_5\n",
      "\n",
      "Gamma :          [1.0615262985229492, 1.039718508720398, 1.0386559963226318, 1.0391103029251099, 1.020777702331543, 1.0288842916488647, 1.0355119705200195, 1.0326181650161743, 1.0356547832489014, 1.032044768333435, 1.0745348930358887, 1.0581867694854736, 1.054201602935791, 1.0620614290237427, 1.0565823316574097, 1.0438120365142822, 1.0563002824783325, 1.03714919090271, 1.0537210702896118, 1.0315293073654175, 1.030641794204712, 1.0392262935638428, 1.037245273590088, 1.0563414096832275, 1.0364030599594116, 1.050891637802124, 1.062117099761963, 1.0535331964492798, 1.0555459260940552, 1.0582879781723022, 1.0416603088378906, 1.0251702070236206, 1.0372323989868164, 1.0637003183364868, 1.0374497175216675, 1.0296249389648438, 1.0574452877044678, 1.0502383708953857, 1.0667757987976074, 1.0457359552383423, 1.057141900062561, 1.0459693670272827, 1.0562243461608887, 1.0574266910552979, 1.0156569480895996, 1.0514897108078003, 1.0520153045654297, 1.0462865829467773, 1.0244661569595337, 1.0396543741226196, 1.049127459526062, 1.0304591655731201, 1.0449737310409546, 1.034093976020813, 1.0514894723892212, 1.0646445751190186, 1.031477451324463, 1.0358623266220093, 1.0460479259490967, 1.0569781064987183, 1.0628811120986938, 1.0610231161117554, 1.0231566429138184, 1.0534456968307495, 1.0600019693374634, 1.0535024404525757, 1.043460488319397, 1.0203520059585571, 1.0438870191574097, 1.0487699508666992, 1.0211361646652222, 1.0495728254318237, 1.0329267978668213, 1.0658795833587646, 1.0367887020111084, 1.065355896949768, 1.0290696620941162, 1.028918743133545, 1.0448110103607178, 1.044732689857483, 1.0588737726211548, 1.0495487451553345, 1.041137933731079, 1.0506408214569092]\n",
      "\n",
      "Beta :          [-0.0006841294816695154, 7.09985542926006e-05, -0.007357184309512377, 0.009201510809361935, 0.0005656683933921158, -0.00413913931697607, 0.0047630551271140575, 0.007424724753946066, -0.002051409101113677, 0.0039805080741643906, -0.0006112591945566237, -0.005290526431053877, -0.0018889709608629346, -0.004128921311348677, -0.003923475276678801, -0.00962114054709673, -8.10254059615545e-05, 0.0021121702156960964, 0.003959633409976959, -0.0020060292445123196, 0.010553175583481789, 0.009844976477324963, -0.007894470356404781, -0.00780536700040102, 0.007520067505538464, 0.01277607399970293, -0.005623600445687771, -0.01407290156930685, -0.0021203721407800913, -0.013125053606927395, -0.008550766855478287, -0.006940011400729418, 0.003943008370697498, 0.0024210987612605095, -0.0057405284605920315, 0.0026334745343774557, 0.00318886898458004, -0.0005172445089556277, 0.005010656546801329, -0.0028656867798417807, 0.004122504033148289, -0.0031393086537718773, 0.0007978860521689057, -0.00093594950158149, -0.0023288445081561804, -0.007952450774610043, -0.0017311711562797427, 0.0003689189616125077, 0.004880804568529129, 0.0024659589398652315, 0.0005395978223532438, -0.0014985940651968122, -0.011738463304936886, 0.010742729529738426, -0.0016814022092148662, 0.0020558559335768223, 0.006316308863461018, -0.005518085788935423, 0.008858502842485905, 0.0036259950138628483, -0.0023634417448192835, 0.0016314126551151276, 0.007857171818614006, 0.0055394042283296585, 0.009218170307576656, -0.010433212853968143, 0.004861378576606512, 0.003201061626896262, 0.006094268523156643, -0.006958217825740576, -0.002350530354306102, 0.007631663233041763, 0.0004118015931453556, -0.00026142041315324605, -0.0048215556889772415, 0.011411644518375397, 0.00257382751442492, -0.00251532974652946, -0.013403575867414474, 0.007119462359696627, 0.0055313087068498135, -0.010606273077428341, -0.0036545847542583942, -0.004875462036579847]\n",
      "\n",
      "####################################################################################################################################################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [1,3,5,7,9,12]:\n",
    "    print(model.layers[i].name)\n",
    "    print()\n",
    "    print('Gamma :          '+ str(model.layers[i].get_weights()[0].tolist()))\n",
    "    print()\n",
    "    print('Beta :          '+ str(model.layers[i].get_weights()[1].tolist()))\n",
    "    print ()\n",
    "    print ('####################################################################################################################################################################################################################################')\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Data without standard normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset as train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Set numeric type to float32 from uint8\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize value to [0, 1]\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Transform lables to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Reshape the dataset into 4D array\n",
    "x_train = x_train.reshape(x_train.shape[0], 28,28,1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28,28,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Batch normalization for input and hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(17)\n",
    "#Instantiate an empty model\n",
    "model_batch = Sequential()\n",
    "# C1 Convolutional Layer\n",
    "model_batch.add(BatchNormalization(input_shape=(28,28,1)))\n",
    "model_batch.add(layers.Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=(28,28,1), padding='same'))\n",
    "model_batch.add(BatchNormalization())\n",
    "# S2 Pooling Layer\n",
    "model_batch.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n",
    "model_batch.add(BatchNormalization())\n",
    "# C3 Convolutional Layer\n",
    "model_batch.add(layers.Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
    "model_batch.add(BatchNormalization())\n",
    "# S4 Pooling Layer\n",
    "model_batch.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "model_batch.add(BatchNormalization())\n",
    "# C5 Fully Connected Convolutional Layer\n",
    "model_batch.add(layers.Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
    "model_batch.add(BatchNormalization())\n",
    "#Flatten the CNN output so that we can connect it with fully connected layers\n",
    "model_batch.add(layers.Flatten())\n",
    "# FC6 Fully Connected Layer\n",
    "model_batch.add(layers.Dense(84, activation='tanh'))\n",
    "model_batch.add(BatchNormalization())\n",
    "#Output Layer with softmax activation\n",
    "model_batch.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_batch.compile(loss=keras.losses.categorical_crossentropy, optimizer='SGD', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_6 (Batch (None, 28, 28, 1)         4         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 28, 28, 6)         24        \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 27, 27, 6)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 27, 27, 6)         24        \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 23, 23, 16)        2416      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 23, 23, 16)        64        \n",
      "_________________________________________________________________\n",
      "average_pooling2d_3 (Average (None, 11, 11, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 11, 11, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 120)         48120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 7, 7, 120)         480       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5880)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                494004    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 84)                336       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 546,542\n",
      "Trainable params: 546,044\n",
      "Non-trainable params: 498\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_batch.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 0.2065 - accuracy: 0.9428 - val_loss: 0.0993 - val_accuracy: 0.9731\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0848 - accuracy: 0.9782 - val_loss: 0.0704 - val_accuracy: 0.9804\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0625 - accuracy: 0.9840 - val_loss: 0.0541 - val_accuracy: 0.9849\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0507 - accuracy: 0.9867 - val_loss: 0.0535 - val_accuracy: 0.9850\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0431 - accuracy: 0.9890 - val_loss: 0.0434 - val_accuracy: 0.9871\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0370 - accuracy: 0.9909 - val_loss: 0.0435 - val_accuracy: 0.9868\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0329 - accuracy: 0.9920 - val_loss: 0.0402 - val_accuracy: 0.9886\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0294 - accuracy: 0.9929 - val_loss: 0.0360 - val_accuracy: 0.9887\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0262 - accuracy: 0.9941 - val_loss: 0.0357 - val_accuracy: 0.9888\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0235 - accuracy: 0.9947 - val_loss: 0.0328 - val_accuracy: 0.9901\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0220 - accuracy: 0.9952 - val_loss: 0.0341 - val_accuracy: 0.9895\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0201 - accuracy: 0.9958 - val_loss: 0.0307 - val_accuracy: 0.9912\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0182 - accuracy: 0.9963 - val_loss: 0.0301 - val_accuracy: 0.9910\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0169 - accuracy: 0.9967 - val_loss: 0.0313 - val_accuracy: 0.9901\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0158 - accuracy: 0.9970 - val_loss: 0.0285 - val_accuracy: 0.9915\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0148 - accuracy: 0.9972 - val_loss: 0.0317 - val_accuracy: 0.9901\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0137 - accuracy: 0.9976 - val_loss: 0.0291 - val_accuracy: 0.9913\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0130 - accuracy: 0.9978 - val_loss: 0.0300 - val_accuracy: 0.9911\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0121 - accuracy: 0.9979 - val_loss: 0.0272 - val_accuracy: 0.9916\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0112 - accuracy: 0.9983 - val_loss: 0.0266 - val_accuracy: 0.9920\n"
     ]
    }
   ],
   "source": [
    "hist_batch = model_batch.fit(x=x_train,y=y_train, epochs=20, batch_size=128, validation_data=(x_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batch.save('model2_batchnorm_all.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batch = keras.models.load_model('model2_batchnorm_all.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [0,2,4,6,8,10,13]:\n",
    "    data_to_plot = [model_batch.layers[i].get_weights()[0].tolist(), model_batch.layers[i].get_weights()[1].tolist()]\n",
    "\n",
    "    # Create a figure instance\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # Create an axes instance\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "    # Create the boxplot\n",
    "    bp = ax.violinplot(data_to_plot)\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    plt.xlabel(\"Gamma / Beta\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_6\n",
      "\n",
      "Gamma :          [1.1744343042373657]\n",
      "\n",
      "Beta :          [0.22457078099250793]\n",
      "\n",
      "####################################################################################################################################################################################################################################\n",
      "\n",
      "batch_normalization_7\n",
      "\n",
      "Gamma :          [1.0000983476638794, 1.0001249313354492, 1.0000451803207397, 1.000023365020752, 1.0000228881835938, 1.0000643730163574]\n",
      "\n",
      "Beta :          [-7.3766730501745315e-09, -4.5491428402044676e-09, 1.0702621427993719e-10, 4.27229318589184e-09, -4.696779853929911e-09, -1.1279193135038668e-09]\n",
      "\n",
      "####################################################################################################################################################################################################################################\n",
      "\n",
      "batch_normalization_8\n",
      "\n",
      "Gamma :          [1.0776439905166626, 1.0955561399459839, 1.0428701639175415, 1.0234375, 1.0232908725738525, 1.0513699054718018]\n",
      "\n",
      "Beta :          [-0.07066694647073746, -0.01616567187011242, 0.20733587443828583, -0.008760466240346432, 0.0963069275021553, -0.0007116174092516303]\n",
      "\n",
      "####################################################################################################################################################################################################################################\n",
      "\n",
      "batch_normalization_9\n",
      "\n",
      "Gamma :          [1.0000356435775757, 1.0000054836273193, 1.0000078678131104, 1.0000122785568237, 1.0000206232070923, 1.0000176429748535, 1.0000079870224, 1.0000152587890625, 1.0000146627426147, 1.000005841255188, 1.000051498413086, 1.000001072883606, 1.0000293254852295, 1.0000181198120117, 1.0000125169754028, 1.0000038146972656]\n",
      "\n",
      "Beta :          [-5.601688002343508e-10, -3.754273281142417e-11, 7.454938333317784e-10, 1.3708636448228617e-09, -3.3621125883342984e-09, -2.1263983907005013e-09, -5.515064793737423e-11, 8.792996886164417e-10, 1.7904006055502464e-09, -2.0890402741002845e-09, -1.368022362058241e-09, 2.1246681497499864e-11, 1.4884056209751861e-09, 7.725603490271737e-10, 3.6389211643950148e-09, 1.5337006109561457e-09]\n",
      "\n",
      "####################################################################################################################################################################################################################################\n",
      "\n",
      "batch_normalization_10\n",
      "\n",
      "Gamma :          [1.0341215133666992, 1.0039653778076172, 1.0128912925720215, 1.0154327154159546, 1.013261079788208, 1.0165857076644897, 1.0095893144607544, 1.0178035497665405, 1.007718801498413, 1.0067623853683472, 1.047182559967041, 1.0002268552780151, 1.0327712297439575, 1.0152435302734375, 1.020142674446106, 1.010878086090088]\n",
      "\n",
      "Beta :          [-0.0045372662134468555, 0.0005203681066632271, -0.003558523254469037, 0.004170415457338095, -0.009523607790470123, -0.01109848078340292, 0.006995310075581074, -0.01202782429754734, -0.005124170798808336, 0.008881361223757267, -0.0030007953755557537, 0.002937089651823044, 0.012201976031064987, 0.0034679649397730827, -0.006961768958717585, 0.0014267554506659508]\n",
      "\n",
      "####################################################################################################################################################################################################################################\n",
      "\n",
      "batch_normalization_11\n",
      "\n",
      "Gamma :          [1.0052587985992432, 1.000139832496643, 1.001922845840454, 1.006265640258789, 1.008581280708313, 1.0073052644729614, 1.0076013803482056, 0.9958900213241577, 1.0108489990234375, 1.0162434577941895, 1.0070840120315552, 1.005264163017273, 1.0054093599319458, 1.003129005432129, 1.004520058631897, 1.009005069732666, 1.0070281028747559, 1.0021543502807617, 1.007168173789978, 1.0047032833099365, 1.0007719993591309, 0.9994423389434814, 1.0065604448318481, 0.9974638819694519, 1.0029118061065674, 1.0043927431106567, 1.0034414529800415, 1.0074909925460815, 1.0040936470031738, 1.0027133226394653, 1.0112006664276123, 1.0089330673217773, 1.00880765914917, 1.0024033784866333, 1.0062967538833618, 1.0024409294128418, 1.003140926361084, 1.0053129196166992, 1.0064294338226318, 0.9981534481048584, 1.0038822889328003, 0.9976579546928406, 1.0109071731567383, 1.0082029104232788, 1.0073271989822388, 1.0019510984420776, 1.0029511451721191, 0.9991244673728943, 0.9989830255508423, 1.003076195716858, 1.0000693798065186, 1.0088599920272827, 0.9984415769577026, 1.0024769306182861, 1.0010745525360107, 1.001888632774353, 1.0082155466079712, 1.013014793395996, 0.9997944831848145, 1.0093351602554321, 1.007354736328125, 1.0060356855392456, 1.0035005807876587, 1.0149379968643188, 1.006460428237915, 1.002143383026123, 1.0090845823287964, 1.0028389692306519, 1.0026636123657227, 1.0194510221481323, 1.0047345161437988, 1.0056579113006592, 1.005800485610962, 1.0022318363189697, 0.9996469020843506, 1.0135589838027954, 1.0101220607757568, 1.0128202438354492, 0.9995080232620239, 1.0044492483139038, 0.9978495836257935, 1.0090162754058838, 0.9997790455818176, 1.0031810998916626, 1.0014065504074097, 1.0024688243865967, 1.0062905550003052, 1.0067492723464966, 1.0024688243865967, 1.0076700448989868, 0.9985764026641846, 1.0017824172973633, 1.0050314664840698, 1.000797986984253, 1.0034449100494385, 1.0026159286499023, 1.0024282932281494, 1.0055179595947266, 1.011406660079956, 0.9994918704032898, 0.9990472197532654, 1.0051100254058838, 1.0086188316345215, 1.0054848194122314, 1.0013679265975952, 1.0197211503982544, 1.0090214014053345, 1.0065478086471558, 1.0085139274597168, 1.0084258317947388, 1.0153049230575562, 1.0012198686599731, 0.9983121156692505, 1.0010859966278076, 1.0039886236190796, 1.0109390020370483, 0.9997484087944031, 0.9975271821022034, 1.007455825805664, 1.0003316402435303]\n",
      "\n",
      "Beta :          [0.0010533033637329936, 0.0007036763709038496, -0.002085362793877721, 9.090618550544605e-05, -0.001968103228136897, -0.0004403733473736793, -0.003414415754377842, -0.0010265377350151539, -0.0035364669747650623, 0.0014711128314957023, -0.0014315377920866013, 0.0013256424572318792, -0.00043726712465286255, 1.2391078030304925e-07, 0.003103038063272834, -0.0017474110936746001, -0.0007057064212858677, 0.0026617127005010843, -0.0015560751780867577, 0.002399908611550927, -0.0011982218129560351, 0.002085247077047825, 0.0018445991445332766, -0.0006213307497091591, 0.0015760164242237806, -0.0005756649188697338, -0.001621804665774107, 0.0006325808935798705, -0.002656911965459585, 0.00015462891315110028, 0.004566132090985775, -0.00044837422319687903, -0.0002062796411337331, -0.0009527691872790456, 0.0006716742063872516, 0.0009920753072947264, -0.003129907650873065, -0.00023055952624417841, -0.0002612094976939261, 0.0010294950334355235, 0.0006142333149909973, -0.0016740458086133003, -0.0014964313013479114, -0.0017876705387607217, -0.0016702774446457624, 0.002510586753487587, 0.0019314270466566086, -0.0007780386949889362, -0.0024118837900459766, 0.0012879582354798913, -0.00036515932879410684, 0.0001681772992014885, -0.0010840485338121653, 0.0012569499667733908, 0.001248324173502624, 0.0018012933433055878, 0.0001351249375147745, -0.0028093259315937757, -0.0012254540342837572, 0.0018925450276583433, -0.0006966259679757059, 0.00011096659727627411, -0.002334713004529476, 6.523869524244219e-05, -0.0006349883042275906, 0.0005626873462460935, -0.0002250545658171177, -0.00048457770026288927, -0.0003365697921253741, -0.00018297780479770154, 0.0019598889630287886, 0.000573505531065166, -0.0007671168423257768, -0.00064840231789276, -0.0011303109349682927, -0.0010996715864166617, -0.0020463543478399515, 0.0028635445050895214, -0.0020908762235194445, 0.0008373147575184703, -0.0012861283030360937, -0.0011515046935528517, -0.0002559356507845223, -0.0008941934211179614, -0.001217835582792759, -0.0006358709651976824, 0.002021131105720997, 0.001275059417821467, 0.002213583793491125, 0.0009163131471723318, -0.0007526439148932695, 2.388357461313717e-05, 0.0007840882171876729, -0.0001455596648156643, 0.001123476424254477, 0.0012963585322722793, -0.0029234394896775484, -0.00032991435728035867, 9.385023440700024e-05, -0.0021514880936592817, -0.001039752154611051, -0.0013921601930633187, -0.001326416851952672, -0.0024970988743007183, -0.0004410594410728663, 0.0027187583036720753, -0.0012052664533257484, 0.0027848673053085804, 0.002017634455114603, -0.0011310462141409516, 0.0004675565578509122, -0.004531759303063154, 0.0012351487530395389, 0.0014310673577710986, -4.221291237627156e-05, -0.0027097766287624836, 0.0027845948934555054, -0.00028952330467291176, -0.0007827087538316846, 0.0005921937408857048]\n",
      "\n",
      "####################################################################################################################################################################################################################################\n",
      "\n",
      "batch_normalization_12\n",
      "\n",
      "Gamma :          [1.0612136125564575, 1.0398560762405396, 1.0395538806915283, 1.0386760234832764, 1.020702838897705, 1.028437614440918, 1.0354441404342651, 1.0325572490692139, 1.0350160598754883, 1.0318264961242676, 1.0736597776412964, 1.0579160451889038, 1.0536558628082275, 1.0625438690185547, 1.0552042722702026, 1.043561339378357, 1.0550681352615356, 1.0371156930923462, 1.0531249046325684, 1.0316314697265625, 1.0305050611495972, 1.0384547710418701, 1.036906361579895, 1.0566296577453613, 1.0364665985107422, 1.0501279830932617, 1.061496376991272, 1.0529338121414185, 1.055050015449524, 1.057979702949524, 1.0413564443588257, 1.0257248878479004, 1.0366038084030151, 1.063535213470459, 1.0369486808776855, 1.0294815301895142, 1.0567388534545898, 1.0504050254821777, 1.066788673400879, 1.0452245473861694, 1.0561500787734985, 1.046491265296936, 1.0567829608917236, 1.0565285682678223, 1.0154483318328857, 1.0512311458587646, 1.05126953125, 1.0461827516555786, 1.0238131284713745, 1.0396432876586914, 1.04896879196167, 1.0306458473205566, 1.0446697473526, 1.03395676612854, 1.0511891841888428, 1.0643560886383057, 1.031791090965271, 1.0357686281204224, 1.0453524589538574, 1.0564340353012085, 1.0625823736190796, 1.060795545578003, 1.024033784866333, 1.052892804145813, 1.0594714879989624, 1.0530486106872559, 1.0443867444992065, 1.0202672481536865, 1.0437109470367432, 1.0490092039108276, 1.0210005044937134, 1.0490741729736328, 1.0338023900985718, 1.065394639968872, 1.0365632772445679, 1.0652766227722168, 1.0288386344909668, 1.0286762714385986, 1.0447653532028198, 1.0443572998046875, 1.0587347745895386, 1.0487102270126343, 1.0407977104187012, 1.0509847402572632]\n",
      "\n",
      "Beta :          [-0.0008692556293681264, 0.0004808119556400925, -0.0072137764655053616, 0.009035460650920868, 0.0006586991949006915, -0.004097374621778727, 0.0046732197515666485, 0.007280650082975626, -0.0019321744330227375, 0.003851494286209345, 2.85836558759911e-05, -0.00538491690531373, -0.0017548587638884783, -0.004126016516238451, -0.00391753762960434, -0.009419938549399376, 0.0001730532676447183, 0.0021508303470909595, 0.003596246475353837, -0.002177702495828271, 0.010159330442547798, 0.009885822422802448, -0.007434303406625986, -0.007985561154782772, 0.0072942329570651054, 0.012714598327875137, -0.005438168533146381, -0.013720403425395489, -0.0019007171504199505, -0.012599308975040913, -0.008123181760311127, -0.006509695202112198, 0.003702538087964058, 0.0021564681082963943, -0.005515757016837597, 0.0023110629990696907, 0.0031397666316479445, -0.000557487946934998, 0.00464083906263113, -0.002909203292801976, 0.0039813644252717495, -0.003198189428076148, 0.0007314523099921644, -0.0008894737111404538, -0.0023849380668252707, -0.007846469059586525, -0.00178744294680655, 0.000701487879268825, 0.0046749962493777275, 0.0022615070920437574, 0.00046671839663758874, -0.0012962330365553498, -0.011212168261408806, 0.010269800201058388, -0.0014958319952711463, 0.002203281968832016, 0.00637700455263257, -0.005733649246394634, 0.008801814168691635, 0.003929481841623783, -0.002303466899320483, 0.0012597948079928756, 0.007495399098843336, 0.0052605909295380116, 0.0088513707742095, -0.009923121891915798, 0.004494238644838333, 0.003209603950381279, 0.005816757213324308, -0.0069657862186431885, -0.0021940283477306366, 0.007474885787814856, 0.00040936964796856046, -0.0004335050762165338, -0.004530641250312328, 0.011161871254444122, 0.002604279201477766, -0.00278839492239058, -0.012905885465443134, 0.007219281978905201, 0.005192923359572887, -0.010496263392269611, -0.0037754857912659645, -0.004482236225157976]\n",
      "\n",
      "####################################################################################################################################################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [0,2,4,6,8,10,13]:\n",
    "    print(model_batch.layers[i].name)\n",
    "    print()\n",
    "    print('Gamma :          '+ str(model_batch.layers[i].get_weights()[0].tolist()))\n",
    "    print()\n",
    "    print('Beta :          '+ str(model_batch.layers[i].get_weights()[1].tolist()))\n",
    "    print ()\n",
    "    print ('####################################################################################################################################################################################################################################')\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The performances have been similar ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Data without standard normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset as train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Set numeric type to float32 from uint8\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize value to [0, 1]\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Transform lables to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Reshape the dataset into 4D array\n",
    "x_train = x_train.reshape(x_train.shape[0], 28,28,1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28,28,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Dropout and no batch normalization\n",
    "(dropout not applied on pooling layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Instantiate an empty model\n",
    "model_dropout = Sequential()\n",
    "# C1 Convolutional Layer\n",
    "model_dropout.add(Dropout(0.2, input_shape=(28,28,1)))\n",
    "model_dropout.add(layers.Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=(28,28,1), padding='same'))\n",
    "# S2 Pooling Layer\n",
    "model_dropout.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n",
    "\n",
    "# C3 Convolutional Layer\n",
    "model_dropout.add(Dropout(0.5))\n",
    "model_dropout.add(layers.Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
    "\n",
    "# S4 Pooling Layer\n",
    "model_dropout.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "\n",
    "# C5 Fully Connected Convolutional Layer\n",
    "model_dropout.add(Dropout(0.5))\n",
    "model_dropout.add(layers.Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
    "\n",
    "#Flatten the CNN output so that we can connect it with fully connected layers\n",
    "model_dropout.add(layers.Flatten())\n",
    "\n",
    "# FC6 Fully Connected Layer\n",
    "model_dropout.add(Dropout(0.5))\n",
    "model_dropout.add(layers.Dense(84, activation='tanh'))\n",
    "\n",
    "#Output Layer with softmax activation\n",
    "model_dropout.add(Dropout(0.5))\n",
    "model_dropout.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_dropout.compile(loss=keras.losses.categorical_crossentropy, optimizer='SGD', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_5 (Dropout)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_6 (Average (None, 27, 27, 6)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 27, 27, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 23, 23, 16)        2416      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_7 (Average (None, 11, 11, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 11, 11, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 7, 7, 120)         48120     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 5880)              0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 5880)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 84)                494004    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 545,546\n",
      "Trainable params: 545,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_dropout.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 1.0002 - accuracy: 0.6741 - val_loss: 0.3821 - val_accuracy: 0.8858\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.5666 - accuracy: 0.8236 - val_loss: 0.3058 - val_accuracy: 0.9061\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.5069 - accuracy: 0.8433 - val_loss: 0.2700 - val_accuracy: 0.9205\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.4630 - accuracy: 0.8571 - val_loss: 0.2478 - val_accuracy: 0.9299\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.4338 - accuracy: 0.8677 - val_loss: 0.2283 - val_accuracy: 0.9333\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 5s 12ms/step - loss: 0.4050 - accuracy: 0.8755 - val_loss: 0.2157 - val_accuracy: 0.9369\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.3858 - accuracy: 0.8819 - val_loss: 0.2026 - val_accuracy: 0.9407\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.3731 - accuracy: 0.8870 - val_loss: 0.1925 - val_accuracy: 0.9429\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.3556 - accuracy: 0.8914 - val_loss: 0.1830 - val_accuracy: 0.9453\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.3442 - accuracy: 0.8954 - val_loss: 0.1723 - val_accuracy: 0.9483\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.3344 - accuracy: 0.8979 - val_loss: 0.1676 - val_accuracy: 0.9492\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.3213 - accuracy: 0.9029 - val_loss: 0.1581 - val_accuracy: 0.9507\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.3091 - accuracy: 0.9066 - val_loss: 0.1519 - val_accuracy: 0.9543\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.3021 - accuracy: 0.9086 - val_loss: 0.1465 - val_accuracy: 0.9555\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.2942 - accuracy: 0.9119 - val_loss: 0.1423 - val_accuracy: 0.9554\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.2850 - accuracy: 0.9159 - val_loss: 0.1342 - val_accuracy: 0.9581\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.2776 - accuracy: 0.9159 - val_loss: 0.1291 - val_accuracy: 0.9603\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.2658 - accuracy: 0.9186 - val_loss: 0.1247 - val_accuracy: 0.9611\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.2603 - accuracy: 0.9216 - val_loss: 0.1224 - val_accuracy: 0.9617\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.2518 - accuracy: 0.9244 - val_loss: 0.1151 - val_accuracy: 0.9641\n"
     ]
    }
   ],
   "source": [
    "hist_dropout = model_dropout.fit(x=x_train,y=y_train, epochs=20, batch_size=128, validation_data=(x_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout.save('model3_dropout_all.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy has significantly dropped ! (0.9641 for dropout only vs 0.9920 for batch norm only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Data without standard normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset as train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Set numeric type to float32 from uint8\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize value to [0, 1]\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# Transform lables to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Reshape the dataset into 4D array\n",
    "x_train = x_train.reshape(x_train.shape[0], 28,28,1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28,28,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4: Dropout and batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "#Instantiate an empty model\n",
    "model_dropout_batch = Sequential()\n",
    "# C1 Convolutional Layer\n",
    "model_dropout_batch.add(BatchNormalization(input_shape=(28,28,1)))\n",
    "model_dropout_batch.add(Dropout(0.2, input_shape=(28,28,1)))\n",
    "model_dropout_batch.add(layers.Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=(28,28,1), padding='same'))\n",
    "model_dropout_batch.add(BatchNormalization())\n",
    "# S2 Pooling Layer\n",
    "model_dropout_batch.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid'))\n",
    "model_dropout_batch.add(BatchNormalization())\n",
    "# C3 Convolutional Layer\n",
    "model_dropout_batch.add(Dropout(0.5))\n",
    "model_dropout_batch.add(layers.Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
    "model_dropout_batch.add(BatchNormalization())\n",
    "# S4 Pooling Layer\n",
    "model_dropout_batch.add(layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "model_dropout_batch.add(BatchNormalization())\n",
    "# C5 Fully Connected Convolutional Layer\n",
    "model_dropout_batch.add(Dropout(0.5))\n",
    "model_dropout_batch.add(layers.Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
    "model_dropout_batch.add(BatchNormalization())\n",
    "#Flatten the CNN output so that we can connect it with fully connected layers\n",
    "model_dropout_batch.add(layers.Flatten())\n",
    "# FC6 Fully Connected Layer\n",
    "model_dropout_batch.add(Dropout(0.5))\n",
    "model_dropout_batch.add(layers.Dense(84, activation='tanh'))\n",
    "model_dropout_batch.add(BatchNormalization())\n",
    "#Output Layer with softmax activation\n",
    "model_dropout_batch.add(Dropout(0.5))\n",
    "model_dropout_batch.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_dropout_batch.compile(loss=keras.losses.categorical_crossentropy, optimizer='SGD', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_13 (Batc (None, 28, 28, 1)         4         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 28, 28, 6)         24        \n",
      "_________________________________________________________________\n",
      "average_pooling2d_8 (Average (None, 27, 27, 6)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 27, 27, 6)         24        \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 27, 27, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 23, 23, 16)        2416      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 23, 23, 16)        64        \n",
      "_________________________________________________________________\n",
      "average_pooling2d_9 (Average (None, 11, 11, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 11, 11, 16)        64        \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 11, 11, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 7, 7, 120)         48120     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 7, 7, 120)         480       \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 5880)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 5880)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 84)                494004    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 84)                336       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 546,542\n",
      "Trainable params: 546,044\n",
      "Non-trainable params: 498\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_dropout_batch.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1473 - accuracy: 0.9561 - val_loss: 0.0587 - val_accuracy: 0.9812\n",
      "Epoch 2/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1435 - accuracy: 0.9575 - val_loss: 0.0568 - val_accuracy: 0.9817\n",
      "Epoch 3/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1419 - accuracy: 0.9586 - val_loss: 0.0554 - val_accuracy: 0.9827\n",
      "Epoch 4/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1397 - accuracy: 0.9583 - val_loss: 0.0540 - val_accuracy: 0.9824\n",
      "Epoch 5/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1377 - accuracy: 0.9592 - val_loss: 0.0525 - val_accuracy: 0.9831\n",
      "Epoch 6/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1375 - accuracy: 0.9588 - val_loss: 0.0529 - val_accuracy: 0.9823\n",
      "Epoch 7/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1359 - accuracy: 0.9596 - val_loss: 0.0527 - val_accuracy: 0.9832\n",
      "Epoch 8/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1355 - accuracy: 0.9597 - val_loss: 0.0500 - val_accuracy: 0.9835\n",
      "Epoch 9/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1310 - accuracy: 0.9604 - val_loss: 0.0508 - val_accuracy: 0.9846\n",
      "Epoch 10/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1266 - accuracy: 0.9628 - val_loss: 0.0523 - val_accuracy: 0.9833\n",
      "Epoch 11/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1258 - accuracy: 0.9632 - val_loss: 0.0501 - val_accuracy: 0.9849\n",
      "Epoch 12/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1237 - accuracy: 0.9632 - val_loss: 0.0474 - val_accuracy: 0.9845\n",
      "Epoch 13/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1240 - accuracy: 0.9635 - val_loss: 0.0490 - val_accuracy: 0.9846\n",
      "Epoch 14/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1250 - accuracy: 0.9624 - val_loss: 0.0481 - val_accuracy: 0.9847\n",
      "Epoch 15/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1224 - accuracy: 0.9633 - val_loss: 0.0448 - val_accuracy: 0.9852\n",
      "Epoch 16/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1189 - accuracy: 0.9650 - val_loss: 0.0463 - val_accuracy: 0.9848\n",
      "Epoch 17/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1179 - accuracy: 0.9652 - val_loss: 0.0449 - val_accuracy: 0.9856\n",
      "Epoch 18/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1189 - accuracy: 0.9639 - val_loss: 0.0453 - val_accuracy: 0.9847\n",
      "Epoch 19/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1161 - accuracy: 0.9659 - val_loss: 0.0451 - val_accuracy: 0.9850\n",
      "Epoch 20/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1152 - accuracy: 0.9649 - val_loss: 0.0462 - val_accuracy: 0.9853\n",
      "Epoch 21/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1150 - accuracy: 0.9657 - val_loss: 0.0436 - val_accuracy: 0.9853\n",
      "Epoch 22/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1124 - accuracy: 0.9663 - val_loss: 0.0433 - val_accuracy: 0.9858\n",
      "Epoch 23/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1141 - accuracy: 0.9657 - val_loss: 0.0432 - val_accuracy: 0.9855\n",
      "Epoch 24/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1119 - accuracy: 0.9663 - val_loss: 0.0440 - val_accuracy: 0.9861\n",
      "Epoch 25/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1092 - accuracy: 0.9671 - val_loss: 0.0417 - val_accuracy: 0.9860\n",
      "Epoch 26/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1104 - accuracy: 0.9671 - val_loss: 0.0423 - val_accuracy: 0.9862\n",
      "Epoch 27/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1100 - accuracy: 0.9670 - val_loss: 0.0423 - val_accuracy: 0.9863\n",
      "Epoch 28/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1053 - accuracy: 0.9680 - val_loss: 0.0404 - val_accuracy: 0.9871\n",
      "Epoch 29/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1067 - accuracy: 0.9680 - val_loss: 0.0423 - val_accuracy: 0.9863\n",
      "Epoch 30/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1054 - accuracy: 0.9684 - val_loss: 0.0399 - val_accuracy: 0.9868\n",
      "Epoch 31/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1068 - accuracy: 0.9680 - val_loss: 0.0404 - val_accuracy: 0.9868\n",
      "Epoch 32/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1061 - accuracy: 0.9677 - val_loss: 0.0395 - val_accuracy: 0.9868\n",
      "Epoch 33/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1042 - accuracy: 0.9682 - val_loss: 0.0401 - val_accuracy: 0.9864\n",
      "Epoch 34/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1060 - accuracy: 0.9687 - val_loss: 0.0390 - val_accuracy: 0.9869\n",
      "Epoch 35/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1020 - accuracy: 0.9691 - val_loss: 0.0397 - val_accuracy: 0.9870\n",
      "Epoch 36/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1038 - accuracy: 0.9687 - val_loss: 0.0397 - val_accuracy: 0.9873\n",
      "Epoch 37/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1030 - accuracy: 0.9693 - val_loss: 0.0394 - val_accuracy: 0.9865\n",
      "Epoch 38/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1007 - accuracy: 0.9699 - val_loss: 0.0408 - val_accuracy: 0.9870\n",
      "Epoch 39/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1017 - accuracy: 0.9699 - val_loss: 0.0375 - val_accuracy: 0.9880\n",
      "Epoch 40/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1001 - accuracy: 0.9703 - val_loss: 0.0374 - val_accuracy: 0.9883\n",
      "Epoch 41/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0987 - accuracy: 0.9704 - val_loss: 0.0361 - val_accuracy: 0.9876\n",
      "Epoch 42/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0978 - accuracy: 0.9704 - val_loss: 0.0370 - val_accuracy: 0.9873\n",
      "Epoch 43/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0973 - accuracy: 0.9709 - val_loss: 0.0376 - val_accuracy: 0.9877\n",
      "Epoch 44/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0960 - accuracy: 0.9711 - val_loss: 0.0380 - val_accuracy: 0.9873\n",
      "Epoch 45/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0954 - accuracy: 0.9711 - val_loss: 0.0372 - val_accuracy: 0.9877\n",
      "Epoch 46/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0984 - accuracy: 0.9712 - val_loss: 0.0363 - val_accuracy: 0.9880\n",
      "Epoch 47/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0946 - accuracy: 0.9710 - val_loss: 0.0361 - val_accuracy: 0.9880\n",
      "Epoch 48/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0958 - accuracy: 0.9711 - val_loss: 0.0368 - val_accuracy: 0.9881\n",
      "Epoch 49/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0922 - accuracy: 0.9719 - val_loss: 0.0371 - val_accuracy: 0.9880\n",
      "Epoch 50/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0929 - accuracy: 0.9717 - val_loss: 0.0367 - val_accuracy: 0.9883\n"
     ]
    }
   ],
   "source": [
    "hist_dropout_batch = model_dropout_batch.fit(x=x_train,y=y_train, epochs=50, batch_size=128, validation_data=(x_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout_batch.save('model4_dropout_batch_all.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The performance compared to dropout only has significantly increased ( 0.9850 for dropout. + batch. norm vs 0.9641 for dropout only) . (compared on 20 epochs)\n",
    "### The performance compared to batch norm only has been slightly reduced( 0.9850 vs 0.9920) . (compared on 20 epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion :\n",
    "\n",
    "### Dropout seems to increase the training time but also avoid overfitting . Dropout is mostly a technique for regularization. It introduces noise into a neural network to force the neural network to learn to generalize well enough to deal with noise. \n",
    "\n",
    "### Batch normalization is mostly a technique for improving optimization and it happens that it also. introduce some noise and therefore help for regularization. As for large dataset like ours, optimization is more important than regularizationso batch normalization seems to be more important. A combination of both can also be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
