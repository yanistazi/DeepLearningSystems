{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cutout regularization aims to use both the advantages from dropout as well as the advantages of data augmenation with regards to regularization. It is a regularization technique applied to convolutional neural neworks where we remove neighboring parts of input images and also augmente the dataset with obstructed versions of existing samples. The idea is to extend the dropout in the sense that we also apply a spatial prior that is relevant for images data as images share local connectivities. The ultimate goal is to force the learning algorithm to get a global sense of the full image rather than learning specific features that might endanger the predictions on unseen data where those features are not present. Finally, the key difference with dropout is that cutout is applied to the input layer so that visual features are removed directly from the image and no dropout is applied in the hidden layer and therefore the image itself is modified which makes cutout similar to dropout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@ombelinelag/cutout-regularization-for-cnns-62670d86bc33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import models, layers\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import BatchNormalization, LayerNormalization\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import cifar10  \n",
    "from numpy import save\n",
    "from numpy import savetxt\n",
    "from numpy import loadtxt\n",
    "from time import time\n",
    "import pandas as pd\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D, Input\n",
    "from tensorflow.keras.layers import Flatten, add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(image, size=12, n_squares=1):\n",
    "    h, w, channels = image.shape\n",
    "    new_image = image\n",
    "    for _ in range(n_squares):\n",
    "        y = np.random.randint(h)\n",
    "        x = np.random.randint(w)\n",
    "        y1 = np.clip(y - size // 2, 0, h)\n",
    "        y2 = np.clip(y + size // 2, 0, h)\n",
    "        x1 = np.clip(x - size // 2, 0, w)\n",
    "        x2 = np.clip(x + size // 2, 0, w)\n",
    "        new_image[y1:y2,x1:x2,:] = 0\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original images:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images with cutout:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2880x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "# Data normalization\n",
    "m, std = np.mean(x_train), np.std(x_train)\n",
    "x_train = (x_train - m)/std\n",
    "x_test = (x_test - m)/std\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "plt.figure(figsize=(40,20))\n",
    "print(\"Original images:\")\n",
    "images_index = [20,100]\n",
    "for i in range(2):\n",
    "    tmp = x_train[images_index[i]]\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    plt.imshow(tmp)\n",
    "plt.show()\n",
    "print(\"Images with cutout:\")\n",
    "plt.figure(figsize=(40,20))\n",
    "for i in range(2):\n",
    "    tmp = x_train[images_index[i]]\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    plt.imshow(apply_mask(tmp,size=10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 32, 32, 16)   448         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 32, 32, 16)   64          conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 32, 32, 16)   0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 32, 32, 16)   2320        activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 32, 32, 16)   64          conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 32, 32, 16)   0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 32, 32, 16)   2320        activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 32, 32, 16)   64          conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 32, 32, 16)   0           activation_86[0][0]              \n",
      "                                                                 batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 32, 32, 16)   0           add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 32, 32, 16)   2320        activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 32, 32, 16)   64          conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 32, 32, 16)   0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 32, 32, 16)   2320        activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 32, 32, 16)   64          conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 32, 32, 16)   0           activation_88[0][0]              \n",
      "                                                                 batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 32, 32, 16)   0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 32, 32, 16)   2320        activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 32, 32, 16)   64          conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 32, 32, 16)   0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 32, 32, 16)   2320        activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 32, 32, 16)   64          conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 32, 32, 16)   0           activation_90[0][0]              \n",
      "                                                                 batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 32, 32, 16)   0           add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 32, 32, 16)   2320        activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 32, 32, 16)   64          conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 32, 32, 16)   0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 32, 32, 16)   2320        activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 32, 32, 16)   64          conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 32, 32, 16)   0           activation_92[0][0]              \n",
      "                                                                 batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 32, 32, 16)   0           add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 32, 32, 16)   2320        activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 32, 32, 16)   64          conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 32, 32, 16)   0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 32, 32, 16)   2320        activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 32, 32, 16)   64          conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 32, 32, 16)   0           activation_94[0][0]              \n",
      "                                                                 batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 32, 32, 16)   0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 32, 32, 16)   2320        activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 32, 32, 16)   64          conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 32, 32, 16)   0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 32, 32, 16)   2320        activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 32, 32, 16)   64          conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 32, 32, 16)   0           activation_96[0][0]              \n",
      "                                                                 batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 32, 32, 16)   0           add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 32, 32, 16)   2320        activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 32, 32, 16)   64          conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 32, 32, 16)   0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 32, 32, 16)   2320        activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 32, 32, 16)   64          conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 32, 32, 16)   0           activation_98[0][0]              \n",
      "                                                                 batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 32, 32, 16)   0           add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 16, 16, 32)   4640        activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 16, 16, 32)   128         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 16, 16, 32)   0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 16, 16, 32)   9248        activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 16, 16, 32)   544         activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 16, 16, 32)   128         conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (None, 16, 16, 32)   0           conv2d_107[0][0]                 \n",
      "                                                                 batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 16, 16, 32)   0           add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 16, 16, 32)   9248        activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 16, 16, 32)   128         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 16, 16, 32)   0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 16, 16, 32)   9248        activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 16, 16, 32)   128         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_50 (Add)                    (None, 16, 16, 32)   0           activation_102[0][0]             \n",
      "                                                                 batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 16, 16, 32)   0           add_50[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 16, 16, 32)   9248        activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 16, 16, 32)   128         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 16, 16, 32)   0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 16, 16, 32)   9248        activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 16, 16, 32)   128         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_51 (Add)                    (None, 16, 16, 32)   0           activation_104[0][0]             \n",
      "                                                                 batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 16, 16, 32)   0           add_51[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 16, 16, 32)   9248        activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 16, 16, 32)   128         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 16, 16, 32)   0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 16, 16, 32)   9248        activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 16, 16, 32)   128         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_52 (Add)                    (None, 16, 16, 32)   0           activation_106[0][0]             \n",
      "                                                                 batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 16, 16, 32)   0           add_52[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 16, 16, 32)   9248        activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 16, 16, 32)   128         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 16, 16, 32)   0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 16, 16, 32)   9248        activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 16, 16, 32)   128         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_53 (Add)                    (None, 16, 16, 32)   0           activation_108[0][0]             \n",
      "                                                                 batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 16, 16, 32)   0           add_53[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 16, 16, 32)   9248        activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 16, 16, 32)   128         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 16, 16, 32)   0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 16, 16, 32)   9248        activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 16, 16, 32)   128         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_54 (Add)                    (None, 16, 16, 32)   0           activation_110[0][0]             \n",
      "                                                                 batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 16, 16, 32)   0           add_54[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 16, 16, 32)   9248        activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 16, 16, 32)   128         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 16, 16, 32)   0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 16, 16, 32)   9248        activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 16, 16, 32)   128         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_55 (Add)                    (None, 16, 16, 32)   0           activation_112[0][0]             \n",
      "                                                                 batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 16, 16, 32)   0           add_55[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 8, 8, 64)     18496       activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 8, 8, 64)     256         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 8, 8, 64)     0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 8, 8, 64)     36928       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 8, 8, 64)     2112        activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 8, 8, 64)     256         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_56 (Add)                    (None, 8, 8, 64)     0           conv2d_122[0][0]                 \n",
      "                                                                 batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 8, 8, 64)     0           add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 8, 8, 64)     36928       activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 8, 8, 64)     256         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 8, 8, 64)     0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 8, 8, 64)     36928       activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 8, 8, 64)     256         conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_57 (Add)                    (None, 8, 8, 64)     0           activation_116[0][0]             \n",
      "                                                                 batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 8, 8, 64)     0           add_57[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 8, 8, 64)     36928       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 8, 8, 64)     256         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 8, 8, 64)     0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 8, 8, 64)     36928       activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 8, 8, 64)     256         conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_58 (Add)                    (None, 8, 8, 64)     0           activation_118[0][0]             \n",
      "                                                                 batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 8, 8, 64)     0           add_58[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 8, 8, 64)     36928       activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 8, 8, 64)     256         conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 8, 8, 64)     0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 8, 8, 64)     36928       activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 8, 8, 64)     256         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_59 (Add)                    (None, 8, 8, 64)     0           activation_120[0][0]             \n",
      "                                                                 batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 8, 8, 64)     0           add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 8, 8, 64)     36928       activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 8, 8, 64)     256         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 8, 8, 64)     0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 8, 8, 64)     36928       activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 8, 8, 64)     256         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_60 (Add)                    (None, 8, 8, 64)     0           activation_122[0][0]             \n",
      "                                                                 batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 8, 8, 64)     0           add_60[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 8, 8, 64)     36928       activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 8, 8, 64)     256         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 8, 8, 64)     0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 8, 8, 64)     36928       activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 8, 8, 64)     256         conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_61 (Add)                    (None, 8, 8, 64)     0           activation_124[0][0]             \n",
      "                                                                 batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 8, 8, 64)     0           add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 8, 8, 64)     36928       activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 8, 8, 64)     256         conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 8, 8, 64)     0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 8, 8, 64)     36928       activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 8, 8, 64)     256         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_62 (Add)                    (None, 8, 8, 64)     0           activation_126[0][0]             \n",
      "                                                                 batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 8, 8, 64)     0           add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 1, 1, 64)     0           activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 64)           0           average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           650         flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 665,994\n",
      "Trainable params: 662,826\n",
      "Non-trainable params: 3,168\n",
      "__________________________________________________________________________________________________\n",
      "With data augmentation\n",
      "Epoch 1/2\n",
      "121/782 [===>..........................] - ETA: 9:21 - loss: 2.3391 - acc: 0.3072"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-6909753f251e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    194\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m           callbacks=callbacks)\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training parameters\n",
    "batch_size = 64 \n",
    "epochs =2\n",
    "data_augmentation = True\n",
    "num_classes = 10\n",
    "\n",
    "# subtracting pixel mean improves accuracy\n",
    "subtract_pixel_mean = True\n",
    "n = 7 # so that we use resnet 44 because 6*n+2\n",
    "depth = n * 6 + 2\n",
    "\n",
    "\n",
    "# model name, depth and version\n",
    "model_type = 'ResNet44'\n",
    "\n",
    "# load the CIFAR10 data.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# if subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3 if epoch <80 else 1e-4\n",
    "    return lr\n",
    "\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, in [a])')\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            # first layer but not first stack\n",
    "            if stack > 0 and res_block == 0:  \n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            # first layer but not first stack\n",
    "            if stack > 0 and res_block == 0:\n",
    "                # linear projection residual shortcut\n",
    "                # connection to match changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=lr_schedule(0)),\n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# prepare model model saving directory.\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "\n",
    "# run training, with or without data augmentation.\n",
    "\n",
    "print('With data augmentation')\n",
    "datagen = ImageDataGenerator(featurewise_center=False,samplewise_center=False,featurewise_std_normalization=False,\n",
    "                             samplewise_std_normalization=False,zca_whitening=False,rotation_range=0,width_shift_range=0.1,height_shift_range=0.1,horizontal_flip=True,vertical_flip=False)\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "steps_per_epoch =  math.ceil(len(x_train) / batch_size)\n",
    "# fit the model on the batches generated by datagen.flow().\n",
    "model.fit(x=datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          verbose=1,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),  ### Testing is done with original images.\n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "\n",
    "# score trained model\n",
    "scores = model.evaluate(x_test,\n",
    "                        y_test,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0)\n",
    "print('Validation loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(x, y, epochs, m, batch_size, augment=None):\n",
    "    for _ in range(epochs):\n",
    "        n = x.shape[0]\n",
    "        reorder = np.random.permutation(n)\n",
    "        cursor = 0\n",
    "        while cursor + batch_size < x.shape[0]:\n",
    "            x_batch = x[reorder[cursor:cursor+batch_size]]\n",
    "            y_batch = y[reorder[cursor:cursor+batch_size]]\n",
    "            if augment != None:\n",
    "                yield np.array([augment(xx) for xx in x_batch for rep in range(m)]), np.array([yy for yy in y_batch for rep in range(m)])\n",
    "            else:\n",
    "                yield x_batch, y_batch\n",
    "            cursor += batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-83-bc1be9215067>:28: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-bc1be9215067>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m64.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m     \u001b[0mdurations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1829\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m   @deprecation.deprecated(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "val_acc_cutout = []\n",
    "epochs = 100\n",
    "durations = []\n",
    "for i in [2,4,8,16,32]:\n",
    "    model = resnet_v1(\n",
    "        input_shape=x_train.shape[1:],\n",
    "        depth=44\n",
    "    )      \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.RMSprop(),\n",
    "        metrics=['accuracy']\n",
    "    )    \n",
    "    duration = time()\n",
    "    hist = model.fit_generator(\n",
    "        batch_generator(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            m=i,\n",
    "            batch_size=64,\n",
    "            epochs=100, \n",
    "            augment=apply_mask\n",
    "        ),\n",
    "        epochs=1, \n",
    "        validation_data=(x_test,y_test),\n",
    "        steps_per_epoch=np.floor(x_train.shape[0]/64.0),\n",
    "        verbose=0,\n",
    "        callbacks=[lr_scheduler]\n",
    "    )\n",
    "    durations.append(time()-duration)\n",
    "    val_acc_cutout.append(hist.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opp(l):\n",
    "    return [1-el for el in l]\n",
    "cutout2_data, cutout4_data, cutout8_data, cutout16_data,    \n",
    "  cutout32_data = val_acc_cutout\n",
    "plt.plot(range(1,101),opp(simple_val_acc),\"y-\")\n",
    "plt.plot(range(1,101),opp(cutout2_data),\"b-\")\n",
    "plt.plot(range(1,101),opp(cutout4_data),\"c-\")\n",
    "plt.plot(range(1,101),opp(cutout8_data),\"g-\")\n",
    "plt.plot(range(1,101),opp(cutout16_data),\"r-\")\n",
    "plt.plot(range(1,101),opp(cutout32_data),\"m-\")\n",
    "plt.legend([\"M=0\",\"M=2\",\"M=4\",\"M=8\",\"M=16\",\"M=32\"])\n",
    "plt.plot(np.linspace(0,100,10000),[0.06]*10000,\"k-\")\n",
    "plt.title(\"Validation error for M instances with cutout generated from each input\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Validation error\")\n",
    "plt.savefig(\"acc_cutout.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
